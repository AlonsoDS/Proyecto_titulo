{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2539dea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Merge per-context baseline embeddings into your main dataset.\n",
    "- Input A: base CSV (e.g., 'Datasets/dataset_final.csv') with apartment rows and 'id'.\n",
    "- Input B: embeddings CSV from the previous step (e.g., 'apartment_embeddings_per_context.csv'),\n",
    "           columns: id, emb_<context> (JSON list or empty/NaN).\n",
    "\n",
    "You can:\n",
    "1) Keep compact schema: one column per context (`emb_<ctx>` as JSON string).\n",
    "2) Expand to numeric columns: `emb_<ctx>_d0 ... emb_<ctx>_d11` (12 dims), with optional zero-imputation.\n",
    "\n",
    "Outputs:\n",
    "- datasets/with_embeddings_compact.csv\n",
    "- datasets/with_embeddings_expanded.csv  (if EXPAND = True)\n",
    "\"\"\"\n",
    "\n",
    "# =============================\n",
    "# Cell 1 — Imports & settings\n",
    "# =============================\n",
    "import os, json, math\n",
    "from pathlib import Path\n",
    "from typing import List, Optional, Any\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Paths\n",
    "BASE_DATASET = Path(\"Datasets/dataset_final.csv\")\n",
    "EMB_CSV      = Path(\"apartment_embeddings_per_context.csv\")\n",
    "OUT_DIR      = Path(\"datasets\")\n",
    "OUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Contexts and embedding size\n",
    "CLASSES = [\n",
    "    'sport_and_leisure','medical','education_prim','veterinary',\n",
    "    'food_and_drink_stores','arts_and_entertainment','food_and_drink',\n",
    "    'park_like','security','religion','education_sup'\n",
    "]\n",
    "ALL_CONTEXTS = CLASSES + ['metro','bus']\n",
    "EMB_DIM = 12\n",
    "\n",
    "# Behaviors\n",
    "EXPAND = True          # also create wide numeric columns\n",
    "IMPUTE_MISSING = False # if True, replace missing with zero vectors in the expanded output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69706c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# Cell 2 — Helpers\n",
    "# =============================\n",
    "\n",
    "def is_nan_like(x: Any) -> bool:\n",
    "    # pandas may give float('nan') for empty cells\n",
    "    try:\n",
    "        return x is None or (isinstance(x, float) and math.isnan(x)) or (isinstance(x, str) and x.strip() == \"\")\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "\n",
    "def parse_vec(cell: Any, dim: int) -> Optional[List[float]]:\n",
    "    \"\"\"Parse a JSON list cell → python list[float]; return None if missing/invalid.\n",
    "    Ensures length==dim when returned (truncate or pad zeros if needed).\"\"\"\n",
    "    if is_nan_like(cell):\n",
    "        return None\n",
    "    try:\n",
    "        if isinstance(cell, list):\n",
    "            vec = [float(v) for v in cell]\n",
    "        elif isinstance(cell, str):\n",
    "            vec = json.loads(cell)\n",
    "            vec = [float(v) for v in vec]\n",
    "        else:\n",
    "            return None\n",
    "    except Exception:\n",
    "        return None\n",
    "    # normalize length\n",
    "    if len(vec) > dim:\n",
    "        vec = vec[:dim]\n",
    "    elif len(vec) < dim:\n",
    "        vec = vec + [0.0] * (dim - len(vec))\n",
    "    return vec\n",
    "\n",
    "\n",
    "def expand_embeddings(df: pd.DataFrame, ctx_cols: List[str], dim: int, impute_missing: bool) -> pd.DataFrame:\n",
    "    \"\"\"Expand emb_<ctx> columns into numeric columns emb_<ctx>_d0..d{dim-1}.\n",
    "    If impute_missing=True, fill missing with zeros; else leave as NaN.\"\"\"\n",
    "    out = df.copy()\n",
    "    for col in ctx_cols:\n",
    "        base = col  # e.g., 'emb_medical'\n",
    "        # prepare target columns\n",
    "        tgt_cols = [f\"{base}_d{i}\" for i in range(dim)]\n",
    "        # initialize with NaN\n",
    "        for c in tgt_cols:\n",
    "            out[c] = np.nan\n",
    "        # fill\n",
    "        for idx, cell in out[col].items():\n",
    "            vec = parse_vec(cell, dim)\n",
    "            if vec is None:\n",
    "                if impute_missing:\n",
    "                    out.loc[idx, tgt_cols] = [0.0] * dim\n",
    "                # else keep NaN\n",
    "            else:\n",
    "                out.loc[idx, tgt_cols] = vec\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3543ad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved compact dataset: datasets\\dataset_embeddings_compact.csv  shape=(25215, 40)\n"
     ]
    }
   ],
   "source": [
    "# =============================\n",
    "# Cell 3 — Load & merge (compact)\n",
    "# =============================\n",
    "base = pd.read_csv(BASE_DATASET)\n",
    "emb  = pd.read_csv(EMB_CSV)\n",
    "\n",
    "# sanity\n",
    "if 'id' not in base.columns:\n",
    "    raise ValueError(\"Base dataset must contain 'id' column\")\n",
    "if 'id' not in emb.columns:\n",
    "    raise ValueError(\"Embeddings CSV must contain 'id' column\")\n",
    "\n",
    "# Keep only expected emb_* cols (ignore extras if any)\n",
    "emb_cols = [f\"emb_{c}\" for c in ALL_CONTEXTS if f\"emb_{c}\" in emb.columns]\n",
    "merged = base.merge(emb[['id'] + emb_cols], on='id', how='left')\n",
    "\n",
    "# Save compact version (JSON strings or empty)\n",
    "out_compact = OUT_DIR / \"dataset_embeddings_compact.csv\"\n",
    "merged.to_csv(out_compact, index=False)\n",
    "print(f\"✅ Saved compact dataset: {out_compact}  shape={merged.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "071a107c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Pc-ADS\\AppData\\Local\\Temp\\ipykernel_6504\\780993208.py:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[c] = np.nan\n",
      "C:\\Users\\Pc-ADS\\AppData\\Local\\Temp\\ipykernel_6504\\780993208.py:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[c] = np.nan\n",
      "C:\\Users\\Pc-ADS\\AppData\\Local\\Temp\\ipykernel_6504\\780993208.py:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[c] = np.nan\n",
      "C:\\Users\\Pc-ADS\\AppData\\Local\\Temp\\ipykernel_6504\\780993208.py:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[c] = np.nan\n",
      "C:\\Users\\Pc-ADS\\AppData\\Local\\Temp\\ipykernel_6504\\780993208.py:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[c] = np.nan\n",
      "C:\\Users\\Pc-ADS\\AppData\\Local\\Temp\\ipykernel_6504\\780993208.py:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[c] = np.nan\n",
      "C:\\Users\\Pc-ADS\\AppData\\Local\\Temp\\ipykernel_6504\\780993208.py:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[c] = np.nan\n",
      "C:\\Users\\Pc-ADS\\AppData\\Local\\Temp\\ipykernel_6504\\780993208.py:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[c] = np.nan\n",
      "C:\\Users\\Pc-ADS\\AppData\\Local\\Temp\\ipykernel_6504\\780993208.py:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[c] = np.nan\n",
      "C:\\Users\\Pc-ADS\\AppData\\Local\\Temp\\ipykernel_6504\\780993208.py:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[c] = np.nan\n",
      "C:\\Users\\Pc-ADS\\AppData\\Local\\Temp\\ipykernel_6504\\780993208.py:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[c] = np.nan\n",
      "C:\\Users\\Pc-ADS\\AppData\\Local\\Temp\\ipykernel_6504\\780993208.py:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[c] = np.nan\n",
      "C:\\Users\\Pc-ADS\\AppData\\Local\\Temp\\ipykernel_6504\\780993208.py:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[c] = np.nan\n",
      "C:\\Users\\Pc-ADS\\AppData\\Local\\Temp\\ipykernel_6504\\780993208.py:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[c] = np.nan\n",
      "C:\\Users\\Pc-ADS\\AppData\\Local\\Temp\\ipykernel_6504\\780993208.py:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[c] = np.nan\n",
      "C:\\Users\\Pc-ADS\\AppData\\Local\\Temp\\ipykernel_6504\\780993208.py:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[c] = np.nan\n",
      "C:\\Users\\Pc-ADS\\AppData\\Local\\Temp\\ipykernel_6504\\780993208.py:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[c] = np.nan\n",
      "C:\\Users\\Pc-ADS\\AppData\\Local\\Temp\\ipykernel_6504\\780993208.py:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[c] = np.nan\n",
      "C:\\Users\\Pc-ADS\\AppData\\Local\\Temp\\ipykernel_6504\\780993208.py:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[c] = np.nan\n",
      "C:\\Users\\Pc-ADS\\AppData\\Local\\Temp\\ipykernel_6504\\780993208.py:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[c] = np.nan\n",
      "C:\\Users\\Pc-ADS\\AppData\\Local\\Temp\\ipykernel_6504\\780993208.py:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[c] = np.nan\n",
      "C:\\Users\\Pc-ADS\\AppData\\Local\\Temp\\ipykernel_6504\\780993208.py:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[c] = np.nan\n",
      "C:\\Users\\Pc-ADS\\AppData\\Local\\Temp\\ipykernel_6504\\780993208.py:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[c] = np.nan\n",
      "C:\\Users\\Pc-ADS\\AppData\\Local\\Temp\\ipykernel_6504\\780993208.py:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[c] = np.nan\n",
      "C:\\Users\\Pc-ADS\\AppData\\Local\\Temp\\ipykernel_6504\\780993208.py:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[c] = np.nan\n",
      "C:\\Users\\Pc-ADS\\AppData\\Local\\Temp\\ipykernel_6504\\780993208.py:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[c] = np.nan\n",
      "C:\\Users\\Pc-ADS\\AppData\\Local\\Temp\\ipykernel_6504\\780993208.py:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[c] = np.nan\n",
      "C:\\Users\\Pc-ADS\\AppData\\Local\\Temp\\ipykernel_6504\\780993208.py:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[c] = np.nan\n",
      "C:\\Users\\Pc-ADS\\AppData\\Local\\Temp\\ipykernel_6504\\780993208.py:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[c] = np.nan\n",
      "C:\\Users\\Pc-ADS\\AppData\\Local\\Temp\\ipykernel_6504\\780993208.py:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[c] = np.nan\n",
      "C:\\Users\\Pc-ADS\\AppData\\Local\\Temp\\ipykernel_6504\\780993208.py:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[c] = np.nan\n",
      "C:\\Users\\Pc-ADS\\AppData\\Local\\Temp\\ipykernel_6504\\780993208.py:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[c] = np.nan\n",
      "C:\\Users\\Pc-ADS\\AppData\\Local\\Temp\\ipykernel_6504\\780993208.py:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[c] = np.nan\n",
      "C:\\Users\\Pc-ADS\\AppData\\Local\\Temp\\ipykernel_6504\\780993208.py:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[c] = np.nan\n",
      "C:\\Users\\Pc-ADS\\AppData\\Local\\Temp\\ipykernel_6504\\780993208.py:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[c] = np.nan\n",
      "C:\\Users\\Pc-ADS\\AppData\\Local\\Temp\\ipykernel_6504\\780993208.py:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[c] = np.nan\n",
      "C:\\Users\\Pc-ADS\\AppData\\Local\\Temp\\ipykernel_6504\\780993208.py:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[c] = np.nan\n",
      "C:\\Users\\Pc-ADS\\AppData\\Local\\Temp\\ipykernel_6504\\780993208.py:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[c] = np.nan\n",
      "C:\\Users\\Pc-ADS\\AppData\\Local\\Temp\\ipykernel_6504\\780993208.py:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[c] = np.nan\n",
      "C:\\Users\\Pc-ADS\\AppData\\Local\\Temp\\ipykernel_6504\\780993208.py:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[c] = np.nan\n",
      "C:\\Users\\Pc-ADS\\AppData\\Local\\Temp\\ipykernel_6504\\780993208.py:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[c] = np.nan\n",
      "C:\\Users\\Pc-ADS\\AppData\\Local\\Temp\\ipykernel_6504\\780993208.py:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[c] = np.nan\n",
      "C:\\Users\\Pc-ADS\\AppData\\Local\\Temp\\ipykernel_6504\\780993208.py:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[c] = np.nan\n",
      "C:\\Users\\Pc-ADS\\AppData\\Local\\Temp\\ipykernel_6504\\780993208.py:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[c] = np.nan\n",
      "C:\\Users\\Pc-ADS\\AppData\\Local\\Temp\\ipykernel_6504\\780993208.py:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[c] = np.nan\n",
      "C:\\Users\\Pc-ADS\\AppData\\Local\\Temp\\ipykernel_6504\\780993208.py:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[c] = np.nan\n",
      "C:\\Users\\Pc-ADS\\AppData\\Local\\Temp\\ipykernel_6504\\780993208.py:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[c] = np.nan\n",
      "C:\\Users\\Pc-ADS\\AppData\\Local\\Temp\\ipykernel_6504\\780993208.py:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[c] = np.nan\n",
      "C:\\Users\\Pc-ADS\\AppData\\Local\\Temp\\ipykernel_6504\\780993208.py:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[c] = np.nan\n",
      "C:\\Users\\Pc-ADS\\AppData\\Local\\Temp\\ipykernel_6504\\780993208.py:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[c] = np.nan\n",
      "C:\\Users\\Pc-ADS\\AppData\\Local\\Temp\\ipykernel_6504\\780993208.py:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[c] = np.nan\n",
      "C:\\Users\\Pc-ADS\\AppData\\Local\\Temp\\ipykernel_6504\\780993208.py:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[c] = np.nan\n",
      "C:\\Users\\Pc-ADS\\AppData\\Local\\Temp\\ipykernel_6504\\780993208.py:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[c] = np.nan\n",
      "C:\\Users\\Pc-ADS\\AppData\\Local\\Temp\\ipykernel_6504\\780993208.py:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[c] = np.nan\n",
      "C:\\Users\\Pc-ADS\\AppData\\Local\\Temp\\ipykernel_6504\\780993208.py:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[c] = np.nan\n",
      "C:\\Users\\Pc-ADS\\AppData\\Local\\Temp\\ipykernel_6504\\780993208.py:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[c] = np.nan\n",
      "C:\\Users\\Pc-ADS\\AppData\\Local\\Temp\\ipykernel_6504\\780993208.py:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[c] = np.nan\n",
      "C:\\Users\\Pc-ADS\\AppData\\Local\\Temp\\ipykernel_6504\\780993208.py:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[c] = np.nan\n",
      "C:\\Users\\Pc-ADS\\AppData\\Local\\Temp\\ipykernel_6504\\780993208.py:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[c] = np.nan\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved expanded dataset: datasets\\dataset_embeddings_expanded.csv  shape=(25215, 196)\n",
      "emb_sport_and_leisure: missing rows (all NaN) = 92\n",
      "emb_medical: missing rows (all NaN) = 128\n",
      "emb_education_prim: missing rows (all NaN) = 696\n",
      "emb_veterinary: missing rows (all NaN) = 2233\n",
      "emb_food_and_drink_stores: missing rows (all NaN) = 521\n",
      "emb_arts_and_entertainment: missing rows (all NaN) = 208\n",
      "emb_food_and_drink: missing rows (all NaN) = 1010\n",
      "emb_park_like: missing rows (all NaN) = 940\n",
      "emb_security: missing rows (all NaN) = 651\n",
      "emb_religion: missing rows (all NaN) = 3924\n",
      "emb_education_sup: missing rows (all NaN) = 634\n",
      "emb_metro: missing rows (all NaN) = 8463\n",
      "emb_bus: missing rows (all NaN) = 1040\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# =============================\n",
    "# Cell 4 — Optional: expand to numeric\n",
    "# =============================\n",
    "if EXPAND:\n",
    "    wide = expand_embeddings(merged, emb_cols, EMB_DIM, impute_missing=IMPUTE_MISSING)\n",
    "    out_expanded = OUT_DIR / \"dataset_embeddings_expanded.csv\"\n",
    "    wide.to_csv(out_expanded, index=False)\n",
    "    print(f\"✅ Saved expanded dataset: {out_expanded}  shape={wide.shape}\")\n",
    "\n",
    "    # Tiny health check: report NaN rates per expanded block\n",
    "    for ctx in ALL_CONTEXTS:\n",
    "        base = f\"emb_{ctx}\"\n",
    "        if base in emb_cols:\n",
    "            cols = [f\"{base}_d{i}\" for i in range(EMB_DIM)]\n",
    "            if all(c in wide.columns for c in cols):\n",
    "                n_missing_rows = wide[cols].isna().all(axis=1).sum()\n",
    "                print(f\"{base}: missing rows (all NaN) = {n_missing_rows}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126c1c8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2edc152",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6763063",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Datasets/dataset_embeddings_compact.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d5d646c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 25215 entries, 0 to 25214\n",
      "Data columns (total 40 columns):\n",
      " #   Column                      Non-Null Count  Dtype  \n",
      "---  ------                      --------------  -----  \n",
      " 0   id                          25215 non-null  int64  \n",
      " 1   monto                       25215 non-null  int64  \n",
      " 2   superficie_t                25215 non-null  float64\n",
      " 3   dormitorios                 25215 non-null  int64  \n",
      " 4   dormitorios_faltante        25215 non-null  int64  \n",
      " 5   banos                       25215 non-null  int64  \n",
      " 6   banos_faltante              25215 non-null  int64  \n",
      " 7   antiguedad                  25215 non-null  int64  \n",
      " 8   antiguedad_faltante         25215 non-null  int64  \n",
      " 9   Or_N                        25215 non-null  int64  \n",
      " 10  Or_S                        25215 non-null  int64  \n",
      " 11  Or_E                        25215 non-null  int64  \n",
      " 12  Or_O                        25215 non-null  int64  \n",
      " 13  Or_Faltante                 25215 non-null  int64  \n",
      " 14  terraza                     25215 non-null  float64\n",
      " 15  estacionamiento             25215 non-null  int64  \n",
      " 16  bodegas                     25215 non-null  int64  \n",
      " 17  flag_Departamento           25215 non-null  int64  \n",
      " 18  flag_Multinivel             25215 non-null  int64  \n",
      " 19  flag_Semipiso               25215 non-null  int64  \n",
      " 20  flag_Premium                25215 non-null  int64  \n",
      " 21  flag_Monoambiente           25215 non-null  int64  \n",
      " 22  flag_Loft                   25215 non-null  int64  \n",
      " 23  latitud                     25215 non-null  float64\n",
      " 24  longitud                    25215 non-null  float64\n",
      " 25  comuna                      25215 non-null  object \n",
      " 26  calle                       25214 non-null  object \n",
      " 27  emb_sport_and_leisure       25123 non-null  object \n",
      " 28  emb_medical                 25087 non-null  object \n",
      " 29  emb_education_prim          24519 non-null  object \n",
      " 30  emb_veterinary              22982 non-null  object \n",
      " 31  emb_food_and_drink_stores   24694 non-null  object \n",
      " 32  emb_arts_and_entertainment  25007 non-null  object \n",
      " 33  emb_food_and_drink          24205 non-null  object \n",
      " 34  emb_park_like               24275 non-null  object \n",
      " 35  emb_security                24564 non-null  object \n",
      " 36  emb_religion                21291 non-null  object \n",
      " 37  emb_education_sup           24581 non-null  object \n",
      " 38  emb_metro                   16752 non-null  object \n",
      " 39  emb_bus                     24175 non-null  object \n",
      "dtypes: float64(4), int64(21), object(15)\n",
      "memory usage: 7.7+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8a89bf3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emb_sport_and_leisure</th>\n",
       "      <th>emb_medical</th>\n",
       "      <th>emb_education_prim</th>\n",
       "      <th>emb_veterinary</th>\n",
       "      <th>emb_food_and_drink_stores</th>\n",
       "      <th>emb_arts_and_entertainment</th>\n",
       "      <th>emb_food_and_drink</th>\n",
       "      <th>emb_park_like</th>\n",
       "      <th>emb_security</th>\n",
       "      <th>emb_religion</th>\n",
       "      <th>emb_education_sup</th>\n",
       "      <th>emb_metro</th>\n",
       "      <th>emb_bus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8591</th>\n",
       "      <td>[15.0, 4.224615573883057, 1.9384450912475586, ...</td>\n",
       "      <td>[27.0, 8.392669677734375, 4.2169976234436035, ...</td>\n",
       "      <td>[9.0, 2.606393337249756, 1.181631088256836, 0....</td>\n",
       "      <td>[5.0, 1.329038143157959, 0.503171980381012, 0....</td>\n",
       "      <td>[14.0, 4.671542644500732, 2.431705951690674, 1...</td>\n",
       "      <td>[15.0, 4.817954063415527, 2.3077478408813477, ...</td>\n",
       "      <td>[16.0, 3.472376585006714, 1.7330747842788696, ...</td>\n",
       "      <td>[3.0, 0.5058801770210266, 0.09826972335577011,...</td>\n",
       "      <td>[5.0, 2.5319695472717285, 1.7620137929916382, ...</td>\n",
       "      <td>[7.0, 1.8511478900909424, 0.6647428274154663, ...</td>\n",
       "      <td>[31.0, 2.805297374725342, 0.6065185070037842, ...</td>\n",
       "      <td>[4.0, 0.006914932746440172, 1.2321779649937525...</td>\n",
       "      <td>[10.0, 0.039180558174848557, 0.000162675249157...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2879</th>\n",
       "      <td>[25.0, 9.492673873901367, 5.565189838409424, 3...</td>\n",
       "      <td>[45.0, 16.78483009338379, 8.280882835388184, 4...</td>\n",
       "      <td>[8.0, 3.0814900398254395, 1.625666856765747, 0...</td>\n",
       "      <td>[7.0, 3.3021271228790283, 1.7709932327270508, ...</td>\n",
       "      <td>[74.0, 25.612770080566406, 11.952367782592773,...</td>\n",
       "      <td>[109.0, 34.0040283203125, 14.539718627929688, ...</td>\n",
       "      <td>[61.0, 20.784473419189453, 10.61701774597168, ...</td>\n",
       "      <td>[10.0, 3.438467264175415, 1.696580171585083, 0...</td>\n",
       "      <td>[25.0, 8.559910774230957, 4.350227355957031, 2...</td>\n",
       "      <td>[6.0, 2.9340734481811523, 1.8986643552780151, ...</td>\n",
       "      <td>[261.0, 75.6412353515625, 30.544252395629883, ...</td>\n",
       "      <td>[2.0, 0.003763932967558503, 7.4524014053167775...</td>\n",
       "      <td>[14.0, 0.06424766033887863, 0.0003481704043224...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11538</th>\n",
       "      <td>[15.0, 5.616213798522949, 3.184290885925293, 2...</td>\n",
       "      <td>[45.0, 15.88559341430664, 7.884194850921631, 4...</td>\n",
       "      <td>[13.0, 5.120848178863525, 2.4622015953063965, ...</td>\n",
       "      <td>[6.0, 1.7465453147888184, 0.6612012386322021, ...</td>\n",
       "      <td>[47.0, 13.251874923706055, 5.611817359924316, ...</td>\n",
       "      <td>[132.0, 42.261619567871094, 19.22429084777832,...</td>\n",
       "      <td>[23.0, 7.932838439941406, 4.073561668395996, 2...</td>\n",
       "      <td>[10.0, 2.696364164352417, 1.1931681632995605, ...</td>\n",
       "      <td>[25.0, 10.482751846313477, 5.687837600708008, ...</td>\n",
       "      <td>[3.0, 1.2304683923721313, 0.5494815707206726, ...</td>\n",
       "      <td>[219.0, 62.78380584716797, 24.358396530151367,...</td>\n",
       "      <td>[4.0, 0.006714826449751854, 1.135989441536367e...</td>\n",
       "      <td>[20.0, 0.08674301207065582, 0.0004941411898471...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9387</th>\n",
       "      <td>[80.0, 26.16571617126465, 13.462533950805664, ...</td>\n",
       "      <td>[44.0, 12.13841438293457, 5.600058078765869, 3...</td>\n",
       "      <td>[15.0, 6.6314544677734375, 3.960141658782959, ...</td>\n",
       "      <td>[9.0, 2.0835227966308594, 0.716124951839447, 0...</td>\n",
       "      <td>[23.0, 7.304398536682129, 4.051912307739258, 2...</td>\n",
       "      <td>[28.0, 9.376432418823242, 4.148211479187012, 2...</td>\n",
       "      <td>[32.0, 9.578359603881836, 5.080397605895996, 3...</td>\n",
       "      <td>[2.0, 0.5157100558280945, 0.18473798036575317,...</td>\n",
       "      <td>[10.0, 3.8968100547790527, 2.049531936645508, ...</td>\n",
       "      <td>[7.0, 2.2695226669311523, 1.148230791091919, 0...</td>\n",
       "      <td>[93.0, 22.98316192626953, 7.621384143829346, 3...</td>\n",
       "      <td>[2.0, 0.003966307733207941, 7.865798579587135e...</td>\n",
       "      <td>[11.0, 0.06362111121416092, 0.0004315402766223...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20751</th>\n",
       "      <td>[165.0, 42.585594177246094, 15.75684928894043,...</td>\n",
       "      <td>[60.0, 23.957597732543945, 12.755298614501953,...</td>\n",
       "      <td>[8.0, 2.783745050430298, 1.2103625535964966, 0...</td>\n",
       "      <td>[5.0, 1.6231358051300049, 0.7118280529975891, ...</td>\n",
       "      <td>[51.0, 17.03934097290039, 7.2759504318237305, ...</td>\n",
       "      <td>[32.0, 10.473343849182129, 5.2005228996276855,...</td>\n",
       "      <td>[124.0, 42.62345886230469, 21.00731658935547, ...</td>\n",
       "      <td>[5.0, 1.8151519298553467, 0.9295656681060791, ...</td>\n",
       "      <td>[3.0, 1.260799765586853, 0.9025589227676392, 0...</td>\n",
       "      <td>[6.0, 1.270039677619934, 0.4241797626018524, 0...</td>\n",
       "      <td>[65.0, 27.582374572753906, 16.757814407348633,...</td>\n",
       "      <td>[5.0, 0.010694421827793121, 2.6754742066259496...</td>\n",
       "      <td>[12.0, 0.05002456530928612, 0.0002235876017948...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   emb_sport_and_leisure  \\\n",
       "8591   [15.0, 4.224615573883057, 1.9384450912475586, ...   \n",
       "2879   [25.0, 9.492673873901367, 5.565189838409424, 3...   \n",
       "11538  [15.0, 5.616213798522949, 3.184290885925293, 2...   \n",
       "9387   [80.0, 26.16571617126465, 13.462533950805664, ...   \n",
       "20751  [165.0, 42.585594177246094, 15.75684928894043,...   \n",
       "\n",
       "                                             emb_medical  \\\n",
       "8591   [27.0, 8.392669677734375, 4.2169976234436035, ...   \n",
       "2879   [45.0, 16.78483009338379, 8.280882835388184, 4...   \n",
       "11538  [45.0, 15.88559341430664, 7.884194850921631, 4...   \n",
       "9387   [44.0, 12.13841438293457, 5.600058078765869, 3...   \n",
       "20751  [60.0, 23.957597732543945, 12.755298614501953,...   \n",
       "\n",
       "                                      emb_education_prim  \\\n",
       "8591   [9.0, 2.606393337249756, 1.181631088256836, 0....   \n",
       "2879   [8.0, 3.0814900398254395, 1.625666856765747, 0...   \n",
       "11538  [13.0, 5.120848178863525, 2.4622015953063965, ...   \n",
       "9387   [15.0, 6.6314544677734375, 3.960141658782959, ...   \n",
       "20751  [8.0, 2.783745050430298, 1.2103625535964966, 0...   \n",
       "\n",
       "                                          emb_veterinary  \\\n",
       "8591   [5.0, 1.329038143157959, 0.503171980381012, 0....   \n",
       "2879   [7.0, 3.3021271228790283, 1.7709932327270508, ...   \n",
       "11538  [6.0, 1.7465453147888184, 0.6612012386322021, ...   \n",
       "9387   [9.0, 2.0835227966308594, 0.716124951839447, 0...   \n",
       "20751  [5.0, 1.6231358051300049, 0.7118280529975891, ...   \n",
       "\n",
       "                               emb_food_and_drink_stores  \\\n",
       "8591   [14.0, 4.671542644500732, 2.431705951690674, 1...   \n",
       "2879   [74.0, 25.612770080566406, 11.952367782592773,...   \n",
       "11538  [47.0, 13.251874923706055, 5.611817359924316, ...   \n",
       "9387   [23.0, 7.304398536682129, 4.051912307739258, 2...   \n",
       "20751  [51.0, 17.03934097290039, 7.2759504318237305, ...   \n",
       "\n",
       "                              emb_arts_and_entertainment  \\\n",
       "8591   [15.0, 4.817954063415527, 2.3077478408813477, ...   \n",
       "2879   [109.0, 34.0040283203125, 14.539718627929688, ...   \n",
       "11538  [132.0, 42.261619567871094, 19.22429084777832,...   \n",
       "9387   [28.0, 9.376432418823242, 4.148211479187012, 2...   \n",
       "20751  [32.0, 10.473343849182129, 5.2005228996276855,...   \n",
       "\n",
       "                                      emb_food_and_drink  \\\n",
       "8591   [16.0, 3.472376585006714, 1.7330747842788696, ...   \n",
       "2879   [61.0, 20.784473419189453, 10.61701774597168, ...   \n",
       "11538  [23.0, 7.932838439941406, 4.073561668395996, 2...   \n",
       "9387   [32.0, 9.578359603881836, 5.080397605895996, 3...   \n",
       "20751  [124.0, 42.62345886230469, 21.00731658935547, ...   \n",
       "\n",
       "                                           emb_park_like  \\\n",
       "8591   [3.0, 0.5058801770210266, 0.09826972335577011,...   \n",
       "2879   [10.0, 3.438467264175415, 1.696580171585083, 0...   \n",
       "11538  [10.0, 2.696364164352417, 1.1931681632995605, ...   \n",
       "9387   [2.0, 0.5157100558280945, 0.18473798036575317,...   \n",
       "20751  [5.0, 1.8151519298553467, 0.9295656681060791, ...   \n",
       "\n",
       "                                            emb_security  \\\n",
       "8591   [5.0, 2.5319695472717285, 1.7620137929916382, ...   \n",
       "2879   [25.0, 8.559910774230957, 4.350227355957031, 2...   \n",
       "11538  [25.0, 10.482751846313477, 5.687837600708008, ...   \n",
       "9387   [10.0, 3.8968100547790527, 2.049531936645508, ...   \n",
       "20751  [3.0, 1.260799765586853, 0.9025589227676392, 0...   \n",
       "\n",
       "                                            emb_religion  \\\n",
       "8591   [7.0, 1.8511478900909424, 0.6647428274154663, ...   \n",
       "2879   [6.0, 2.9340734481811523, 1.8986643552780151, ...   \n",
       "11538  [3.0, 1.2304683923721313, 0.5494815707206726, ...   \n",
       "9387   [7.0, 2.2695226669311523, 1.148230791091919, 0...   \n",
       "20751  [6.0, 1.270039677619934, 0.4241797626018524, 0...   \n",
       "\n",
       "                                       emb_education_sup  \\\n",
       "8591   [31.0, 2.805297374725342, 0.6065185070037842, ...   \n",
       "2879   [261.0, 75.6412353515625, 30.544252395629883, ...   \n",
       "11538  [219.0, 62.78380584716797, 24.358396530151367,...   \n",
       "9387   [93.0, 22.98316192626953, 7.621384143829346, 3...   \n",
       "20751  [65.0, 27.582374572753906, 16.757814407348633,...   \n",
       "\n",
       "                                               emb_metro  \\\n",
       "8591   [4.0, 0.006914932746440172, 1.2321779649937525...   \n",
       "2879   [2.0, 0.003763932967558503, 7.4524014053167775...   \n",
       "11538  [4.0, 0.006714826449751854, 1.135989441536367e...   \n",
       "9387   [2.0, 0.003966307733207941, 7.865798579587135e...   \n",
       "20751  [5.0, 0.010694421827793121, 2.6754742066259496...   \n",
       "\n",
       "                                                 emb_bus  \n",
       "8591   [10.0, 0.039180558174848557, 0.000162675249157...  \n",
       "2879   [14.0, 0.06424766033887863, 0.0003481704043224...  \n",
       "11538  [20.0, 0.08674301207065582, 0.0004941411898471...  \n",
       "9387   [11.0, 0.06362111121416092, 0.0004315402766223...  \n",
       "20751  [12.0, 0.05002456530928612, 0.0002235876017948...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[:, -13:].sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5edb8869",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apartment ID: 2857804660\n",
      "Medical embedding values:\n",
      "  dim0: 4.000000\n",
      "  dim1: 1.016588\n",
      "  dim2: 0.392391\n",
      "  dim3: 0.195022\n",
      "  dim4: 0.254147\n",
      "  dim5: 0.183050\n",
      "  dim6: 0.114936\n",
      "  dim7: 0.569072\n",
      "  dim8: 0.569072\n",
      "  dim9: 0.300551\n",
      "  dim10: 0.000000\n",
      "  dim11: 0.250000\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import ast\n",
    "\n",
    "# Pick a random row where medical embedding is not missing\n",
    "row = df[df[\"emb_medical\"].notna()].sample(1, random_state=random.randint(0, 9999)).iloc[0]\n",
    "\n",
    "apt_id = row[\"id\"]  # or whatever your apartment ID column is named\n",
    "emb_str = row[\"emb_medical\"]\n",
    "\n",
    "# Convert the string \"[...]\" into a Python list of floats\n",
    "emb_list = ast.literal_eval(emb_str)\n",
    "print(f\"Apartment ID: {apt_id}\")\n",
    "print(\"Medical embedding values:\")\n",
    "for i, v in enumerate(emb_list):\n",
    "    print(f\"  dim{i}: {v:.6f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a0a7b4",
   "metadata": {},
   "source": [
    "### Explicación embeddings:\n",
    " - Apartment ID: 1563616351\n",
    "\n",
    "Medical embedding values:\n",
    "\n",
    " -  dim0: 9.000000 &rarr; Cantidad de POIs de la clase vinculados al departamento\n",
    " -  dim1: 3.402536 &rarr; Distancia media\n",
    " -  dim2: 1.822208 &rarr; Distancia Minima\n",
    " -  dim3: 1.077151 &rarr; Distancia Maxima\n",
    " -  dim4: 0.378060 &rarr; Mediana de la distancia\n",
    " -  dim5: 0.244005 &rarr; Desviación estandar de la distancia\n",
    " -  dim6: 0.002132 &rarr; Cercanía media (distancia inversa)\n",
    " -  dim7: 0.795181 &rarr; Cercanía máxima (POI más cercano)\n",
    " -  dim8: 0.795181 &rarr; Cercanía total (suma de distancias inversas)\n",
    " -  dim9: 0.625955 &rarr; Proporción dentro del radio cercano (600m) \n",
    " -  dim10: 0.111111 &rarr; Proporción dentro del radio medio (1200m)\n",
    " -  dim11: 0.222222 &rarr; Proporción dentro del radio lejano (2400m)\n",
    "\n",
    "Sobre dim 9, 10 y 11, se refieren a la fracción de POIs que caen dentro de un cierto radio respecto del total de POIs del contexto o clase para ese departamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8924de6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apartment ID: 1542059051\n",
      "dim00: count_pois                   = 35.000000\n",
      "dim01: mean_distance                = 9.235236\n",
      "dim02: min_distance                 = 2.353643\n",
      "dim03: max_distance                 = 3.971305\n",
      "dim04: median_distance              = 0.263864\n",
      "dim05: std_distance                 = 0.209384\n",
      "dim06: mean_inverse_distance        = 0.012688\n",
      "dim07: max_inverse_distance         = 0.917077\n",
      "dim08: sum_inverse_distance         = 0.917077\n",
      "dim09: ratio_within_near_radius     = 0.799653\n",
      "dim10: ratio_within_mid_radius      = 0.057143\n",
      "dim11: ratio_within_far_radius      = 0.085714\n",
      "\n",
      "⚠️ Warnings:\n",
      " - Median is not between min and max; check how median was computed (or units).\n",
      " - max_inv equals sum_inv; likely a bug in the builder (sum_inv was set from max_inv).\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# expected columns exist: 'id', 'emb_medical'\n",
    "row = df[df[\"emb_medical\"].notna()].sample(1, random_state=42).iloc[0]\n",
    "emb = ast.literal_eval(row[\"emb_medical\"])\n",
    "assert len(emb) == 12, f\"Expected 12 dims, got {len(emb)}\"\n",
    "\n",
    "# Indices for readability\n",
    "COUNT = 0\n",
    "MEAN  = 1\n",
    "D2    = 2   # min or max (we'll decide below)\n",
    "D3    = 3   # max or min\n",
    "MED   = 4\n",
    "STD   = 5\n",
    "MEAN_INV = 6\n",
    "MAX_INV  = 7\n",
    "SUM_INV  = 8\n",
    "R_NEAR   = 9\n",
    "R_MID    = 10\n",
    "R_FAR    = 11\n",
    "\n",
    "# Heuristic to assign min/max consistently\n",
    "d2, d3, med = emb[D2], emb[D3], emb[MED]\n",
    "# choose MIN = smaller of (d2, d3), MAX = bigger; keeps things sane for docs\n",
    "d_min, d_max = (min(d2, d3), max(d2, d3))\n",
    "# optional sanity checks\n",
    "warnings = []\n",
    "if not (d_min <= med <= d_max):\n",
    "    warnings.append(\"Median is not between min and max; check how median was computed (or units).\")\n",
    "\n",
    "# Detect suspicious duplication of max_inv and sum_inv\n",
    "if abs(emb[MAX_INV] - emb[SUM_INV]) < 1e-9:\n",
    "    warnings.append(\"max_inv equals sum_inv; likely a bug in the builder (sum_inv was set from max_inv).\")\n",
    "\n",
    "# Pretty print with corrected labels\n",
    "print(f\"Apartment ID: {row['id']}\")\n",
    "lines = [\n",
    "    (0,  \"count_pois\",                emb[COUNT]),\n",
    "    (1,  \"mean_distance\",             emb[MEAN]),\n",
    "    (2,  \"min_distance\",              d_min),\n",
    "    (3,  \"max_distance\",              d_max),\n",
    "    (4,  \"median_distance\",           emb[MED]),\n",
    "    (5,  \"std_distance\",              emb[STD]),\n",
    "    (6,  \"mean_inverse_distance\",     emb[MEAN_INV]),\n",
    "    (7,  \"max_inverse_distance\",      emb[MAX_INV]),\n",
    "    (8,  \"sum_inverse_distance\",      emb[SUM_INV]),\n",
    "    (9,  \"ratio_within_near_radius\",  emb[R_NEAR]),\n",
    "    (10, \"ratio_within_mid_radius\",   emb[R_MID]),\n",
    "    (11, \"ratio_within_far_radius\",   emb[R_FAR]),\n",
    "]\n",
    "for i, name, val in lines:\n",
    "    print(f\"dim{i:02d}: {name:28s} = {val:.6f}\")\n",
    "\n",
    "if warnings:\n",
    "    print(\"\\n⚠️ Warnings:\")\n",
    "    for w in warnings:\n",
    "        print(\" -\", w)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0ccda2ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[METRO] Latest shard: METROSHARD_20250826_091005_1553843137-1548097259.pkl\n",
      "  apt 2854009162: edges=2, dist[min,med,max]=[36.91, 69.03, 101.15]\n",
      "  apt 2852968410: edges=2, dist[min,med,max]=[624.46, 650.02, 675.58]\n",
      "  apt 2863800448: edges=1, dist[min,med,max]=[727.03, 727.03, 727.03]\n",
      "[BUS] Latest shard: BUSSHARD_20250826_153430_2862820058-1548097259.pkl\n",
      "  apt 1572173539: edges=6, dist[min,med,max]=[60.46, 279.38, 361.33]\n",
      "  apt 2862820058: edges=7, dist[min,med,max]=[66.24, 285.77, 349.29]\n",
      "  apt 2833913964: edges=23, dist[min,med,max]=[24.66, 241.85, 350.63]\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pickle, torch, numpy as np\n",
    "\n",
    "SHARD_DIR = Path(\"Graph_data_OLD\")  # your new folder name\n",
    "metro_shards = sorted(SHARD_DIR.glob(\"METROSHARD_*.pkl\"))\n",
    "bus_shards   = sorted(SHARD_DIR.glob(\"BUSSHARD_*.pkl\"))\n",
    "\n",
    "def peek_some(shard_paths, label, n_apts=3):\n",
    "    if not shard_paths:\n",
    "        print(f\"No {label} shards found.\")\n",
    "        return\n",
    "    p = shard_paths[-1]\n",
    "    print(f\"[{label}] Latest shard: {p.name}\")\n",
    "    with open(p, \"rb\") as f:\n",
    "        d = pickle.load(f)  # dict[int -> graphs-dict or Data]\n",
    "    cnt = 0\n",
    "    for aid, v in d.items():\n",
    "        if isinstance(v, dict):\n",
    "            # common structure: {'metro': Data|None} or {'bus': Data|None}\n",
    "            g = next((gg for gg in v.values() if gg is not None), None)\n",
    "        else:\n",
    "            # rare case: stored Data directly\n",
    "            g = v\n",
    "        if g is None:\n",
    "            print(f\"  apt {aid}: None\")\n",
    "            continue\n",
    "        # edge_attr should be meters (values in ~[10, 3000+] typically)\n",
    "        ea = g.edge_attr.view(-1).cpu().numpy()\n",
    "        print(f\"  apt {aid}: edges={ea.size}, dist[min,med,max]=[{ea.min():.2f}, {np.median(ea):.2f}, {ea.max():.2f}]\")\n",
    "        cnt += 1\n",
    "        if cnt >= n_apts:\n",
    "            break\n",
    "\n",
    "peek_some(metro_shards, \"METRO\")\n",
    "peek_some(bus_shards,   \"BUS\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7714b434",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ wrote embeddings_metro_bus_from_shards.csv with 24234 rows (metro non-null: 16752, bus non-null: 24175)\n"
     ]
    }
   ],
   "source": [
    "import json, numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "\n",
    "SHARD_DIR = Path(\"Graph_data_OLD\")\n",
    "OUT_CSV   = \"embeddings_metro_bus_from_shards.csv\"\n",
    "\n",
    "R1, R2, R3 = 600.0, 1200.0, 2400.0\n",
    "EPS = 1e-6\n",
    "\n",
    "def baseline_from_meters(d):\n",
    "    if len(d) == 0:\n",
    "        return None\n",
    "    d = np.asarray(d, dtype=float)\n",
    "    d_sorted = np.sort(d)\n",
    "    inv = 1.0 / (d_sorted + EPS)\n",
    "    return [\n",
    "        float(len(d_sorted)),               # count\n",
    "        float(d_sorted.mean()),             # mean\n",
    "        float(d_sorted.min()),              # min\n",
    "        float(d_sorted.max()),              # max\n",
    "        float(np.median(d_sorted)),         # median\n",
    "        float(d_sorted.std()),              # std\n",
    "        float(inv.mean()),                  # mean_inv\n",
    "        float(inv.max()),                   # max_inv\n",
    "        float(inv.sum()),                   # sum_inv\n",
    "        float((d_sorted <= R1).mean()),     # frac <= 600m\n",
    "        float((d_sorted <= R2).mean()),     # frac <= 1200m\n",
    "        float((d_sorted <= R3).mean()),     # frac <= 2400m\n",
    "    ]\n",
    "\n",
    "def collect_context(shard_glob, ctx_key_guess):\n",
    "    rows = {}\n",
    "    for p in sorted(SHARD_DIR.glob(shard_glob)):\n",
    "        with open(p, \"rb\") as f:\n",
    "            d = pickle.load(f)  # dict[int -> graphs-dict or Data]\n",
    "        for aid, v in d.items():\n",
    "            if isinstance(v, dict):\n",
    "                # try explicit key, else first non-None\n",
    "                g = v.get(ctx_key_guess)\n",
    "                if g is None:\n",
    "                    g = next((gg for gg in v.values() if gg is not None), None)\n",
    "            else:\n",
    "                g = v\n",
    "            if g is None:\n",
    "                rows[aid] = None\n",
    "                continue\n",
    "            ea = g.edge_attr.view(-1).cpu().numpy()  # meters\n",
    "            rows[aid] = baseline_from_meters(ea.tolist())\n",
    "    return rows\n",
    "\n",
    "metro = collect_context(\"METROSHARD_*.pkl\", \"metro\")\n",
    "bus   = collect_context(\"BUSSHARD_*.pkl\",   \"bus\")\n",
    "\n",
    "# Merge to a single dataframe; store vectors as JSON strings for now\n",
    "all_ids = sorted(set(metro.keys()) | set(bus.keys()))\n",
    "out = []\n",
    "for aid in all_ids:\n",
    "    rec = {\"id\": int(aid)}\n",
    "    rec[\"emb_metro\"] = json.dumps(metro.get(aid)) if metro.get(aid) is not None else None\n",
    "    rec[\"emb_bus\"]   = json.dumps(bus.get(aid))   if bus.get(aid)   is not None else None\n",
    "    out.append(rec)\n",
    "\n",
    "df = pd.DataFrame(out)\n",
    "df.to_csv(OUT_CSV, index=False)\n",
    "print(f\"✅ wrote {OUT_CSV} with {len(df)} rows \"\n",
    "      f\"(metro non-null: {df['emb_metro'].notna().sum()}, bus non-null: {df['emb_bus'].notna().sum()})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f89c176",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
