{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "27746be0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Notebook: Unsupervised GNN embeddings for POI / Metro / Bus (Python 3.9)\n",
    "# ======================================================================\n",
    "# Goals\n",
    "# - Train a tiny self-supervised encoder on exported star graphs\n",
    "# - Produce D-dim embeddings (8–16) per (apartment × context)\n",
    "# - Save a CSV: one row per apartment, 13 columns with JSON-serialized vectors\n",
    "# - Keep None where a context has no graph (we'll impute later if desired)\n",
    "#\n",
    "# Contexts: 11 POI classes (from 'shard_*.pkl') + Metro (METROSHARD_*.pkl) + Bus (BUSSHARD_*.pkl)\n",
    "# Python 3.9 compatible typing & APIs.\n",
    "\n",
    "# ============================\n",
    "# Cell 1 — Imports & Config\n",
    "# ============================\n",
    "import os, json, math, time, pickle\n",
    "from typing import Optional, Dict, List, Tuple, Set\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import GraphConv\n",
    "\n",
    "# Device\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Device:', DEVICE)\n",
    "\n",
    "# Embedding size (8–16)\n",
    "EMB_DIM = 12\n",
    "HIDDEN = 32\n",
    "EPOCHS = 5          # small & fast; increase if needed\n",
    "BATCH_SIZE = 128    # number of graphs per batch (increase if RAM allows)\n",
    "LR = 1e-3\n",
    "\n",
    "# Input shards directory\n",
    "SHARD_DIR = Path('Graph_data')\n",
    "\n",
    "# Provide explicit shard file names (as given by the user)\n",
    "GENERAL_SHARDS = [\n",
    "    'shard_20250823_193931_1555433697-1539080245.pkl',\n",
    "    'shard_20250823_195306_2853468746-2862582048.pkl',\n",
    "    'shard_20250823_212915_1575719893-1579917449.pkl',\n",
    "    'shard_20250823_224953_1586671181-1586639873.pkl',\n",
    "    'shard_20250824_001026_1584809495-1548097259.pkl',\n",
    "]\n",
    "METRO_SHARDS = [\n",
    "    'METROSHARD_20250825_211657_1555433697-2854216564.pkl',\n",
    "    'METROSHARD_20250826_091005_1553843137-1548097259.pkl',\n",
    "]\n",
    "BUS_SHARDS = [\n",
    "    'BUSSHARD_20250826_122800_1555433697-1584388845.pkl',\n",
    "    'BUSSHARD_20250826_153430_2862820058-1548097259.pkl',\n",
    "]\n",
    "\n",
    "# Output CSV path\n",
    "OUT_CSV = 'apartment_embeddings.csv'\n",
    "\n",
    "# Context keys (13 total)\n",
    "CLASSES = [\n",
    "    'sport_and_leisure','medical','education_prim','veterinary','food_and_drink_stores',\n",
    "    'arts_and_entertainment','food_and_drink','park_like','security','religion','education_sup'\n",
    "]\n",
    "CONTEXT_KEYS = CLASSES + ['metro', 'bus']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "00e64246",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected items: 307255\n",
      "COMMON_F: 9 | INPUT_DIM: 12\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# Cell 2 — Load & unify graph items\n",
    "# ================================\n",
    "# We will build a list of items: (apt_id, context_key, Data)\n",
    "# Data may have different feature widths; we'll pad to the max later.\n",
    "\n",
    "def load_pickle(path: Path):\n",
    "    with open(path, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "\n",
    "def collect_items() -> List[Tuple[int, str, Data]]:\n",
    "    items: List[Tuple[int, str, Data]] = []\n",
    "\n",
    "    # General POI shards: dict[int -> dict[str -> Data|None]]\n",
    "    for name in GENERAL_SHARDS:\n",
    "        p = SHARD_DIR / name\n",
    "        if not p.exists():\n",
    "            print(f\"[warn] missing general shard: {p}\")\n",
    "            continue\n",
    "        part = load_pickle(p)\n",
    "        for apt_id, gdict in part.items():\n",
    "            for cls in CLASSES:\n",
    "                g = gdict.get(cls)\n",
    "                if g is None:\n",
    "                    continue\n",
    "                # ensure a context key for downstream\n",
    "                g.poi_tag = cls\n",
    "                items.append((apt_id, cls, g))\n",
    "\n",
    "    # Metro shards: dict[int -> Data]\n",
    "    for name in METRO_SHARDS:\n",
    "        p = SHARD_DIR / name\n",
    "        if not p.exists():\n",
    "            print(f\"[warn] missing metro shard: {p}\")\n",
    "            continue\n",
    "        part = load_pickle(p)\n",
    "        for apt_id, g in part.items():\n",
    "            if isinstance(g, Data):\n",
    "                g.poi_tag = 'metro'\n",
    "                items.append((apt_id, 'metro', g))\n",
    "\n",
    "    # Bus shards: dict[int -> Data]\n",
    "    for name in BUS_SHARDS:\n",
    "        p = SHARD_DIR / name\n",
    "        if not p.exists():\n",
    "            print(f\"[warn] missing bus shard: {p}\")\n",
    "            continue\n",
    "        part = load_pickle(p)\n",
    "        for apt_id, g in part.items():\n",
    "            if isinstance(g, Data):\n",
    "                g.poi_tag = 'bus'\n",
    "                items.append((apt_id, 'bus', g))\n",
    "\n",
    "    print(f\"Collected items: {len(items)}\")\n",
    "    return items\n",
    "\n",
    "\n",
    "items = collect_items()\n",
    "\n",
    "# Determine common feature width for padding\n",
    "feat_dims = [g.x.size(1) for (_, _, g) in items]\n",
    "COMMON_F = max(feat_dims) if feat_dims else 0\n",
    "# Load apt coords and standardize\n",
    "DF_DEPTOS = pd.read_csv('Datasets/dataset_final.csv')[['id', 'latitud', 'longitud']].dropna()\n",
    "lat_mean, lat_std = DF_DEPTOS['latitud'].mean(), DF_DEPTOS['latitud'].std() + 1e-8\n",
    "lon_mean, lon_std = DF_DEPTOS['longitud'].mean(), DF_DEPTOS['longitud'].std() + 1e-8\n",
    "\n",
    "APT2COORD = {int(r.id): ((r.latitud - lat_mean)/lat_std, (r.longitud - lon_mean)/lon_std)\n",
    "             for r in DF_DEPTOS.itertuples(index=False)}\n",
    "\n",
    "# New input width = base COMMON_F + 1 (distance scalar) + 2 (lat_z, lon_z)\n",
    "INPUT_DIM = COMMON_F + 1 + 2\n",
    "print(\"COMMON_F:\", COMMON_F, \"| INPUT_DIM:\", INPUT_DIM)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a8cbc7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# Cell 3 — Dataset & collate utils\n",
    "# ================================\n",
    "def augment_with_distance_feature(g: Data) -> Data:\n",
    "    num_nodes = g.num_nodes\n",
    "    dist_feat = torch.zeros(num_nodes, 1, dtype=g.x.dtype)\n",
    "    if g.edge_attr is not None and g.edge_index is not None:\n",
    "        _, dst = g.edge_index\n",
    "        w = g.edge_attr.view(-1)\n",
    "        dist_feat[dst] = w.unsqueeze(1)\n",
    "    g.x = torch.cat([g.x, dist_feat], dim=1)\n",
    "    return g\n",
    "\n",
    "def augment_with_apt_coords(g: Data, apt_id: int, apt_idx: int) -> Data:\n",
    "    \"\"\"Append 2-dim feature; fill apt row with (lat_z, lon_z), zeros for others.\"\"\"\n",
    "    coords = APT2COORD.get(int(apt_id), (0.0, 0.0))\n",
    "    add = torch.zeros((g.num_nodes, 2), dtype=g.x.dtype)\n",
    "    add[apt_idx, 0] = float(coords[0])\n",
    "    add[apt_idx, 1] = float(coords[1])\n",
    "    g.x = torch.cat([g.x, add], dim=1)\n",
    "    return g\n",
    "\n",
    "class StarItemDataset(Dataset):\n",
    "    def __init__(self, items: List[Tuple[int, str, Data]], common_f: int):\n",
    "        self.items = items\n",
    "        self.common_f = common_f\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.items)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Data:\n",
    "        apt_id, ctx, g = self.items[idx]\n",
    "        # pad to COMMON_F\n",
    "        if g.x.size(1) < self.common_f:\n",
    "            pad_w = self.common_f - g.x.size(1)\n",
    "            pad = torch.zeros((g.x.size(0), pad_w), dtype=g.x.dtype)\n",
    "            g = Data(x=torch.cat([g.x, pad], dim=1),\n",
    "                     edge_index=g.edge_index, edge_attr=g.edge_attr)\n",
    "        # locate apartment node\n",
    "        apt_mask = (g.x[:, 0] > 0.5)\n",
    "        apt_idx = int(torch.nonzero(apt_mask, as_tuple=False)[0].item()) if apt_mask.any() else 0\n",
    "        # add distance scalar (+1)\n",
    "        g = augment_with_distance_feature(g)\n",
    "        # add coords (+2) on apt node only\n",
    "        g = augment_with_apt_coords(g, int(apt_id), apt_idx)\n",
    "        # enforce exact INPUT_DIM (pad/trunc for safety)\n",
    "        f = g.x.size(1)\n",
    "        if f < INPUT_DIM:\n",
    "            pad = torch.zeros((g.num_nodes, INPUT_DIM - f), dtype=g.x.dtype)\n",
    "            g.x = torch.cat([g.x, pad], dim=1)\n",
    "        elif f > INPUT_DIM:\n",
    "            g.x = g.x[:, :INPUT_DIM]\n",
    "        # metadata\n",
    "        g.apt_id = int(apt_id)\n",
    "        g.poi_tag = ctx\n",
    "        g.apt_idx = apt_idx\n",
    "        return g\n",
    "\n",
    "dataset = StarItemDataset(items, COMMON_F)\n",
    "loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "28425b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# Cell 4 — Model (tiny GCN)\n",
    "# ============================\n",
    "class TinyGraphConv(nn.Module):\n",
    "    def __init__(self, in_dim: int, hidden: int, out_dim: int):\n",
    "        super().__init__()\n",
    "        self.conv1 = GraphConv(in_dim, hidden)\n",
    "        self.conv2 = GraphConv(hidden, out_dim)\n",
    "        self.act = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "        self.W = nn.Linear(out_dim, out_dim, bias=False)  # bilinear decoder for edges\n",
    "        # auxiliary heads from apartment embedding:\n",
    "        self.head_deg   = nn.Linear(out_dim, 1)\n",
    "        self.head_meanw = nn.Linear(out_dim, 1)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, ei = data.x, data.edge_index\n",
    "        ew = data.edge_attr.view(-1) if data.edge_attr is not None else None\n",
    "        h = self.conv1(x, ei, edge_weight=ew)\n",
    "        h = self.act(h)\n",
    "        h = self.dropout(h)\n",
    "        h = self.conv2(h, ei, edge_weight=ew)\n",
    "        return h\n",
    "\n",
    "    def score(self, h_src: torch.Tensor, h_dst: torch.Tensor) -> torch.Tensor:\n",
    "        return (self.W(h_src) * h_dst).sum(dim=-1)\n",
    "\n",
    "model = TinyGraphConv(INPUT_DIM, HIDDEN, EMB_DIM).to(DEVICE)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9937f8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert getattr(model.conv1.lin_rel, 'weight').shape[1] == INPUT_DIM, \\\n",
    "    f\"Model expects {getattr(model.conv1.lin_rel, 'weight').shape[1]} features, but INPUT_DIM={INPUT_DIM}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8c302f57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | loss=0.031822 | 662.06s\n",
      "Epoch 02 | loss=0.027477 | 500.53s\n",
      "Epoch 03 | loss=0.027400 | 519.82s\n",
      "Epoch 04 | loss=0.027402 | 550.55s\n",
      "Epoch 05 | loss=0.027386 | 545.43s\n"
     ]
    }
   ],
   "source": [
    "# ======================================\n",
    "# Cell 5 — Train (edge weight reconstruction)\n",
    "# ======================================\n",
    "def train_one_epoch() -> float:\n",
    "    model.train()\n",
    "    total = 0.0\n",
    "    count = 0\n",
    "    for big in loader:  # big: Batch\n",
    "        big = big.to(DEVICE)\n",
    "        opt.zero_grad()\n",
    "\n",
    "        H = model(big)   # [N_total, EMB_DIM]\n",
    "        src, dst = big.edge_index\n",
    "\n",
    "        # apartment global indices per graph\n",
    "        num_graphs = big.ptr.numel() - 1\n",
    "        apt_global = []\n",
    "        for i in range(num_graphs):\n",
    "            base = int(big.ptr[i].item())\n",
    "            aidx = int(big.apt_idx[i].item())\n",
    "            apt_global.append(base + aidx)\n",
    "        apt_global = torch.tensor(apt_global, device=big.x.device, dtype=src.dtype)  # [G]\n",
    "\n",
    "        # mask edges whose src is the apartment in its graph\n",
    "        mask = (src.view(-1, 1) == apt_global.view(1, -1)).any(dim=1)\n",
    "        src_pos = src[mask]\n",
    "        dst_pos = dst[mask]\n",
    "        if src_pos.numel() == 0:\n",
    "            continue\n",
    "\n",
    "        targets_all = edge_weights_for_training(big)  # [E] in [0,1]\n",
    "        targets_pos = targets_all[mask]\n",
    "\n",
    "        # positive edge loss\n",
    "        scores_pos = model.score(H[src_pos], H[dst_pos])\n",
    "        preds_pos  = torch.sigmoid(scores_pos)\n",
    "        loss_pos = F.mse_loss(preds_pos, targets_pos)\n",
    "\n",
    "        # negatives: sample non-neighbors per graph\n",
    "        K = 5\n",
    "        neg_src_list, neg_dst_list = [], []\n",
    "        for i in range(num_graphs):\n",
    "            base = int(big.ptr[i].item())\n",
    "            end  = int(big.ptr[i+1].item())\n",
    "            a_gi = base + int(big.apt_idx[i].item())\n",
    "            nei_mask = (src_pos == a_gi)\n",
    "            true_dsts = set(dst_pos[nei_mask].tolist())\n",
    "            candidates = [j for j in range(base, end) if j != a_gi and j not in true_dsts]\n",
    "            if not candidates:\n",
    "                continue\n",
    "            pick = np.random.choice(candidates, size=min(K, len(candidates)), replace=False)\n",
    "            neg_src_list.extend([a_gi] * len(pick))\n",
    "            neg_dst_list.extend([int(x) for x in pick])\n",
    "\n",
    "        if neg_src_list:\n",
    "            neg_src = torch.tensor(neg_src_list, device=big.x.device, dtype=src.dtype)\n",
    "            neg_dst = torch.tensor(neg_dst_list, device=big.x.device, dtype=dst.dtype)\n",
    "            scores_neg = model.score(H[neg_src], H[neg_dst])\n",
    "            preds_neg  = torch.sigmoid(scores_neg)\n",
    "            loss_neg = F.mse_loss(preds_neg, torch.zeros_like(preds_neg))\n",
    "        else:\n",
    "            loss_neg = torch.tensor(0.0, device=big.x.device)\n",
    "\n",
    "        # --- auxiliary graph-level targets from apartment embedding\n",
    "        # deg: count of outgoing apt edges per graph\n",
    "        deg_targets = []\n",
    "        meanw_targets = []\n",
    "        for i in range(num_graphs):\n",
    "            base = int(big.ptr[i].item())\n",
    "            a_gi = base + int(big.apt_idx[i].item())\n",
    "            sel = (src_pos == a_gi)\n",
    "            deg_i = int(sel.sum().item())\n",
    "            deg_targets.append(deg_i)\n",
    "            if deg_i > 0:\n",
    "                meanw_targets.append(float(targets_pos[sel].mean().item()))\n",
    "            else:\n",
    "                meanw_targets.append(0.0)\n",
    "\n",
    "        deg_targets   = torch.tensor(deg_targets, device=big.x.device, dtype=torch.float).view(-1, 1)\n",
    "        meanw_targets = torch.tensor(meanw_targets, device=big.x.device, dtype=torch.float).view(-1, 1)\n",
    "\n",
    "        # normalize degree for stability (log1p)\n",
    "        deg_targets_norm = torch.log1p(deg_targets) / 4.0  # rough scale\n",
    "        apt_h = H[apt_global]  # [G, EMB_DIM]\n",
    "\n",
    "        pred_deg   = model.head_deg(apt_h)\n",
    "        pred_meanw = torch.sigmoid(model.head_meanw(apt_h))\n",
    "\n",
    "        loss_deg   = F.mse_loss(pred_deg, deg_targets_norm)\n",
    "        loss_meanw = F.mse_loss(pred_meanw, meanw_targets)\n",
    "\n",
    "        loss = loss_pos + 0.5 * loss_neg + 0.2 * loss_deg + 0.2 * loss_meanw\n",
    "\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        total += float(loss.item())\n",
    "        count += 1\n",
    "\n",
    "    return total / max(count, 1)\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    t0 = time.time()\n",
    "    loss = train_one_epoch()\n",
    "    print(f\"Epoch {epoch:02d} | loss={loss:.6f} | {time.time()-t0:.2f}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "198fe08f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total apartments with at least one context: 25211\n"
     ]
    }
   ],
   "source": [
    "# ======================================\n",
    "# Cell 6 — Inference: per (apt, context) embedding\n",
    "# ======================================\n",
    "@torch.no_grad()\n",
    "def embed_graph(g: Data, apt_id: int) -> np.ndarray:\n",
    "    # pad to COMMON_F\n",
    "    if g.x.size(1) < COMMON_F:\n",
    "        pad_w = COMMON_F - g.x.size(1)\n",
    "        pad = torch.zeros((g.x.size(0), pad_w), dtype=g.x.dtype)\n",
    "        g = Data(x=torch.cat([g.x, pad], dim=1),\n",
    "                 edge_index=g.edge_index, edge_attr=g.edge_attr)\n",
    "    # apartment index\n",
    "    apt_mask = (g.x[:, 0] > 0.5)\n",
    "    aidx = int(torch.nonzero(apt_mask, as_tuple=False)[0].item()) if apt_mask.any() else 0\n",
    "    # add distance and coords\n",
    "    g = augment_with_distance_feature(g)\n",
    "    g = augment_with_apt_coords(g, int(apt_id), aidx)\n",
    "    # enforce INPUT_DIM\n",
    "    f = g.x.size(1)\n",
    "    if f < INPUT_DIM:\n",
    "        pad = torch.zeros((g.num_nodes, INPUT_DIM - f), dtype=g.x.dtype)\n",
    "        g.x = torch.cat([g.x, pad], dim=1)\n",
    "    elif f > INPUT_DIM:\n",
    "        g.x = g.x[:, :INPUT_DIM]\n",
    "\n",
    "    g = g.to(DEVICE)\n",
    "    H = model(g)\n",
    "    return H[aidx].cpu().numpy()\n",
    "\n",
    "\n",
    "# We want one embedding per (apartment × context). Some contexts may be missing.\n",
    "# We'll collect into: emb_map[apt_id][context] = list or None\n",
    "emb_map: Dict[int, Dict[str, Optional[List[float]]]] = {}\n",
    "\n",
    "# Prepare a per-apartment buckets for faster inference\n",
    "from collections import defaultdict\n",
    "apt_ctx_graphs: Dict[int, Dict[str, Data]] = defaultdict(dict)\n",
    "for apt_id, ctx, g in items:\n",
    "    # pad if needed\n",
    "    if g.x.size(1) < COMMON_F:\n",
    "        pad_w = COMMON_F - g.x.size(1)\n",
    "        pad = torch.zeros((g.x.size(0), pad_w), dtype=g.x.dtype)\n",
    "        g = Data(x=torch.cat([g.x, pad], dim=1), edge_index=g.edge_index, edge_attr=g.edge_attr)\n",
    "        g.apt_id = apt_id\n",
    "        g.poi_tag = ctx\n",
    "    apt_ctx_graphs[apt_id][ctx] = g\n",
    "\n",
    "# Collect all apartment IDs seen across shards\n",
    "all_apt_ids: Set[int] = set([aid for (aid, _, _) in items])\n",
    "\n",
    "print('Total apartments with at least one context:', len(all_apt_ids))\n",
    "\n",
    "# Inference loop (single-threaded, stable)\n",
    "model.eval()\n",
    "for aid in sorted(all_apt_ids):\n",
    "    emb_map[aid] = {}\n",
    "    for ctx in CONTEXT_KEYS:\n",
    "        g = apt_ctx_graphs[aid].get(ctx)\n",
    "        if g is None:\n",
    "            emb_map[aid][ctx] = None\n",
    "        else:\n",
    "            vec = embed_graph(g, aid)\n",
    "            # store as python list for JSON serialization\n",
    "            emb_map[aid][ctx] = [float(x) for x in vec.tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "fe76dffe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: (25211, 14)\n",
      "Saved CSV -> apartment_embeddings.csv\n",
      "           id                              emb_sport_and_leisure  \\\n",
      "0  1359204515  [0.11949535459280014, 8.156523108482361e-05, -...   \n",
      "1  1366496843  [0.14379562437534332, 0.0005593057721853256, -...   \n",
      "2  1367599797  [0.2016446441411972, -1.3947486877441406e-05, ...   \n",
      "\n",
      "                                         emb_medical  \\\n",
      "0  [0.11949535459280014, 8.156523108482361e-05, -...   \n",
      "1  [0.14379562437534332, 0.0005593057721853256, -...   \n",
      "2  [0.2016446441411972, -1.3947486877441406e-05, ...   \n",
      "\n",
      "                                  emb_education_prim  \\\n",
      "0  [0.11949535459280014, 8.156523108482361e-05, -...   \n",
      "1  [0.14379562437534332, 0.0005593057721853256, -...   \n",
      "2  [0.2016446441411972, -1.3947486877441406e-05, ...   \n",
      "\n",
      "                                      emb_veterinary  \\\n",
      "0  [0.11949535459280014, 8.156523108482361e-05, -...   \n",
      "1  [0.14379562437534332, 0.0005593057721853256, -...   \n",
      "2  [0.2016446441411972, -1.3947486877441406e-05, ...   \n",
      "\n",
      "                           emb_food_and_drink_stores  \\\n",
      "0  [0.11949535459280014, 8.156523108482361e-05, -...   \n",
      "1  [0.14379562437534332, 0.0005593057721853256, -...   \n",
      "2  [0.2016446441411972, -1.3947486877441406e-05, ...   \n",
      "\n",
      "                          emb_arts_and_entertainment  \\\n",
      "0  [0.11949535459280014, 8.156523108482361e-05, -...   \n",
      "1  [0.14379562437534332, 0.0005593057721853256, -...   \n",
      "2  [0.2016446441411972, -1.3947486877441406e-05, ...   \n",
      "\n",
      "                                  emb_food_and_drink  \\\n",
      "0  [0.11949535459280014, 8.156523108482361e-05, -...   \n",
      "1  [0.14379562437534332, 0.0005593057721853256, -...   \n",
      "2  [0.2016446441411972, -1.3947486877441406e-05, ...   \n",
      "\n",
      "                                       emb_park_like  \\\n",
      "0  [0.11949536204338074, 8.156523108482361e-05, -...   \n",
      "1  [0.14379562437534332, 0.0005593057721853256, -...   \n",
      "2  [0.2016446441411972, -1.394655555486679e-05, -...   \n",
      "\n",
      "                                        emb_security  \\\n",
      "0  [0.11949535459280014, 8.156523108482361e-05, -...   \n",
      "1  [0.14379562437534332, 0.0005593057721853256, -...   \n",
      "2  [0.2016446441411972, -1.3947486877441406e-05, ...   \n",
      "\n",
      "                                        emb_religion  \\\n",
      "0  [0.11949536204338074, 8.156523108482361e-05, -...   \n",
      "1  [0.14379562437534332, 0.0005593057721853256, -...   \n",
      "2  [0.2016446441411972, -1.3947486877441406e-05, ...   \n",
      "\n",
      "                                   emb_education_sup  \\\n",
      "0  [0.11949535459280014, 8.156523108482361e-05, -...   \n",
      "1  [0.14379562437534332, 0.0005593057721853256, -...   \n",
      "2  [0.2016446441411972, -1.3947486877441406e-05, ...   \n",
      "\n",
      "                                           emb_metro  \\\n",
      "0  [0.11949536204338074, 8.156523108482361e-05, -...   \n",
      "1  [0.14379562437534332, 0.0005593057721853256, -...   \n",
      "2  [0.2016446441411972, -1.394655555486679e-05, -...   \n",
      "\n",
      "                                             emb_bus  \n",
      "0  [0.11949535459280014, 8.156523108482361e-05, -...  \n",
      "1  [0.14379562437534332, 0.0005593057721853256, -...  \n",
      "2  [0.2016446441411972, -1.3947486877441406e-05, ...  \n"
     ]
    }
   ],
   "source": [
    "# ======================================\n",
    "# Cell 7 — Build CSV with 13 columns of JSON vectors\n",
    "# ======================================\n",
    "\n",
    "rows = []\n",
    "for aid in sorted(emb_map.keys()):\n",
    "    row = {'id': aid}\n",
    "    for ctx in CONTEXT_KEYS:\n",
    "        val = emb_map[aid].get(ctx)\n",
    "        row[f'emb_{ctx}'] = None if val is None else json.dumps(val)\n",
    "    rows.append(row)\n",
    "\n",
    "out_df = pd.DataFrame(rows)\n",
    "print('Output shape:', out_df.shape)\n",
    "out_df.to_csv(OUT_CSV, index=False)\n",
    "print('Saved CSV ->', OUT_CSV)\n",
    "\n",
    "# Quick peek\n",
    "print(out_df.head(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7d93f9fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['[0.11949535459280014, 8.156523108482361e-05, -0.018419355154037476, 0.1900814026594162, -1.2534259557724, -0.4860982894897461, 0.4763750731945038, 0.7971335649490356, 1.1067334413528442, 0.41799676418304443, -0.143602192401886, 0.6868269443511963]',\n",
       "       '[0.14379562437534332, 0.0005593057721853256, -0.047350674867630005, 0.1632624715566635, -1.1872875690460205, -0.4749646484851837, 0.45391929149627686, 0.7937610149383545, 1.0362839698791504, 0.36910170316696167, -0.07874393463134766, 0.6427090167999268]',\n",
       "       '[0.2016446441411972, -1.3947486877441406e-05, -0.0971163809299469, 0.1933460235595703, -1.1318022012710571, -0.5013339519500732, 0.46635499596595764, 0.7998327016830444, 1.011732816696167, 0.3695237636566162, 0.008165597915649414, 0.6156975030899048]',\n",
       "       ...,\n",
       "       '[0.16911165416240692, -0.0002922527492046356, -0.08437836170196533, 0.08540749549865723, -1.1322500705718994, -0.4804706275463104, 0.48595064878463745, 0.7938352823257446, 0.9984339475631714, 0.3065822720527649, -0.029645323753356934, 0.5973696112632751]',\n",
       "       '[0.253903329372406, -0.0005901893600821495, -0.14381155371665955, 0.21493291854858398, -1.0751408338546753, -0.5226873755455017, 0.4777642488479614, 0.802477240562439, 0.9839224815368652, 0.363788902759552, 0.09012830257415771, 0.5868282318115234]',\n",
       "       '[0.09733504056930542, 0.0003720466047525406, -0.00014734268188476562, 0.1746649444103241, -1.2704555988311768, -0.4738059341907501, 0.46888071298599243, 0.794223427772522, 1.110205888748169, 0.4129941463470459, -0.17409801483154297, 0.6939413547515869]'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_df['emb_medical'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "343d478c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['[0.11949535459280014, 8.156523108482361e-05, -0.018419355154037476, 0.1900814026594162, -1.2534259557724, -0.4860982894897461, 0.4763750731945038, 0.7971335649490356, 1.1067334413528442, 0.41799676418304443, -0.143602192401886, 0.6868269443511963]',\n",
       "       '[0.14379562437534332, 0.0005593057721853256, -0.047350674867630005, 0.1632624715566635, -1.1872875690460205, -0.4749646484851837, 0.45391929149627686, 0.7937610149383545, 1.0362839698791504, 0.36910170316696167, -0.07874393463134766, 0.6427090167999268]',\n",
       "       '[0.2016446441411972, -1.3947486877441406e-05, -0.0971163809299469, 0.1933460235595703, -1.1318022012710571, -0.5013339519500732, 0.46635499596595764, 0.7998327016830444, 1.011732816696167, 0.3695237636566162, 0.008165597915649414, 0.6156975030899048]',\n",
       "       ...,\n",
       "       '[0.16911165416240692, -0.0002922527492046356, -0.08437836170196533, 0.08540749549865723, -1.1322500705718994, -0.4804706275463104, 0.48595064878463745, 0.7938352823257446, 0.9984339475631714, 0.3065822720527649, -0.029645323753356934, 0.5973696112632751]',\n",
       "       '[0.253903329372406, -0.0005901893600821495, -0.14381155371665955, 0.21493291854858398, -1.0751408338546753, -0.5226873755455017, 0.4777642488479614, 0.802477240562439, 0.9839224815368652, 0.363788902759552, 0.09012830257415771, 0.5868282318115234]',\n",
       "       '[0.09733504056930542, 0.0003720466047525406, -0.00014734268188476562, 0.1746649444103241, -1.2704555988311768, -0.4738059341907501, 0.46888071298599243, 0.794223427772522, 1.110205888748169, 0.4129941463470459, -0.17409801483154297, 0.6939413547515869]'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_df['emb_bus'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e90673a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
