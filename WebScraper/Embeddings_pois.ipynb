{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac31d2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file: notebooks/per_context_embeddings.py\n",
    "\"\"\"\n",
    "Build 13 per-context embeddings (11 POI classes + metro + bus) for each apartment.\n",
    "- Default: FAST BASELINE (no training). Always distinct across contexts when data differs.\n",
    "- Optional: Per-context GNN with adjustable EPOCHS and NEG_K (slower).\n",
    "\n",
    "Outputs a CSV with columns: id, emb_<context> (JSON vector or null).\n",
    "Looks for shards in Graph_data/ with these patterns:\n",
    "  - shard_*.pkl           (general POI classes)\n",
    "  - METROSHARD_*.pkl      (metro)\n",
    "  - BUSSHARD_*.pkl        (bus)\n",
    "\"\"\"\n",
    "\n",
    "# ==============================\n",
    "# Cell 1 — Imports & switches\n",
    "# ==============================\n",
    "import os, json, time, pickle, glob\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple, Set\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import GraphConv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f87bba99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Mode: \"baseline\" (fast, no training) or \"gnn\" (train per context)\n",
    "MODE = \"baseline\"   # change to \"gnn\" to train encoders per context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce757a9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mode': 'baseline', 'device': 'cuda'}\n"
     ]
    }
   ],
   "source": [
    "# ==============================\n",
    "# Cell 2 — Paths & hyperparams\n",
    "# ==============================\n",
    "ROOT = Path(\"Graph_data\")  # folder containing exported shards\n",
    "OUT_CSV = \"apartment_embeddings_per_context.csv\"\n",
    "\n",
    "# Contexts\n",
    "CLASSES = [\n",
    "    'sport_and_leisure','medical','education_prim','veterinary',\n",
    "    'food_and_drink_stores','arts_and_entertainment','food_and_drink',\n",
    "    'park_like','security','religion','education_sup'\n",
    "]\n",
    "ALL_CONTEXTS = CLASSES + ['metro','bus']\n",
    "\n",
    "# GNN knobs (used only when MODE==\"gnn\")\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "EMB_DIM = 12\n",
    "HIDDEN  = 32\n",
    "EPOCHS  = 8         # increase for better quality\n",
    "BATCH_SZ = 512      # larger is faster\n",
    "LR = 1e-3\n",
    "WEIGHT_DECAY = 1e-4\n",
    "NEG_K = 8           # increase for stronger contrast\n",
    "\n",
    "print({\"mode\": MODE, \"device\": str(DEVICE)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d1982e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found: general=5, metro=2, bus=2\n"
     ]
    }
   ],
   "source": [
    "# ======================================\n",
    "# Cell 3 — Load exported shard helpers\n",
    "# ======================================\n",
    "\n",
    "def load_pickle(p: Path):\n",
    "    with open(p, \"rb\") as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "# General shards hold: dict[apt_id] -> dict[class] -> Data|None\n",
    "# Metro/Bus shards hold: dict[apt_id] -> Data|None\n",
    "\n",
    "def find_shards() -> Tuple[List[Path], List[Path], List[Path]]:\n",
    "    gens  = sorted((ROOT).glob(\"shard_*.pkl\"))\n",
    "    metro = sorted((ROOT).glob(\"METROSHARD_*.pkl\"))\n",
    "    bus   = sorted((ROOT).glob(\"BUSSHARD_*.pkl\"))\n",
    "    print(f\"found: general={len(gens)}, metro={len(metro)}, bus={len(bus)}\")\n",
    "    return gens, metro, bus\n",
    "\n",
    "GEN_SHARDS, METRO_SHARDS, BUS_SHARDS = find_shards()\n",
    "\n",
    "\n",
    "def load_general_items_for_class(ctx: str) -> List[Tuple[int, Data]]:\n",
    "    items: List[Tuple[int, Data]] = []\n",
    "    for p in GEN_SHARDS:\n",
    "        part = load_pickle(p)\n",
    "        for aid, gdict in part.items():\n",
    "            g = gdict.get(ctx)\n",
    "            if isinstance(g, Data):\n",
    "                items.append((int(aid), g))\n",
    "    return items\n",
    "\n",
    "\n",
    "def load_simple_items(shards: List[Path]) -> List[Tuple[int, Data]]:\n",
    "    items: List[Tuple[int, Data]] = []\n",
    "    for p in shards:\n",
    "        part = load_pickle(p)\n",
    "        for aid, g in part.items():\n",
    "            if isinstance(g, Data):\n",
    "                items.append((int(aid), g))\n",
    "    return items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dc01424b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================\n",
    "# Cell 4 — FAST baseline (no train)\n",
    "# ===================================\n",
    "\n",
    "def weights_from_graph(g: Data) -> Optional[torch.Tensor]:\n",
    "    if g is None or g.edge_attr is None or g.edge_attr.numel() == 0:\n",
    "        return None\n",
    "    w = g.edge_attr.view(-1).clone()\n",
    "    if (w > 1.0).any():  # meters → squash\n",
    "        w = 1.0 / (1.0 + torch.clamp(w, min=0.0))\n",
    "    return torch.clamp(w, 0.0, 1.0)\n",
    "\n",
    "\n",
    "def star_stats_12d(w: torch.Tensor) -> np.ndarray:\n",
    "    E = float(w.numel())\n",
    "    if E == 0:\n",
    "        return np.full(12, np.nan, dtype=np.float32)\n",
    "    s1  = w.sum(); s2 = (w**2).sum(); s3 = (w**3).sum()\n",
    "    mean = float(s1 / E)\n",
    "    var  = float(s2 / E - mean**2)\n",
    "    std  = float(np.sqrt(max(var, 0.0)))\n",
    "    w_np = w.cpu().numpy()\n",
    "    w_sorted = np.sort(w_np)[::-1]\n",
    "    top1 = float(w_sorted[0])\n",
    "    top3_mean = float(w_sorted[:min(3, w_sorted.size)].mean())\n",
    "    tail = float((w_np > 0.75).mean())\n",
    "    mn = float(w_np.min()); mx = float(w_np.max())\n",
    "    bin2 = float((w_np > 0.5).mean())\n",
    "    return np.array([E, float(s1), float(s2), float(s3), mean, std, mn, mx, top1, top3_mean, tail, bin2], dtype=np.float32)\n",
    "\n",
    "\n",
    "def build_baseline_embeddings(out_csv: str) -> pd.DataFrame:\n",
    "    per_ctx: Dict[str, List[Tuple[int, Data]]] = {}\n",
    "    for ctx in CLASSES:\n",
    "        per_ctx[ctx] = load_general_items_for_class(ctx)\n",
    "        print(f\"[collect] {ctx}: {len(per_ctx[ctx])}\")\n",
    "    per_ctx['metro'] = load_simple_items(METRO_SHARDS); print(f\"[collect] metro: {len(per_ctx['metro'])}\")\n",
    "    per_ctx['bus']   = load_simple_items(BUS_SHARDS);   print(f\"[collect] bus:   {len(per_ctx['bus'])}\")\n",
    "\n",
    "    all_ids: Set[int] = set()\n",
    "    for ctx, items in per_ctx.items():\n",
    "        all_ids |= {aid for aid, _ in items}\n",
    "    all_ids = sorted(all_ids)\n",
    "    print(\"apartments total:\", len(all_ids))\n",
    "\n",
    "    ctx_embs: Dict[str, Dict[int, Optional[str]]] = {c: {} for c in ALL_CONTEXTS}\n",
    "    t0 = time.time()\n",
    "    for ctx in ALL_CONTEXTS:\n",
    "        items = dict(per_ctx.get(ctx, []))  # apt_id -> Data\n",
    "        for aid in all_ids:\n",
    "            g = items.get(aid)\n",
    "            if g is None:\n",
    "                ctx_embs[ctx][aid] = None\n",
    "            else:\n",
    "                w = weights_from_graph(g)\n",
    "                if w is None:\n",
    "                    ctx_embs[ctx][aid] = None\n",
    "                else:\n",
    "                    vec = star_stats_12d(w)\n",
    "                    ctx_embs[ctx][aid] = json.dumps([float(x) for x in vec.tolist()])\n",
    "    print(f\"baseline built in {time.time()-t0:.2f}s\")\n",
    "\n",
    "    rows = []\n",
    "    for aid in all_ids:\n",
    "        row = {\"id\": aid}\n",
    "        for ctx in ALL_CONTEXTS:\n",
    "            row[f\"emb_{ctx}\"] = ctx_embs[ctx].get(aid)\n",
    "        rows.append(row)\n",
    "    df = pd.DataFrame(rows)\n",
    "    df.to_csv(out_csv, index=False)\n",
    "    print(\"saved:\", out_csv, \"shape:\", df.shape)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ee3aac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================\n",
    "# Cell 5 — Optional: per-context GNN\n",
    "# ======================================\n",
    "class TinyGraphConv(nn.Module):\n",
    "    def __init__(self, in_dim: int, hidden: int, out_dim: int):\n",
    "        super().__init__()\n",
    "        self.conv1 = GraphConv(in_dim, hidden)\n",
    "        self.conv2 = GraphConv(hidden, out_dim)\n",
    "        self.act = nn.ReLU(); self.drop = nn.Dropout(0.1)\n",
    "        self.W = nn.Linear(out_dim, out_dim, bias=False)      # for edges\n",
    "        self.h_deg = nn.Linear(out_dim, 1)\n",
    "        self.h_mean = nn.Linear(out_dim, 1)\n",
    "\n",
    "    def forward(self, data: Data):\n",
    "        x, ei = data.x, data.edge_index\n",
    "        ew = data.edge_attr.view(-1) if data.edge_attr is not None else None\n",
    "        h = self.conv1(x, ei, edge_weight=ew); h = self.act(h); h = self.drop(h)\n",
    "        h = self.conv2(h, ei, edge_weight=ew)\n",
    "        return h\n",
    "\n",
    "    def score(self, h_src, h_dst):\n",
    "        return (self.W(h_src) * h_dst).sum(dim=-1)\n",
    "\n",
    "\n",
    "def add_distance_scalar(g: Data) -> Data:\n",
    "    n = g.num_nodes\n",
    "    dist = torch.zeros((n,1), dtype=g.x.dtype)\n",
    "    if g.edge_attr is not None and g.edge_index is not None:\n",
    "        _, dst = g.edge_index\n",
    "        w = g.edge_attr.view(-1)\n",
    "        dist[dst] = w.unsqueeze(1) if w.dim()==1 else w\n",
    "    g.x = torch.cat([g.x, dist], dim=1)\n",
    "    return g\n",
    "\n",
    "\n",
    "def common_feat_dim(items: List[Tuple[int, Data]]) -> int:\n",
    "    return max((g.x.size(1) for _, g in items), default=0)\n",
    "\n",
    "class StarDataset(Dataset):\n",
    "    def __init__(self, items: List[Tuple[int, Data]], base_f: int):\n",
    "        self.items = items; self.base_f = base_f; self.input_dim = base_f + 1\n",
    "    def __len__(self): return len(self.items)\n",
    "    def __getitem__(self, i: int) -> Data:\n",
    "        aid, g = self.items[i]\n",
    "        if g.x.size(1) < self.base_f:\n",
    "            pad = torch.zeros((g.num_nodes, self.base_f - g.x.size(1)), dtype=g.x.dtype)\n",
    "            g = Data(x=torch.cat([g.x, pad], dim=1), edge_index=g.edge_index, edge_attr=g.edge_attr)\n",
    "        g = add_distance_scalar(g)\n",
    "        if g.x.size(1) > self.input_dim:\n",
    "            g.x = g.x[:, :self.input_dim]\n",
    "        elif g.x.size(1) < self.input_dim:\n",
    "            pad = torch.zeros((g.num_nodes, self.input_dim - g.x.size(1)), dtype=g.x.dtype)\n",
    "            g.x = torch.cat([g.x, pad], dim=1)\n",
    "        apt_idx = int(torch.nonzero(g.x[:,0] > 0.5, as_tuple=False)[0].item()) if (g.x[:,0] > 0.5).any() else 0\n",
    "        g.apt_idx = apt_idx; g.apt_id = int(aid)\n",
    "        return g\n",
    "\n",
    "\n",
    "def edge_weights_for_training(g: Data) -> torch.Tensor:\n",
    "    w = g.edge_attr.view(-1)\n",
    "    if (w > 1.0).any():\n",
    "        w = 1.0 / (1.0 + torch.clamp(w, min=0.0))\n",
    "    return torch.clamp(w, 0.0, 1.0)\n",
    "\n",
    "\n",
    "def train_context(items: List[Tuple[int, Data]], name: str) -> Tuple[Optional[TinyGraphConv], Optional[StarDataset]]:\n",
    "    if not items:\n",
    "        print(f\"[{name}] no graphs; skip\")\n",
    "        return None, None\n",
    "    base_f = common_feat_dim(items)\n",
    "    ds = StarDataset(items, base_f)\n",
    "    dl = DataLoader(ds, batch_size=BATCH_SZ, shuffle=True)\n",
    "    model = TinyGraphConv(ds.input_dim, HIDDEN, EMB_DIM).to(DEVICE)\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "    def train_epoch() -> float:\n",
    "        model.train(); tot=0.0; cnt=0\n",
    "        for big in dl:\n",
    "            big = big.to(DEVICE); opt.zero_grad()\n",
    "            H = model(big); src, dst = big.edge_index\n",
    "            # apartment global indices\n",
    "            num_g = big.ptr.numel()-1\n",
    "            apt_gl = []\n",
    "            for i in range(num_g):\n",
    "                base = int(big.ptr[i].item()); aidx = int(big.apt_idx[i].item())\n",
    "                apt_gl.append(base + aidx)\n",
    "            apt_gl = torch.tensor(apt_gl, device=big.x.device, dtype=src.dtype)\n",
    "            mask = (src.view(-1,1) == apt_gl.view(1,-1)).any(dim=1)\n",
    "            if not mask.any():\n",
    "                continue\n",
    "            src_pos = src[mask]; dst_pos = dst[mask]\n",
    "            tgt_all = edge_weights_for_training(big); tgt_pos = tgt_all[mask]\n",
    "            # pos\n",
    "            pred_pos = torch.sigmoid(model.score(H[src_pos], H[dst_pos]))\n",
    "            loss_pos = F.mse_loss(pred_pos, tgt_pos)\n",
    "            # negs\n",
    "            neg_s, neg_d = [], []\n",
    "            for i in range(num_g):\n",
    "                base = int(big.ptr[i].item()); end=int(big.ptr[i+1].item())\n",
    "                a_gi = base + int(big.apt_idx[i].item())\n",
    "                true = set(dst_pos[(src_pos==a_gi)].tolist())\n",
    "                cands = [j for j in range(base,end) if j!=a_gi and j not in true]\n",
    "                if not cands: continue\n",
    "                pick = np.random.choice(cands, size=min(NEG_K,len(cands)), replace=False)\n",
    "                neg_s += [a_gi]*len(pick); neg_d += [int(x) for x in pick]\n",
    "            if neg_s:\n",
    "                neg_s = torch.tensor(neg_s, device=big.x.device, dtype=src.dtype)\n",
    "                neg_d = torch.tensor(neg_d, device=big.x.device, dtype=dst.dtype)\n",
    "                pred_neg = torch.sigmoid(model.score(H[neg_s], H[neg_d]))\n",
    "                loss_neg = F.mse_loss(pred_neg, torch.zeros_like(pred_neg))\n",
    "            else:\n",
    "                loss_neg = torch.tensor(0.0, device=big.x.device)\n",
    "            # aux: degree + mean weight from apt embedding\n",
    "            deg_t, mean_t = [], []\n",
    "            for i in range(num_g):\n",
    "                base = int(big.ptr[i].item()); a_gi = base + int(big.apt_idx[i].item())\n",
    "                sel = (src_pos == a_gi); d = int(sel.sum().item()); deg_t.append(d)\n",
    "                mean_t.append(float(tgt_pos[sel].mean().item()) if d>0 else 0.0)\n",
    "            deg_t = torch.tensor(deg_t, device=big.x.device, dtype=torch.float).view(-1,1)\n",
    "            mean_t= torch.tensor(mean_t,device=big.x.device, dtype=torch.float).view(-1,1)\n",
    "            deg_n = torch.log1p(deg_t)/4.0\n",
    "            apt_h = H[apt_gl]\n",
    "            loss_deg = F.mse_loss(model.h_deg(apt_h), deg_n)\n",
    "            loss_mean= F.mse_loss(torch.sigmoid(model.h_mean(apt_h)), mean_t)\n",
    "            loss = loss_pos + 0.5*loss_neg + 0.2*loss_deg + 0.2*loss_mean\n",
    "            loss.backward(); opt.step(); tot += float(loss.item()); cnt += 1\n",
    "        return tot/max(cnt,1)\n",
    "\n",
    "    print(f\"[{name}] graphs={len(ds)} in_dim={ds.input_dim}\")\n",
    "    for ep in range(1, EPOCHS+1):\n",
    "        t0=time.time(); L=train_epoch();\n",
    "        print(f\"[{name}] epoch {ep:02d} loss={L:.6f} ({time.time()-t0:.1f}s)\")\n",
    "    return model, ds\n",
    "\n",
    "@torch.no_grad()\n",
    "def embed_with_model(model: TinyGraphConv, g: Data, input_dim: int) -> np.ndarray:\n",
    "    # ensure exact input_dim\n",
    "    if g.x.size(1) < input_dim:\n",
    "        pad = torch.zeros((g.num_nodes, input_dim - g.x.size(1)), dtype=g.x.dtype)\n",
    "        g.x = torch.cat([g.x, pad], dim=1)\n",
    "    elif g.x.size(1) > input_dim:\n",
    "        g.x = g.x[:, :input_dim]\n",
    "    g = g.to(DEVICE)\n",
    "    H = model(g)\n",
    "    aidx = int(torch.nonzero(g.x[:,0] > 0.5, as_tuple=False)[0].item()) if (g.x[:,0] > 0.5).any() else 0\n",
    "    return H[aidx].cpu().numpy()\n",
    "\n",
    "\n",
    "def build_gnn_embeddings(out_csv: str) -> pd.DataFrame:\n",
    "    # collect items\n",
    "    per_ctx: Dict[str, List[Tuple[int, Data]]] = {}\n",
    "    for ctx in CLASSES:\n",
    "        per_ctx[ctx] = load_general_items_for_class(ctx)\n",
    "        print(f\"[collect] {ctx}: {len(per_ctx[ctx])}\")\n",
    "    per_ctx['metro'] = load_simple_items(METRO_SHARDS); print(f\"[collect] metro: {len(per_ctx['metro'])}\")\n",
    "    per_ctx['bus']   = load_simple_items(BUS_SHARDS);   print(f\"[collect] bus:   {len(per_ctx['bus'])}\")\n",
    "\n",
    "    all_ids: Set[int] = set()\n",
    "    for ctx, items in per_ctx.items():\n",
    "        all_ids |= {aid for aid, _ in items}\n",
    "    all_ids = sorted(all_ids)\n",
    "    print(\"apartments total:\", len(all_ids))\n",
    "\n",
    "    ctx_embs: Dict[str, Dict[int, Optional[str]]] = {}\n",
    "    for ctx in ALL_CONTEXTS:\n",
    "        items = per_ctx.get(ctx, [])\n",
    "        if not items:\n",
    "            ctx_embs[ctx] = {aid: None for aid in all_ids}\n",
    "            continue\n",
    "        model, ds = train_context(items, ctx)\n",
    "        base_f = common_feat_dim(items); input_dim = base_f + 1\n",
    "        by_id = {aid: g for aid,g in items}\n",
    "        model.eval(); emb_map: Dict[int, Optional[str]] = {}\n",
    "        for aid in all_ids:\n",
    "            g = by_id.get(aid)\n",
    "            if g is None:\n",
    "                emb_map[aid] = None\n",
    "            else:\n",
    "                # replicate train pipeline\n",
    "                if g.x.size(1) < base_f:\n",
    "                    pad = torch.zeros((g.num_nodes, base_f - g.x.size(1)), dtype=g.x.dtype)\n",
    "                    g = Data(x=torch.cat([g.x, pad], dim=1), edge_index=g.edge_index, edge_attr=g.edge_attr)\n",
    "                g = add_distance_scalar(g)\n",
    "                vec = embed_with_model(model, g, input_dim)\n",
    "                emb_map[aid] = json.dumps([float(x) for x in vec.tolist()])\n",
    "        ctx_embs[ctx] = emb_map\n",
    "\n",
    "    rows = []\n",
    "    for aid in all_ids:\n",
    "        row = {\"id\": aid}\n",
    "        for ctx in ALL_CONTEXTS:\n",
    "            row[f\"emb_{ctx}\"] = ctx_embs[ctx].get(aid)\n",
    "        rows.append(row)\n",
    "    df = pd.DataFrame(rows)\n",
    "    df.to_csv(out_csv, index=False)\n",
    "    print(\"saved:\", out_csv, \"shape:\", df.shape)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cc0741af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[collect] sport_and_leisure: 25123\n",
      "[collect] medical: 25087\n",
      "[collect] education_prim: 24519\n",
      "[collect] veterinary: 22982\n",
      "[collect] food_and_drink_stores: 24694\n",
      "[collect] arts_and_entertainment: 25007\n",
      "[collect] food_and_drink: 24205\n",
      "[collect] park_like: 24275\n",
      "[collect] security: 24564\n",
      "[collect] religion: 21291\n",
      "[collect] education_sup: 24581\n",
      "[collect] metro: 16752\n",
      "[collect] bus:   24175\n",
      "apartments total: 25211\n",
      "baseline built in 74.44s\n",
      "saved: apartment_embeddings_per_context.csv shape: (25211, 14)\n",
      "           id                              emb_sport_and_leisure  \\\n",
      "0  1359204515  [60.0, 16.075674057006836, 6.5722761154174805,...   \n",
      "1  1366496843  [12.0, 5.705898284912109, 3.5092594623565674, ...   \n",
      "2  1367599797  [78.0, 26.780254364013672, 13.415040969848633,...   \n",
      "3  1391886163  [11.0, 5.413773536682129, 3.2894861698150635, ...   \n",
      "4  1408157926  [33.0, 8.578935623168945, 2.700268268585205, 0...   \n",
      "\n",
      "                                         emb_medical  \\\n",
      "0  [38.0, 15.997368812561035, 9.423164367675781, ...   \n",
      "1  [22.0, 9.402782440185547, 5.164984226226807, 3...   \n",
      "2  [41.0, 12.142561912536621, 5.951859474182129, ...   \n",
      "3  [23.0, 8.883554458618164, 4.721118450164795, 2...   \n",
      "4  [18.0, 6.049106121063232, 2.7733232975006104, ...   \n",
      "\n",
      "                                  emb_education_prim  \\\n",
      "0  [13.0, 3.977703809738159, 1.8542356491088867, ...   \n",
      "1  [9.0, 3.0975704193115234, 1.4777079820632935, ...   \n",
      "2  [11.0, 4.3323211669921875, 2.161160469055176, ...   \n",
      "3  [10.0, 3.3879919052124023, 1.6319491863250732,...   \n",
      "4  [6.0, 2.608611822128296, 1.2793641090393066, 0...   \n",
      "\n",
      "                                      emb_veterinary  \\\n",
      "0  [6.0, 2.277141571044922, 1.3051209449768066, 0...   \n",
      "1  [8.0, 2.286616325378418, 0.9103704690933228, 0...   \n",
      "2  [7.0, 2.1093690395355225, 0.9442524313926697, ...   \n",
      "3  [8.0, 1.9532856941223145, 0.6671395897865295, ...   \n",
      "4  [7.0, 2.411283254623413, 1.3507696390151978, 0...   \n",
      "\n",
      "                           emb_food_and_drink_stores  \\\n",
      "0  [27.0, 9.532333374023438, 4.8009796142578125, ...   \n",
      "1  [14.0, 6.050302982330322, 3.726116895675659, 2...   \n",
      "2  [23.0, 6.6719489097595215, 2.913447618484497, ...   \n",
      "3  [13.0, 5.776050090789795, 3.2777152061462402, ...   \n",
      "4  [11.0, 2.56019926071167, 0.9970377683639526, 0...   \n",
      "\n",
      "                          emb_arts_and_entertainment  \\\n",
      "0  [14.0, 5.213222026824951, 2.8603620529174805, ...   \n",
      "1  [18.0, 5.941456317901611, 2.87703275680542, 1....   \n",
      "2  [30.0, 9.518875122070312, 4.2043681144714355, ...   \n",
      "3  [17.0, 6.013828277587891, 2.922703742980957, 1...   \n",
      "4  [15.0, 6.22273063659668, 3.067983388900757, 1....   \n",
      "\n",
      "                                  emb_food_and_drink  \\\n",
      "0  [41.0, 11.050660133361816, 5.310342311859131, ...   \n",
      "1  [16.0, 7.021714687347412, 4.83206844329834, 3....   \n",
      "2  [25.0, 7.744740009307861, 3.39424204826355, 1....   \n",
      "3  [16.0, 6.098169803619385, 3.082339286804199, 1...   \n",
      "4  [19.0, 4.564805507659912, 2.0996875762939453, ...   \n",
      "\n",
      "                                       emb_park_like  \\\n",
      "0  [3.0, 1.726252555847168, 0.9990460276603699, 0...   \n",
      "1  [6.0, 2.8207437992095947, 1.765191674232483, 1...   \n",
      "2  [2.0, 0.5252547264099121, 0.23184120655059814,...   \n",
      "3  [7.0, 2.3664896488189697, 1.018655776977539, 0...   \n",
      "4  [3.0, 1.3252400159835815, 0.6238857507705688, ...   \n",
      "\n",
      "                                        emb_security  \\\n",
      "0  [6.0, 1.7289631366729736, 1.121602177619934, 0...   \n",
      "1  [5.0, 3.2117748260498047, 2.646604061126709, 2...   \n",
      "2  [11.0, 4.0218987464904785, 2.126202344894409, ...   \n",
      "3  [5.0, 3.004030704498291, 2.287025213241577, 1....   \n",
      "4  [7.0, 2.4722888469696045, 1.1537070274353027, ...   \n",
      "\n",
      "                                        emb_religion  \\\n",
      "0  [3.0, 0.8515275120735168, 0.3497370481491089, ...   \n",
      "1  [5.0, 1.2922619581222534, 0.6314557194709778, ...   \n",
      "2  [4.0, 2.0272626876831055, 1.0776073932647705, ...   \n",
      "3  [8.0, 2.3232483863830566, 0.9722421765327454, ...   \n",
      "4                                               None   \n",
      "\n",
      "                                   emb_education_sup  \\\n",
      "0  [9.0, 3.1892800331115723, 1.4277830123901367, ...   \n",
      "1  [28.0, 3.475991725921631, 0.8413114547729492, ...   \n",
      "2  [100.0, 25.64299964904785, 9.471589088439941, ...   \n",
      "3  [35.0, 4.483249664306641, 0.968339204788208, 0...   \n",
      "4  [4.0, 2.344971179962158, 1.6418575048446655, 1...   \n",
      "\n",
      "                                           emb_metro  \\\n",
      "0  [1.0, 0.002815012587234378, 7.924295459815767e...   \n",
      "1  [4.0, 0.007331489585340023, 1.392192643834278e...   \n",
      "2  [3.0, 0.004855748265981674, 8.05325817054836e-...   \n",
      "3  [3.0, 0.005734436679631472, 1.103953763958998e...   \n",
      "4  [1.0, 0.0015795428771525621, 2.49495565185498e...   \n",
      "\n",
      "                                             emb_bus  \n",
      "0  [10.0, 0.03267307206988335, 0.0001179291430162...  \n",
      "1  [11.0, 0.062020767480134964, 0.000560429005417...  \n",
      "2  [12.0, 0.0572342574596405, 0.00030509982025250...  \n",
      "3  [9.0, 0.043397918343544006, 0.0002502724819350...  \n",
      "4  [6.0, 0.09124578535556793, 0.00285489996895194...  \n"
     ]
    }
   ],
   "source": [
    "# ======================================\n",
    "# Cell 6 — Run\n",
    "# ======================================\n",
    "if MODE == \"baseline\":\n",
    "    df_out = build_baseline_embeddings(OUT_CSV)\n",
    "else:\n",
    "    df_out = build_gnn_embeddings(OUT_CSV)\n",
    "\n",
    "print(df_out.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99a396dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 0 shards. Apartments gathered: 0\n",
      "✅ wrote embeddings_poi_from_shards.csv with 0 rows. Non-null counts: sport_and_leisure=0, medical=0, education_prim=0, veterinary=0, food_and_drink_stores=0, arts_and_entertainment=0, food_and_drink=0, park_like=0, security=0, religion=0, education_sup=0\n"
     ]
    }
   ],
   "source": [
    "# POI baseline embeddings (12-dim per class) → CSV (JSON columns, not expanded)\n",
    "\n",
    "import json, pickle\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Where your POI shards live:\n",
    "SHARD_DIR = Path(\"WebScraper/Graph_data\")\n",
    "\n",
    "# Output CSV (one row per apartment; 11 JSON columns, one per class)\n",
    "OUT_CSV = \"embeddings_poi_from_shards.csv\"\n",
    "\n",
    "# The 11 POI classes (column order will follow this)\n",
    "CLASSES: List[str] = [\n",
    "    \"sport_and_leisure\",\n",
    "    \"medical\",\n",
    "    \"education_prim\",\n",
    "    \"veterinary\",\n",
    "    \"food_and_drink_stores\",\n",
    "    \"arts_and_entertainment\",\n",
    "    \"food_and_drink\",\n",
    "    \"park_like\",\n",
    "    \"security\",\n",
    "    \"religion\",\n",
    "    \"education_sup\",\n",
    "]\n",
    "\n",
    "# POI thresholds (meters) used in the 12-dim baseline schema\n",
    "R1, R2, R3 = 600.0, 1200.0, 2400.0\n",
    "EPS = 1e-6\n",
    "\n",
    "def baseline_from_meters(d: np.ndarray) -> Optional[List[float]]:\n",
    "    \"\"\"Return 12-dim baseline vector from raw distances in meters; None if empty.\"\"\"\n",
    "    if d.size == 0:\n",
    "        return None\n",
    "    d = np.asarray(d, dtype=float)\n",
    "    d_sorted = np.sort(d)\n",
    "    inv = 1.0 / (d_sorted + EPS)\n",
    "    return [\n",
    "        float(d_sorted.size),               # 0: count\n",
    "        float(d_sorted.mean()),             # 1: mean\n",
    "        float(d_sorted.min()),              # 2: min\n",
    "        float(d_sorted.max()),              # 3: max\n",
    "        float(np.median(d_sorted)),         # 4: median\n",
    "        float(d_sorted.std()),              # 5: std\n",
    "        float(inv.mean()),                  # 6: mean_inv\n",
    "        float(inv.max()),                   # 7: max_inv\n",
    "        float(inv.sum()),                   # 8: sum_inv\n",
    "        float((d_sorted <= R1).mean()),     # 9: frac <= 600m\n",
    "        float((d_sorted <= R2).mean()),     # 10: frac <= 1200m\n",
    "        float((d_sorted <= R3).mean()),     # 11: frac <= 2400m\n",
    "    ]\n",
    "\n",
    "def iter_shards():\n",
    "    # Only use the POI shards (pattern provided); adapt if needed.\n",
    "    for p in sorted(SHARD_DIR.glob(\"shard_*.pkl\")):\n",
    "        yield p\n",
    "\n",
    "def extract_poi_distances_from_graph(g) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Get raw meters from a PyG Data 'g'.\n",
    "    Assumes updated shards stored meters in g.edge_attr (shape [E, 1]).\n",
    "    \"\"\"\n",
    "    if g is None or getattr(g, \"edge_attr\", None) is None:\n",
    "        return np.array([], dtype=float)\n",
    "    ea = g.edge_attr\n",
    "    if ea.dim() == 2 and ea.size(1) >= 1:\n",
    "        arr = ea.view(-1).detach().cpu().numpy()\n",
    "        return arr\n",
    "    # Fallback: treat as empty if shape is unexpected\n",
    "    return np.array([], dtype=float)\n",
    "\n",
    "# Collect per-apartment per-class baselines\n",
    "# Structure: results[apt_id][class] = list[12] or None\n",
    "results: Dict[int, Dict[str, Optional[List[float]]]] = {}\n",
    "\n",
    "count_shards = 0\n",
    "for shard in iter_shards():\n",
    "    with open(shard, \"rb\") as f:\n",
    "        d = pickle.load(f)  # dict[int -> dict[class -> Data|None]]\n",
    "    count_shards += 1\n",
    "\n",
    "    for apt_id, class_map in d.items():\n",
    "        # Ensure we have a dict for this apartment\n",
    "        if apt_id not in results:\n",
    "            results[apt_id] = {cls: None for cls in CLASSES}\n",
    "\n",
    "        # class_map should be dict[class -> Data|None]; compute per class\n",
    "        for cls in CLASSES:\n",
    "            g = class_map.get(cls, None)\n",
    "            if g is None:\n",
    "                # leave as None\n",
    "                continue\n",
    "            meters = extract_poi_distances_from_graph(g)\n",
    "            vec = baseline_from_meters(meters)\n",
    "            results[apt_id][cls] = vec\n",
    "\n",
    "print(f\"Processed {count_shards} shards. Apartments gathered: {len(results)}\")\n",
    "\n",
    "# Build output rows (keep JSON-encoded vectors; None stays None)\n",
    "rows = []\n",
    "for apt_id in sorted(results.keys()):\n",
    "    rec = {\"id\": int(apt_id)}\n",
    "    for cls in CLASSES:\n",
    "        vec = results[apt_id][cls]\n",
    "        rec[f\"emb_{cls}\"] = json.dumps(vec) if vec is not None else None\n",
    "    rows.append(rec)\n",
    "\n",
    "df = pd.DataFrame(rows, columns=[\"id\"] + [f\"emb_{c}\" for c in CLASSES])\n",
    "df.to_csv(OUT_CSV, index=False)\n",
    "print(\n",
    "    f\"✅ wrote {OUT_CSV} with {len(df)} rows. \"\n",
    "    + \"Non-null counts: \"\n",
    "    + \", \".join([f\"{c}={df[f'emb_{c}'].notna().sum()}\" for c in CLASSES])\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "051b53a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ wrote embeddings_poi_from_shards.csv with 25215 rows.\n",
      "emb_sport_and_leisure: samples=25123, order_ok=True, inv_ok=True\n",
      "emb_medical: samples=25087, order_ok=True, inv_ok=True\n",
      "emb_education_prim: samples=24519, order_ok=True, inv_ok=True\n"
     ]
    }
   ],
   "source": [
    "# Rebuild embeddings_poi_from_shards.csv using RAW METERS from POI shards (Py 3.9 compatible)\n",
    "import json, numpy as np, pandas as pd, pickle\n",
    "from pathlib import Path\n",
    "from typing import Optional, List\n",
    "\n",
    "SHARD_DIR = Path(\"Graph_data\")  # your updated POI shards live here\n",
    "OUT_CSV   = \"embeddings_poi_from_shards.csv\"\n",
    "\n",
    "# POI classes\n",
    "CLASSES = [\n",
    "    'sport_and_leisure','medical','education_prim','veterinary','food_and_drink_stores',\n",
    "    'arts_and_entertainment','food_and_drink','park_like','security','religion','education_sup'\n",
    "]\n",
    "\n",
    "# radii bands (meters)\n",
    "R1, R2, R3 = 600.0, 1200.0, 2400.0\n",
    "EPS = 1e-6\n",
    "\n",
    "def baseline_from_meters(d_meters: np.ndarray) -> Optional[List[float]]:\n",
    "    if d_meters.size == 0:\n",
    "        return None\n",
    "    d = np.sort(d_meters.astype(float))\n",
    "    inv = 1.0 / (d + EPS)\n",
    "    return [\n",
    "        float(d.size),          # 0 count\n",
    "        float(d.mean()),        # 1 mean\n",
    "        float(d.min()),         # 2 min\n",
    "        float(d.max()),         # 3 max\n",
    "        float(np.median(d)),    # 4 median\n",
    "        float(d.std()),         # 5 std\n",
    "        float(inv.mean()),      # 6 mean_inv\n",
    "        float(inv.max()),       # 7 max_inv\n",
    "        float(inv.sum()),       # 8 sum_inv   <-- fixed\n",
    "        float((d <= R1).mean()),# 9 frac <= 600m\n",
    "        float((d <= R2).mean()),#10 frac <= 1200m\n",
    "        float((d <= R3).mean()),#11 frac <= 2400m\n",
    "    ]\n",
    "\n",
    "def extract_meters_tensor(g):\n",
    "    \"\"\"\n",
    "    Robustly pick the RAW meters array from a POI Data object.\n",
    "    Prefer explicit fields if present, otherwise fallback to edge_attr\n",
    "    only if it looks like meters (max > 10).\n",
    "    \"\"\"\n",
    "    for name in (\"edge_attr_meters\", \"raw_dist\", \"edge_dist_meters\"):\n",
    "        if hasattr(g, name):\n",
    "            t = getattr(g, name)\n",
    "            return t.view(-1).cpu().numpy()\n",
    "\n",
    "    if hasattr(g, \"edge_attr\") and g.edge_attr is not None:\n",
    "        arr = g.edge_attr.view(-1).cpu().numpy()\n",
    "        if np.nanmax(arr) > 10.0:   # heuristic: meters exceed 10\n",
    "            return arr\n",
    "\n",
    "    return np.array([])\n",
    "\n",
    "def collect_all():\n",
    "    shards = sorted(SHARD_DIR.glob(\"shard_*.pkl\"))\n",
    "    if not shards:\n",
    "        print(\"No POI shards found in Graph_data/.\")\n",
    "        return []\n",
    "\n",
    "    rows = []\n",
    "    seen = set()\n",
    "    for p in shards:\n",
    "        try:\n",
    "            with open(p, \"rb\") as f:\n",
    "                d = pickle.load(f)  # dict[int -> dict[class -> Data|None]]\n",
    "        except Exception as e:\n",
    "            print(f\"[warn] skipping shard {p.name}: {e}\")\n",
    "            continue\n",
    "\n",
    "        for aid, gdict in d.items():\n",
    "            if aid in seen:\n",
    "                continue\n",
    "            seen.add(aid)\n",
    "            rec = {\"id\": int(aid)}\n",
    "            for cls in CLASSES:\n",
    "                g = gdict.get(cls)\n",
    "                if g is None:\n",
    "                    rec[f\"emb_{cls}\"] = None\n",
    "                    continue\n",
    "                meters = extract_meters_tensor(g)\n",
    "                if meters.size == 0:\n",
    "                    rec[f\"emb_{cls}\"] = None\n",
    "                else:\n",
    "                    vec = baseline_from_meters(meters)\n",
    "                    rec[f\"emb_{cls}\"] = json.dumps(vec)\n",
    "            rows.append(rec)\n",
    "    return rows\n",
    "\n",
    "rows = collect_all()\n",
    "df = pd.DataFrame(rows)\n",
    "df.to_csv(OUT_CSV, index=False)\n",
    "\n",
    "print(f\"✅ wrote {OUT_CSV} with {len(df)} rows.\")\n",
    "for cls in CLASSES[:3]:\n",
    "    col = f\"emb_{cls}\"\n",
    "    s = df[col].dropna()\n",
    "    if s.empty:\n",
    "        continue\n",
    "    vec = json.loads(s.iloc[0])\n",
    "    cnt, mean_d, min_d, max_d, med_d, std_d, mean_inv, max_inv, sum_inv, f1, f2, f3 = vec\n",
    "    ok_order = (min_d <= med_d <= max_d)\n",
    "    ok_inv   = (sum_inv >= max_inv)\n",
    "    print(f\"{col}: samples={len(s)}, order_ok={ok_order}, inv_ok={ok_inv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1e22098",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 POI shards under Graph_data\n",
      "✅ wrote embeddings_poi_from_shards.csv with 25215 rows. Non-nulls per class:\n",
      "  - emb_sport_and_leisure: 25123\n",
      "  - emb_medical: 25087\n",
      "  - emb_education_prim: 24519\n",
      "  - emb_veterinary: 22982\n",
      "  - emb_food_and_drink_stores: 24694\n",
      "  - emb_arts_and_entertainment: 25007\n",
      "  - emb_food_and_drink: 24205\n",
      "  - emb_park_like: 24275\n",
      "  - emb_security: 24564\n",
      "  - emb_religion: 21291\n",
      "  - emb_education_sup: 24581\n"
     ]
    }
   ],
   "source": [
    "# Rebuild POI baseline embeddings strictly from RAW METERS stored in shards.\n",
    "# Outputs: embeddings_poi_from_shards.csv\n",
    "\n",
    "import json, pickle\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from typing import Optional\n",
    "\n",
    "SHARD_DIR = Path(\"Graph_data\")\n",
    "OUT_CSV   = \"embeddings_poi_from_shards.csv\"\n",
    "\n",
    "CLASSES = [\n",
    "    'sport_and_leisure','medical','education_prim','veterinary','food_and_drink_stores',\n",
    "    'arts_and_entertainment','food_and_drink','park_like','security','religion','education_sup'\n",
    "]\n",
    "\n",
    "R1, R2, R3 = 600.0, 1200.0, 2400.0\n",
    "EPS = 1e-6\n",
    "\n",
    "def as_numpy_1d(t: Optional[torch.Tensor]) -> Optional[np.ndarray]:\n",
    "    if t is None:\n",
    "        return None\n",
    "    if isinstance(t, torch.Tensor):\n",
    "        return t.view(-1).detach().cpu().numpy()\n",
    "    return None\n",
    "\n",
    "def get_raw_meters(g) -> Optional[np.ndarray]:\n",
    "    # Prefer explicit meters field if present; else edge_attr (new shards keep meters there)\n",
    "    if hasattr(g, \"edge_attr_meters\"):\n",
    "        return as_numpy_1d(getattr(g, \"edge_attr_meters\"))\n",
    "    return as_numpy_1d(getattr(g, \"edge_attr\", None))\n",
    "\n",
    "def baseline_from_meters(d: Optional[np.ndarray]) -> Optional[list]:\n",
    "    if d is None or d.size == 0:\n",
    "        return None\n",
    "    d = d.astype(float)\n",
    "    d_sorted = np.sort(d)\n",
    "    inv = 1.0 / (d_sorted + EPS)\n",
    "    return [\n",
    "        float(d_sorted.size),            # 0 count\n",
    "        float(d_sorted.mean()),          # 1 mean\n",
    "        float(d_sorted.min()),           # 2 min\n",
    "        float(d_sorted.max()),           # 3 max\n",
    "        float(np.median(d_sorted)),      # 4 median\n",
    "        float(d_sorted.std()),           # 5 std\n",
    "        float(inv.mean()),               # 6 mean_inv\n",
    "        float(inv.max()),                # 7 max_inv\n",
    "        float(inv.sum()),                # 8 sum_inv\n",
    "        float((d_sorted <= R1).mean()),  # 9  <= 600m\n",
    "        float((d_sorted <= R2).mean()),  # 10 <= 1200m\n",
    "        float((d_sorted <= R3).mean()),  # 11 <= 2400m\n",
    "    ]\n",
    "\n",
    "# --- Gather baselines per apartment ---\n",
    "rows = {}\n",
    "shards = sorted(SHARD_DIR.glob(\"shard_*.pkl\"))\n",
    "print(f\"Found {len(shards)} POI shards under {SHARD_DIR}\")\n",
    "\n",
    "for p in shards:\n",
    "    with open(p, \"rb\") as f:\n",
    "        d = pickle.load(f)  # dict[int -> dict[class -> Data|None]]\n",
    "    for aid, per_cls in d.items():\n",
    "        if aid not in rows:\n",
    "            rows[aid] = {f\"emb_{c}\": None for c in CLASSES}\n",
    "        for cls in CLASSES:\n",
    "            g = per_cls.get(cls)\n",
    "            if g is None:\n",
    "                rows[aid][f\"emb_{cls}\"] = None\n",
    "                continue\n",
    "            meters = get_raw_meters(g)\n",
    "            rows[aid][f\"emb_{cls}\"] = baseline_from_meters(meters)\n",
    "\n",
    "# --- Build DataFrame with JSON strings per class ---\n",
    "out = []\n",
    "for aid, rec in rows.items():\n",
    "    row = {\"id\": int(aid)}\n",
    "    for cls in CLASSES:\n",
    "        key = f\"emb_{cls}\"\n",
    "        row[key] = json.dumps(rec[key]) if rec[key] is not None else None\n",
    "    out.append(row)\n",
    "\n",
    "df_poi = pd.DataFrame(out).sort_values(\"id\").reset_index(drop=True)\n",
    "df_poi.to_csv(OUT_CSV, index=False)\n",
    "non_null_counts = {c: int(df_poi[c].notna().sum()) for c in df_poi.columns if c.startswith(\"emb_\")}\n",
    "print(f\"✅ wrote {OUT_CSV} with {len(df_poi)} rows. Non-nulls per class:\")\n",
    "for k,v in non_null_counts.items():\n",
    "    print(f\"  - {k}: {v}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96d6c07e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apartment ID: 1583341281\n",
      "\n",
      "Class: medical embedding values:\n",
      "  dim00: 31.000000 → count_pois\n",
      "  dim01: 1619.239470 → mean_distance\n",
      "  dim02: 528.707642 → min_distance\n",
      "  dim03: 2341.403320 → max_distance\n",
      "  dim04: 1808.764771 → median_distance\n",
      "  dim05: 549.732507 → std_distance\n",
      "  dim06: 0.000758 → mean_inverse_distance\n",
      "  dim07: 0.001891 → max_inverse_distance\n",
      "  dim08: 0.023494 → sum_inverse_distance\n",
      "  dim09: 0.129032 → ratio_within_near_radius\n",
      "  dim10: 0.258065 → ratio_within_mid_radius\n",
      "  dim11: 1.000000 → ratio_within_far_radius\n"
     ]
    }
   ],
   "source": [
    "import random, json\n",
    "\n",
    "# Pick a random apartment or set manually\n",
    "apt_id = random.choice(df_poi[\"id\"].tolist())\n",
    "# apt_id = 1540798899  # <- uncomment to force a specific ID\n",
    "\n",
    "cls = \"medical\"  # change to any: sport_and_leisure, education_prim, etc.\n",
    "\n",
    "row = df_poi[df_poi[\"id\"] == apt_id].iloc[0]\n",
    "val_json = row[f\"emb_{cls}\"]\n",
    "\n",
    "print(f\"Apartment ID: {apt_id}\")\n",
    "if val_json is None:\n",
    "    print(f\"No embedding for class {cls}\")\n",
    "else:\n",
    "    vals = json.loads(val_json)\n",
    "    print(f\"\\nClass: {cls} embedding values:\")\n",
    "    labels = [\n",
    "        \"count_pois\", \"mean_distance\", \"min_distance\", \"max_distance\",\n",
    "        \"median_distance\", \"std_distance\", \"mean_inverse_distance\",\n",
    "        \"max_inverse_distance\", \"sum_inverse_distance\",\n",
    "        \"ratio_within_near_radius\", \"ratio_within_mid_radius\", \"ratio_within_far_radius\"\n",
    "    ]\n",
    "    for i, (lab, v) in enumerate(zip(labels, vals)):\n",
    "        print(f\"  dim{i:02d}: {v:.6f} → {lab}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739ad34f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
