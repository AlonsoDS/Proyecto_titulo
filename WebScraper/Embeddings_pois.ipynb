{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac31d2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file: notebooks/per_context_embeddings.py\n",
    "\"\"\"\n",
    "Build 13 per-context embeddings (11 POI classes + metro + bus) for each apartment.\n",
    "- Default: FAST BASELINE (no training). Always distinct across contexts when data differs.\n",
    "- Optional: Per-context GNN with adjustable EPOCHS and NEG_K (slower).\n",
    "\n",
    "Outputs a CSV with columns: id, emb_<context> (JSON vector or null).\n",
    "Looks for shards in Graph_data/ with these patterns:\n",
    "  - shard_*.pkl           (general POI classes)\n",
    "  - METROSHARD_*.pkl      (metro)\n",
    "  - BUSSHARD_*.pkl        (bus)\n",
    "\"\"\"\n",
    "\n",
    "# ==============================\n",
    "# Cell 1 — Imports & switches\n",
    "# ==============================\n",
    "import os, json, time, pickle, glob\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple, Set\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import GraphConv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f87bba99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Mode: \"baseline\" (fast, no training) or \"gnn\" (train per context)\n",
    "MODE = \"baseline\"   # change to \"gnn\" to train encoders per context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce757a9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mode': 'baseline', 'device': 'cuda'}\n"
     ]
    }
   ],
   "source": [
    "# ==============================\n",
    "# Cell 2 — Paths & hyperparams\n",
    "# ==============================\n",
    "ROOT = Path(\"Graph_data\")  # folder containing exported shards\n",
    "OUT_CSV = \"apartment_embeddings_per_context.csv\"\n",
    "\n",
    "# Contexts\n",
    "CLASSES = [\n",
    "    'sport_and_leisure','medical','education_prim','veterinary',\n",
    "    'food_and_drink_stores','arts_and_entertainment','food_and_drink',\n",
    "    'park_like','security','religion','education_sup'\n",
    "]\n",
    "ALL_CONTEXTS = CLASSES + ['metro','bus']\n",
    "\n",
    "# GNN knobs (used only when MODE==\"gnn\")\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "EMB_DIM = 12\n",
    "HIDDEN  = 32\n",
    "EPOCHS  = 8         # increase for better quality\n",
    "BATCH_SZ = 512      # larger is faster\n",
    "LR = 1e-3\n",
    "WEIGHT_DECAY = 1e-4\n",
    "NEG_K = 8           # increase for stronger contrast\n",
    "\n",
    "print({\"mode\": MODE, \"device\": str(DEVICE)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d1982e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found: general=5, metro=2, bus=2\n"
     ]
    }
   ],
   "source": [
    "# ======================================\n",
    "# Cell 3 — Load exported shard helpers\n",
    "# ======================================\n",
    "\n",
    "def load_pickle(p: Path):\n",
    "    with open(p, \"rb\") as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "# General shards hold: dict[apt_id] -> dict[class] -> Data|None\n",
    "# Metro/Bus shards hold: dict[apt_id] -> Data|None\n",
    "\n",
    "def find_shards() -> Tuple[List[Path], List[Path], List[Path]]:\n",
    "    gens  = sorted((ROOT).glob(\"shard_*.pkl\"))\n",
    "    metro = sorted((ROOT).glob(\"METROSHARD_*.pkl\"))\n",
    "    bus   = sorted((ROOT).glob(\"BUSSHARD_*.pkl\"))\n",
    "    print(f\"found: general={len(gens)}, metro={len(metro)}, bus={len(bus)}\")\n",
    "    return gens, metro, bus\n",
    "\n",
    "GEN_SHARDS, METRO_SHARDS, BUS_SHARDS = find_shards()\n",
    "\n",
    "\n",
    "def load_general_items_for_class(ctx: str) -> List[Tuple[int, Data]]:\n",
    "    items: List[Tuple[int, Data]] = []\n",
    "    for p in GEN_SHARDS:\n",
    "        part = load_pickle(p)\n",
    "        for aid, gdict in part.items():\n",
    "            g = gdict.get(ctx)\n",
    "            if isinstance(g, Data):\n",
    "                items.append((int(aid), g))\n",
    "    return items\n",
    "\n",
    "\n",
    "def load_simple_items(shards: List[Path]) -> List[Tuple[int, Data]]:\n",
    "    items: List[Tuple[int, Data]] = []\n",
    "    for p in shards:\n",
    "        part = load_pickle(p)\n",
    "        for aid, g in part.items():\n",
    "            if isinstance(g, Data):\n",
    "                items.append((int(aid), g))\n",
    "    return items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dc01424b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================\n",
    "# Cell 4 — FAST baseline (no train)\n",
    "# ===================================\n",
    "\n",
    "def weights_from_graph(g: Data) -> Optional[torch.Tensor]:\n",
    "    if g is None or g.edge_attr is None or g.edge_attr.numel() == 0:\n",
    "        return None\n",
    "    w = g.edge_attr.view(-1).clone()\n",
    "    if (w > 1.0).any():  # meters → squash\n",
    "        w = 1.0 / (1.0 + torch.clamp(w, min=0.0))\n",
    "    return torch.clamp(w, 0.0, 1.0)\n",
    "\n",
    "\n",
    "def star_stats_12d(w: torch.Tensor) -> np.ndarray:\n",
    "    E = float(w.numel())\n",
    "    if E == 0:\n",
    "        return np.full(12, np.nan, dtype=np.float32)\n",
    "    s1  = w.sum(); s2 = (w**2).sum(); s3 = (w**3).sum()\n",
    "    mean = float(s1 / E)\n",
    "    var  = float(s2 / E - mean**2)\n",
    "    std  = float(np.sqrt(max(var, 0.0)))\n",
    "    w_np = w.cpu().numpy()\n",
    "    w_sorted = np.sort(w_np)[::-1]\n",
    "    top1 = float(w_sorted[0])\n",
    "    top3_mean = float(w_sorted[:min(3, w_sorted.size)].mean())\n",
    "    tail = float((w_np > 0.75).mean())\n",
    "    mn = float(w_np.min()); mx = float(w_np.max())\n",
    "    bin2 = float((w_np > 0.5).mean())\n",
    "    return np.array([E, float(s1), float(s2), float(s3), mean, std, mn, mx, top1, top3_mean, tail, bin2], dtype=np.float32)\n",
    "\n",
    "\n",
    "def build_baseline_embeddings(out_csv: str) -> pd.DataFrame:\n",
    "    per_ctx: Dict[str, List[Tuple[int, Data]]] = {}\n",
    "    for ctx in CLASSES:\n",
    "        per_ctx[ctx] = load_general_items_for_class(ctx)\n",
    "        print(f\"[collect] {ctx}: {len(per_ctx[ctx])}\")\n",
    "    per_ctx['metro'] = load_simple_items(METRO_SHARDS); print(f\"[collect] metro: {len(per_ctx['metro'])}\")\n",
    "    per_ctx['bus']   = load_simple_items(BUS_SHARDS);   print(f\"[collect] bus:   {len(per_ctx['bus'])}\")\n",
    "\n",
    "    all_ids: Set[int] = set()\n",
    "    for ctx, items in per_ctx.items():\n",
    "        all_ids |= {aid for aid, _ in items}\n",
    "    all_ids = sorted(all_ids)\n",
    "    print(\"apartments total:\", len(all_ids))\n",
    "\n",
    "    ctx_embs: Dict[str, Dict[int, Optional[str]]] = {c: {} for c in ALL_CONTEXTS}\n",
    "    t0 = time.time()\n",
    "    for ctx in ALL_CONTEXTS:\n",
    "        items = dict(per_ctx.get(ctx, []))  # apt_id -> Data\n",
    "        for aid in all_ids:\n",
    "            g = items.get(aid)\n",
    "            if g is None:\n",
    "                ctx_embs[ctx][aid] = None\n",
    "            else:\n",
    "                w = weights_from_graph(g)\n",
    "                if w is None:\n",
    "                    ctx_embs[ctx][aid] = None\n",
    "                else:\n",
    "                    vec = star_stats_12d(w)\n",
    "                    ctx_embs[ctx][aid] = json.dumps([float(x) for x in vec.tolist()])\n",
    "    print(f\"baseline built in {time.time()-t0:.2f}s\")\n",
    "\n",
    "    rows = []\n",
    "    for aid in all_ids:\n",
    "        row = {\"id\": aid}\n",
    "        for ctx in ALL_CONTEXTS:\n",
    "            row[f\"emb_{ctx}\"] = ctx_embs[ctx].get(aid)\n",
    "        rows.append(row)\n",
    "    df = pd.DataFrame(rows)\n",
    "    df.to_csv(out_csv, index=False)\n",
    "    print(\"saved:\", out_csv, \"shape:\", df.shape)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ee3aac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================\n",
    "# Cell 5 — Optional: per-context GNN\n",
    "# ======================================\n",
    "class TinyGraphConv(nn.Module):\n",
    "    def __init__(self, in_dim: int, hidden: int, out_dim: int):\n",
    "        super().__init__()\n",
    "        self.conv1 = GraphConv(in_dim, hidden)\n",
    "        self.conv2 = GraphConv(hidden, out_dim)\n",
    "        self.act = nn.ReLU(); self.drop = nn.Dropout(0.1)\n",
    "        self.W = nn.Linear(out_dim, out_dim, bias=False)      # for edges\n",
    "        self.h_deg = nn.Linear(out_dim, 1)\n",
    "        self.h_mean = nn.Linear(out_dim, 1)\n",
    "\n",
    "    def forward(self, data: Data):\n",
    "        x, ei = data.x, data.edge_index\n",
    "        ew = data.edge_attr.view(-1) if data.edge_attr is not None else None\n",
    "        h = self.conv1(x, ei, edge_weight=ew); h = self.act(h); h = self.drop(h)\n",
    "        h = self.conv2(h, ei, edge_weight=ew)\n",
    "        return h\n",
    "\n",
    "    def score(self, h_src, h_dst):\n",
    "        return (self.W(h_src) * h_dst).sum(dim=-1)\n",
    "\n",
    "\n",
    "def add_distance_scalar(g: Data) -> Data:\n",
    "    n = g.num_nodes\n",
    "    dist = torch.zeros((n,1), dtype=g.x.dtype)\n",
    "    if g.edge_attr is not None and g.edge_index is not None:\n",
    "        _, dst = g.edge_index\n",
    "        w = g.edge_attr.view(-1)\n",
    "        dist[dst] = w.unsqueeze(1) if w.dim()==1 else w\n",
    "    g.x = torch.cat([g.x, dist], dim=1)\n",
    "    return g\n",
    "\n",
    "\n",
    "def common_feat_dim(items: List[Tuple[int, Data]]) -> int:\n",
    "    return max((g.x.size(1) for _, g in items), default=0)\n",
    "\n",
    "class StarDataset(Dataset):\n",
    "    def __init__(self, items: List[Tuple[int, Data]], base_f: int):\n",
    "        self.items = items; self.base_f = base_f; self.input_dim = base_f + 1\n",
    "    def __len__(self): return len(self.items)\n",
    "    def __getitem__(self, i: int) -> Data:\n",
    "        aid, g = self.items[i]\n",
    "        if g.x.size(1) < self.base_f:\n",
    "            pad = torch.zeros((g.num_nodes, self.base_f - g.x.size(1)), dtype=g.x.dtype)\n",
    "            g = Data(x=torch.cat([g.x, pad], dim=1), edge_index=g.edge_index, edge_attr=g.edge_attr)\n",
    "        g = add_distance_scalar(g)\n",
    "        if g.x.size(1) > self.input_dim:\n",
    "            g.x = g.x[:, :self.input_dim]\n",
    "        elif g.x.size(1) < self.input_dim:\n",
    "            pad = torch.zeros((g.num_nodes, self.input_dim - g.x.size(1)), dtype=g.x.dtype)\n",
    "            g.x = torch.cat([g.x, pad], dim=1)\n",
    "        apt_idx = int(torch.nonzero(g.x[:,0] > 0.5, as_tuple=False)[0].item()) if (g.x[:,0] > 0.5).any() else 0\n",
    "        g.apt_idx = apt_idx; g.apt_id = int(aid)\n",
    "        return g\n",
    "\n",
    "\n",
    "def edge_weights_for_training(g: Data) -> torch.Tensor:\n",
    "    w = g.edge_attr.view(-1)\n",
    "    if (w > 1.0).any():\n",
    "        w = 1.0 / (1.0 + torch.clamp(w, min=0.0))\n",
    "    return torch.clamp(w, 0.0, 1.0)\n",
    "\n",
    "\n",
    "def train_context(items: List[Tuple[int, Data]], name: str) -> Tuple[Optional[TinyGraphConv], Optional[StarDataset]]:\n",
    "    if not items:\n",
    "        print(f\"[{name}] no graphs; skip\")\n",
    "        return None, None\n",
    "    base_f = common_feat_dim(items)\n",
    "    ds = StarDataset(items, base_f)\n",
    "    dl = DataLoader(ds, batch_size=BATCH_SZ, shuffle=True)\n",
    "    model = TinyGraphConv(ds.input_dim, HIDDEN, EMB_DIM).to(DEVICE)\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "    def train_epoch() -> float:\n",
    "        model.train(); tot=0.0; cnt=0\n",
    "        for big in dl:\n",
    "            big = big.to(DEVICE); opt.zero_grad()\n",
    "            H = model(big); src, dst = big.edge_index\n",
    "            # apartment global indices\n",
    "            num_g = big.ptr.numel()-1\n",
    "            apt_gl = []\n",
    "            for i in range(num_g):\n",
    "                base = int(big.ptr[i].item()); aidx = int(big.apt_idx[i].item())\n",
    "                apt_gl.append(base + aidx)\n",
    "            apt_gl = torch.tensor(apt_gl, device=big.x.device, dtype=src.dtype)\n",
    "            mask = (src.view(-1,1) == apt_gl.view(1,-1)).any(dim=1)\n",
    "            if not mask.any():\n",
    "                continue\n",
    "            src_pos = src[mask]; dst_pos = dst[mask]\n",
    "            tgt_all = edge_weights_for_training(big); tgt_pos = tgt_all[mask]\n",
    "            # pos\n",
    "            pred_pos = torch.sigmoid(model.score(H[src_pos], H[dst_pos]))\n",
    "            loss_pos = F.mse_loss(pred_pos, tgt_pos)\n",
    "            # negs\n",
    "            neg_s, neg_d = [], []\n",
    "            for i in range(num_g):\n",
    "                base = int(big.ptr[i].item()); end=int(big.ptr[i+1].item())\n",
    "                a_gi = base + int(big.apt_idx[i].item())\n",
    "                true = set(dst_pos[(src_pos==a_gi)].tolist())\n",
    "                cands = [j for j in range(base,end) if j!=a_gi and j not in true]\n",
    "                if not cands: continue\n",
    "                pick = np.random.choice(cands, size=min(NEG_K,len(cands)), replace=False)\n",
    "                neg_s += [a_gi]*len(pick); neg_d += [int(x) for x in pick]\n",
    "            if neg_s:\n",
    "                neg_s = torch.tensor(neg_s, device=big.x.device, dtype=src.dtype)\n",
    "                neg_d = torch.tensor(neg_d, device=big.x.device, dtype=dst.dtype)\n",
    "                pred_neg = torch.sigmoid(model.score(H[neg_s], H[neg_d]))\n",
    "                loss_neg = F.mse_loss(pred_neg, torch.zeros_like(pred_neg))\n",
    "            else:\n",
    "                loss_neg = torch.tensor(0.0, device=big.x.device)\n",
    "            # aux: degree + mean weight from apt embedding\n",
    "            deg_t, mean_t = [], []\n",
    "            for i in range(num_g):\n",
    "                base = int(big.ptr[i].item()); a_gi = base + int(big.apt_idx[i].item())\n",
    "                sel = (src_pos == a_gi); d = int(sel.sum().item()); deg_t.append(d)\n",
    "                mean_t.append(float(tgt_pos[sel].mean().item()) if d>0 else 0.0)\n",
    "            deg_t = torch.tensor(deg_t, device=big.x.device, dtype=torch.float).view(-1,1)\n",
    "            mean_t= torch.tensor(mean_t,device=big.x.device, dtype=torch.float).view(-1,1)\n",
    "            deg_n = torch.log1p(deg_t)/4.0\n",
    "            apt_h = H[apt_gl]\n",
    "            loss_deg = F.mse_loss(model.h_deg(apt_h), deg_n)\n",
    "            loss_mean= F.mse_loss(torch.sigmoid(model.h_mean(apt_h)), mean_t)\n",
    "            loss = loss_pos + 0.5*loss_neg + 0.2*loss_deg + 0.2*loss_mean\n",
    "            loss.backward(); opt.step(); tot += float(loss.item()); cnt += 1\n",
    "        return tot/max(cnt,1)\n",
    "\n",
    "    print(f\"[{name}] graphs={len(ds)} in_dim={ds.input_dim}\")\n",
    "    for ep in range(1, EPOCHS+1):\n",
    "        t0=time.time(); L=train_epoch();\n",
    "        print(f\"[{name}] epoch {ep:02d} loss={L:.6f} ({time.time()-t0:.1f}s)\")\n",
    "    return model, ds\n",
    "\n",
    "@torch.no_grad()\n",
    "def embed_with_model(model: TinyGraphConv, g: Data, input_dim: int) -> np.ndarray:\n",
    "    # ensure exact input_dim\n",
    "    if g.x.size(1) < input_dim:\n",
    "        pad = torch.zeros((g.num_nodes, input_dim - g.x.size(1)), dtype=g.x.dtype)\n",
    "        g.x = torch.cat([g.x, pad], dim=1)\n",
    "    elif g.x.size(1) > input_dim:\n",
    "        g.x = g.x[:, :input_dim]\n",
    "    g = g.to(DEVICE)\n",
    "    H = model(g)\n",
    "    aidx = int(torch.nonzero(g.x[:,0] > 0.5, as_tuple=False)[0].item()) if (g.x[:,0] > 0.5).any() else 0\n",
    "    return H[aidx].cpu().numpy()\n",
    "\n",
    "\n",
    "def build_gnn_embeddings(out_csv: str) -> pd.DataFrame:\n",
    "    # collect items\n",
    "    per_ctx: Dict[str, List[Tuple[int, Data]]] = {}\n",
    "    for ctx in CLASSES:\n",
    "        per_ctx[ctx] = load_general_items_for_class(ctx)\n",
    "        print(f\"[collect] {ctx}: {len(per_ctx[ctx])}\")\n",
    "    per_ctx['metro'] = load_simple_items(METRO_SHARDS); print(f\"[collect] metro: {len(per_ctx['metro'])}\")\n",
    "    per_ctx['bus']   = load_simple_items(BUS_SHARDS);   print(f\"[collect] bus:   {len(per_ctx['bus'])}\")\n",
    "\n",
    "    all_ids: Set[int] = set()\n",
    "    for ctx, items in per_ctx.items():\n",
    "        all_ids |= {aid for aid, _ in items}\n",
    "    all_ids = sorted(all_ids)\n",
    "    print(\"apartments total:\", len(all_ids))\n",
    "\n",
    "    ctx_embs: Dict[str, Dict[int, Optional[str]]] = {}\n",
    "    for ctx in ALL_CONTEXTS:\n",
    "        items = per_ctx.get(ctx, [])\n",
    "        if not items:\n",
    "            ctx_embs[ctx] = {aid: None for aid in all_ids}\n",
    "            continue\n",
    "        model, ds = train_context(items, ctx)\n",
    "        base_f = common_feat_dim(items); input_dim = base_f + 1\n",
    "        by_id = {aid: g for aid,g in items}\n",
    "        model.eval(); emb_map: Dict[int, Optional[str]] = {}\n",
    "        for aid in all_ids:\n",
    "            g = by_id.get(aid)\n",
    "            if g is None:\n",
    "                emb_map[aid] = None\n",
    "            else:\n",
    "                # replicate train pipeline\n",
    "                if g.x.size(1) < base_f:\n",
    "                    pad = torch.zeros((g.num_nodes, base_f - g.x.size(1)), dtype=g.x.dtype)\n",
    "                    g = Data(x=torch.cat([g.x, pad], dim=1), edge_index=g.edge_index, edge_attr=g.edge_attr)\n",
    "                g = add_distance_scalar(g)\n",
    "                vec = embed_with_model(model, g, input_dim)\n",
    "                emb_map[aid] = json.dumps([float(x) for x in vec.tolist()])\n",
    "        ctx_embs[ctx] = emb_map\n",
    "\n",
    "    rows = []\n",
    "    for aid in all_ids:\n",
    "        row = {\"id\": aid}\n",
    "        for ctx in ALL_CONTEXTS:\n",
    "            row[f\"emb_{ctx}\"] = ctx_embs[ctx].get(aid)\n",
    "        rows.append(row)\n",
    "    df = pd.DataFrame(rows)\n",
    "    df.to_csv(out_csv, index=False)\n",
    "    print(\"saved:\", out_csv, \"shape:\", df.shape)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cc0741af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[collect] sport_and_leisure: 25123\n",
      "[collect] medical: 25087\n",
      "[collect] education_prim: 24519\n",
      "[collect] veterinary: 22982\n",
      "[collect] food_and_drink_stores: 24694\n",
      "[collect] arts_and_entertainment: 25007\n",
      "[collect] food_and_drink: 24205\n",
      "[collect] park_like: 24275\n",
      "[collect] security: 24564\n",
      "[collect] religion: 21291\n",
      "[collect] education_sup: 24581\n",
      "[collect] metro: 16752\n",
      "[collect] bus:   24175\n",
      "apartments total: 25211\n",
      "baseline built in 74.44s\n",
      "saved: apartment_embeddings_per_context.csv shape: (25211, 14)\n",
      "           id                              emb_sport_and_leisure  \\\n",
      "0  1359204515  [60.0, 16.075674057006836, 6.5722761154174805,...   \n",
      "1  1366496843  [12.0, 5.705898284912109, 3.5092594623565674, ...   \n",
      "2  1367599797  [78.0, 26.780254364013672, 13.415040969848633,...   \n",
      "3  1391886163  [11.0, 5.413773536682129, 3.2894861698150635, ...   \n",
      "4  1408157926  [33.0, 8.578935623168945, 2.700268268585205, 0...   \n",
      "\n",
      "                                         emb_medical  \\\n",
      "0  [38.0, 15.997368812561035, 9.423164367675781, ...   \n",
      "1  [22.0, 9.402782440185547, 5.164984226226807, 3...   \n",
      "2  [41.0, 12.142561912536621, 5.951859474182129, ...   \n",
      "3  [23.0, 8.883554458618164, 4.721118450164795, 2...   \n",
      "4  [18.0, 6.049106121063232, 2.7733232975006104, ...   \n",
      "\n",
      "                                  emb_education_prim  \\\n",
      "0  [13.0, 3.977703809738159, 1.8542356491088867, ...   \n",
      "1  [9.0, 3.0975704193115234, 1.4777079820632935, ...   \n",
      "2  [11.0, 4.3323211669921875, 2.161160469055176, ...   \n",
      "3  [10.0, 3.3879919052124023, 1.6319491863250732,...   \n",
      "4  [6.0, 2.608611822128296, 1.2793641090393066, 0...   \n",
      "\n",
      "                                      emb_veterinary  \\\n",
      "0  [6.0, 2.277141571044922, 1.3051209449768066, 0...   \n",
      "1  [8.0, 2.286616325378418, 0.9103704690933228, 0...   \n",
      "2  [7.0, 2.1093690395355225, 0.9442524313926697, ...   \n",
      "3  [8.0, 1.9532856941223145, 0.6671395897865295, ...   \n",
      "4  [7.0, 2.411283254623413, 1.3507696390151978, 0...   \n",
      "\n",
      "                           emb_food_and_drink_stores  \\\n",
      "0  [27.0, 9.532333374023438, 4.8009796142578125, ...   \n",
      "1  [14.0, 6.050302982330322, 3.726116895675659, 2...   \n",
      "2  [23.0, 6.6719489097595215, 2.913447618484497, ...   \n",
      "3  [13.0, 5.776050090789795, 3.2777152061462402, ...   \n",
      "4  [11.0, 2.56019926071167, 0.9970377683639526, 0...   \n",
      "\n",
      "                          emb_arts_and_entertainment  \\\n",
      "0  [14.0, 5.213222026824951, 2.8603620529174805, ...   \n",
      "1  [18.0, 5.941456317901611, 2.87703275680542, 1....   \n",
      "2  [30.0, 9.518875122070312, 4.2043681144714355, ...   \n",
      "3  [17.0, 6.013828277587891, 2.922703742980957, 1...   \n",
      "4  [15.0, 6.22273063659668, 3.067983388900757, 1....   \n",
      "\n",
      "                                  emb_food_and_drink  \\\n",
      "0  [41.0, 11.050660133361816, 5.310342311859131, ...   \n",
      "1  [16.0, 7.021714687347412, 4.83206844329834, 3....   \n",
      "2  [25.0, 7.744740009307861, 3.39424204826355, 1....   \n",
      "3  [16.0, 6.098169803619385, 3.082339286804199, 1...   \n",
      "4  [19.0, 4.564805507659912, 2.0996875762939453, ...   \n",
      "\n",
      "                                       emb_park_like  \\\n",
      "0  [3.0, 1.726252555847168, 0.9990460276603699, 0...   \n",
      "1  [6.0, 2.8207437992095947, 1.765191674232483, 1...   \n",
      "2  [2.0, 0.5252547264099121, 0.23184120655059814,...   \n",
      "3  [7.0, 2.3664896488189697, 1.018655776977539, 0...   \n",
      "4  [3.0, 1.3252400159835815, 0.6238857507705688, ...   \n",
      "\n",
      "                                        emb_security  \\\n",
      "0  [6.0, 1.7289631366729736, 1.121602177619934, 0...   \n",
      "1  [5.0, 3.2117748260498047, 2.646604061126709, 2...   \n",
      "2  [11.0, 4.0218987464904785, 2.126202344894409, ...   \n",
      "3  [5.0, 3.004030704498291, 2.287025213241577, 1....   \n",
      "4  [7.0, 2.4722888469696045, 1.1537070274353027, ...   \n",
      "\n",
      "                                        emb_religion  \\\n",
      "0  [3.0, 0.8515275120735168, 0.3497370481491089, ...   \n",
      "1  [5.0, 1.2922619581222534, 0.6314557194709778, ...   \n",
      "2  [4.0, 2.0272626876831055, 1.0776073932647705, ...   \n",
      "3  [8.0, 2.3232483863830566, 0.9722421765327454, ...   \n",
      "4                                               None   \n",
      "\n",
      "                                   emb_education_sup  \\\n",
      "0  [9.0, 3.1892800331115723, 1.4277830123901367, ...   \n",
      "1  [28.0, 3.475991725921631, 0.8413114547729492, ...   \n",
      "2  [100.0, 25.64299964904785, 9.471589088439941, ...   \n",
      "3  [35.0, 4.483249664306641, 0.968339204788208, 0...   \n",
      "4  [4.0, 2.344971179962158, 1.6418575048446655, 1...   \n",
      "\n",
      "                                           emb_metro  \\\n",
      "0  [1.0, 0.002815012587234378, 7.924295459815767e...   \n",
      "1  [4.0, 0.007331489585340023, 1.392192643834278e...   \n",
      "2  [3.0, 0.004855748265981674, 8.05325817054836e-...   \n",
      "3  [3.0, 0.005734436679631472, 1.103953763958998e...   \n",
      "4  [1.0, 0.0015795428771525621, 2.49495565185498e...   \n",
      "\n",
      "                                             emb_bus  \n",
      "0  [10.0, 0.03267307206988335, 0.0001179291430162...  \n",
      "1  [11.0, 0.062020767480134964, 0.000560429005417...  \n",
      "2  [12.0, 0.0572342574596405, 0.00030509982025250...  \n",
      "3  [9.0, 0.043397918343544006, 0.0002502724819350...  \n",
      "4  [6.0, 0.09124578535556793, 0.00285489996895194...  \n"
     ]
    }
   ],
   "source": [
    "# ======================================\n",
    "# Cell 6 — Run\n",
    "# ======================================\n",
    "if MODE == \"baseline\":\n",
    "    df_out = build_baseline_embeddings(OUT_CSV)\n",
    "else:\n",
    "    df_out = build_gnn_embeddings(OUT_CSV)\n",
    "\n",
    "print(df_out.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99a396dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 0 shards. Apartments gathered: 0\n",
      "✅ wrote embeddings_poi_from_shards.csv with 0 rows. Non-null counts: sport_and_leisure=0, medical=0, education_prim=0, veterinary=0, food_and_drink_stores=0, arts_and_entertainment=0, food_and_drink=0, park_like=0, security=0, religion=0, education_sup=0\n"
     ]
    }
   ],
   "source": [
    "# POI baseline embeddings (12-dim per class) → CSV (JSON columns, not expanded)\n",
    "\n",
    "import json, pickle\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Where your POI shards live:\n",
    "SHARD_DIR = Path(\"WebScraper/Graph_data\")\n",
    "\n",
    "# Output CSV (one row per apartment; 11 JSON columns, one per class)\n",
    "OUT_CSV = \"embeddings_poi_from_shards.csv\"\n",
    "\n",
    "# The 11 POI classes (column order will follow this)\n",
    "CLASSES: List[str] = [\n",
    "    \"sport_and_leisure\",\n",
    "    \"medical\",\n",
    "    \"education_prim\",\n",
    "    \"veterinary\",\n",
    "    \"food_and_drink_stores\",\n",
    "    \"arts_and_entertainment\",\n",
    "    \"food_and_drink\",\n",
    "    \"park_like\",\n",
    "    \"security\",\n",
    "    \"religion\",\n",
    "    \"education_sup\",\n",
    "]\n",
    "\n",
    "# POI thresholds (meters) used in the 12-dim baseline schema\n",
    "R1, R2, R3 = 600.0, 1200.0, 2400.0\n",
    "EPS = 1e-6\n",
    "\n",
    "def baseline_from_meters(d: np.ndarray) -> Optional[List[float]]:\n",
    "    \"\"\"Return 12-dim baseline vector from raw distances in meters; None if empty.\"\"\"\n",
    "    if d.size == 0:\n",
    "        return None\n",
    "    d = np.asarray(d, dtype=float)\n",
    "    d_sorted = np.sort(d)\n",
    "    inv = 1.0 / (d_sorted + EPS)\n",
    "    return [\n",
    "        float(d_sorted.size),               # 0: count\n",
    "        float(d_sorted.mean()),             # 1: mean\n",
    "        float(d_sorted.min()),              # 2: min\n",
    "        float(d_sorted.max()),              # 3: max\n",
    "        float(np.median(d_sorted)),         # 4: median\n",
    "        float(d_sorted.std()),              # 5: std\n",
    "        float(inv.mean()),                  # 6: mean_inv\n",
    "        float(inv.max()),                   # 7: max_inv\n",
    "        float(inv.sum()),                   # 8: sum_inv\n",
    "        float((d_sorted <= R1).mean()),     # 9: frac <= 600m\n",
    "        float((d_sorted <= R2).mean()),     # 10: frac <= 1200m\n",
    "        float((d_sorted <= R3).mean()),     # 11: frac <= 2400m\n",
    "    ]\n",
    "\n",
    "def iter_shards():\n",
    "    # Only use the POI shards (pattern provided); adapt if needed.\n",
    "    for p in sorted(SHARD_DIR.glob(\"shard_*.pkl\")):\n",
    "        yield p\n",
    "\n",
    "def extract_poi_distances_from_graph(g) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Get raw meters from a PyG Data 'g'.\n",
    "    Assumes updated shards stored meters in g.edge_attr (shape [E, 1]).\n",
    "    \"\"\"\n",
    "    if g is None or getattr(g, \"edge_attr\", None) is None:\n",
    "        return np.array([], dtype=float)\n",
    "    ea = g.edge_attr\n",
    "    if ea.dim() == 2 and ea.size(1) >= 1:\n",
    "        arr = ea.view(-1).detach().cpu().numpy()\n",
    "        return arr\n",
    "    # Fallback: treat as empty if shape is unexpected\n",
    "    return np.array([], dtype=float)\n",
    "\n",
    "# Collect per-apartment per-class baselines\n",
    "# Structure: results[apt_id][class] = list[12] or None\n",
    "results: Dict[int, Dict[str, Optional[List[float]]]] = {}\n",
    "\n",
    "count_shards = 0\n",
    "for shard in iter_shards():\n",
    "    with open(shard, \"rb\") as f:\n",
    "        d = pickle.load(f)  # dict[int -> dict[class -> Data|None]]\n",
    "    count_shards += 1\n",
    "\n",
    "    for apt_id, class_map in d.items():\n",
    "        # Ensure we have a dict for this apartment\n",
    "        if apt_id not in results:\n",
    "            results[apt_id] = {cls: None for cls in CLASSES}\n",
    "\n",
    "        # class_map should be dict[class -> Data|None]; compute per class\n",
    "        for cls in CLASSES:\n",
    "            g = class_map.get(cls, None)\n",
    "            if g is None:\n",
    "                # leave as None\n",
    "                continue\n",
    "            meters = extract_poi_distances_from_graph(g)\n",
    "            vec = baseline_from_meters(meters)\n",
    "            results[apt_id][cls] = vec\n",
    "\n",
    "print(f\"Processed {count_shards} shards. Apartments gathered: {len(results)}\")\n",
    "\n",
    "# Build output rows (keep JSON-encoded vectors; None stays None)\n",
    "rows = []\n",
    "for apt_id in sorted(results.keys()):\n",
    "    rec = {\"id\": int(apt_id)}\n",
    "    for cls in CLASSES:\n",
    "        vec = results[apt_id][cls]\n",
    "        rec[f\"emb_{cls}\"] = json.dumps(vec) if vec is not None else None\n",
    "    rows.append(rec)\n",
    "\n",
    "df = pd.DataFrame(rows, columns=[\"id\"] + [f\"emb_{c}\" for c in CLASSES])\n",
    "df.to_csv(OUT_CSV, index=False)\n",
    "print(\n",
    "    f\"✅ wrote {OUT_CSV} with {len(df)} rows. \"\n",
    "    + \"Non-null counts: \"\n",
    "    + \", \".join([f\"{c}={df[f'emb_{c}'].notna().sum()}\" for c in CLASSES])\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "051b53a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CWD: c:\\Users\\Pc-ADS\\Documents\\Cosas Universidad\\UNAB\\2024 sem 2\\Seminario licenciatura\\WebScraper\n",
      "Search roots checked:\n",
      " - WebScraper\\Graph_data\n",
      " - C:\\Users\\Pc-ADS\\Documents\\Cosas Universidad\\UNAB\\2024 sem 2\\Seminario licenciatura\\WebScraper\\Graph_data\n",
      " - C:\\Users\\Pc-ADS\\Documents\\Cosas Universidad\\UNAB\\2024 sem 2\\Seminario licenciatura\\WebScraper\\WebScraper\\Graph_data\n",
      " - C:\\Users\\Pc-ADS\\Documents\\Cosas Universidad\\UNAB\\2024 sem 2\\Seminario licenciatura\\WebScraper\\Graph_data\n",
      " - C:\\Users\\Pc-ADS\\Documents\\Cosas Universidad\\UNAB\\2024 sem 2\\Seminario licenciatura\\WebScraper\\Graph_data\n",
      " - C:\\Users\\Pc-ADS\\Documents\\Cosas Universidad\\UNAB\\2024 sem 2\\Seminario licenciatura\\Graph_data\n",
      "Found 9 shards:\n",
      "  • c:\\Users\\Pc-ADS\\Documents\\Cosas Universidad\\UNAB\\2024 sem 2\\Seminario licenciatura\\WebScraper\\Graph_data\\shard_20250829_183814_2862820058-1535195651.pkl\n",
      "  • Graph_data\\shard_20250829_183814_2862820058-1535195651.pkl\n",
      "  • ..\\WebScraper\\Graph_data\\shard_20250829_183814_2862820058-1535195651.pkl\n",
      "  • c:\\Users\\Pc-ADS\\Documents\\Cosas Universidad\\UNAB\\2024 sem 2\\Seminario licenciatura\\WebScraper\\Graph_data\\shard_20250829_155254_1555433697-1584388845.pkl\n",
      "  • Graph_data\\shard_20250829_155254_1555433697-1584388845.pkl\n",
      "  • ...\n",
      "Processed 9 shards. Apartments gathered: 25215\n",
      "✅ wrote embeddings_poi_from_shards.csv with 25215 rows. Non-null counts: sport_and_leisure=25123, medical=25087, education_prim=24519, veterinary=22982, food_and_drink_stores=24694, arts_and_entertainment=25007, food_and_drink=24205, park_like=24275, security=24564, religion=21291, education_sup=24581\n"
     ]
    }
   ],
   "source": [
    "# POI baseline embeddings (12-dim per class) → CSV (JSON columns, not expanded)\n",
    "\n",
    "import json, pickle, os, sys\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Ensure PyG Data is importable for unpickling\n",
    "from torch_geometric.data import Data  # noqa: F401\n",
    "\n",
    "# --- Config ---\n",
    "# Primary (as you said): shards live in WebScraper/Graph_data\n",
    "PREFERRED_DIR = Path(\"WebScraper/Graph_data\")\n",
    "OUT_CSV = \"embeddings_poi_from_shards.csv\"\n",
    "\n",
    "CLASSES: List[str] = [\n",
    "    \"sport_and_leisure\",\n",
    "    \"medical\",\n",
    "    \"education_prim\",\n",
    "    \"veterinary\",\n",
    "    \"food_and_drink_stores\",\n",
    "    \"arts_and_entertainment\",\n",
    "    \"food_and_drink\",\n",
    "    \"park_like\",\n",
    "    \"security\",\n",
    "    \"religion\",\n",
    "    \"education_sup\",\n",
    "]\n",
    "\n",
    "# POI thresholds (meters)\n",
    "R1, R2, R3 = 600.0, 1200.0, 2400.0\n",
    "EPS = 1e-6\n",
    "\n",
    "def baseline_from_meters(d: np.ndarray) -> Optional[List[float]]:\n",
    "    if d.size == 0:\n",
    "        return None\n",
    "    d = np.asarray(d, dtype=float)\n",
    "    d_sorted = np.sort(d)\n",
    "    inv = 1.0 / (d_sorted + EPS)\n",
    "    return [\n",
    "        float(d_sorted.size),               # 0: count\n",
    "        float(d_sorted.mean()),             # 1: mean\n",
    "        float(d_sorted.min()),              # 2: min\n",
    "        float(d_sorted.max()),              # 3: max\n",
    "        float(np.median(d_sorted)),         # 4: median\n",
    "        float(d_sorted.std()),              # 5: std\n",
    "        float(inv.mean()),                  # 6: mean_inv\n",
    "        float(inv.max()),                   # 7: max_inv\n",
    "        float(inv.sum()),                   # 8: sum_inv\n",
    "        float((d_sorted <= R1).mean()),     # 9: frac <= 600m\n",
    "        float((d_sorted <= R2).mean()),     # 10: frac <= 1200m\n",
    "        float((d_sorted <= R3).mean()),     # 11: frac <= 2400m\n",
    "    ]\n",
    "\n",
    "def extract_poi_distances_from_graph(g) -> np.ndarray:\n",
    "    # expects g.edge_attr to store raw meters (shape [E,1] or [E])\n",
    "    if g is None or getattr(g, \"edge_attr\", None) is None:\n",
    "        return np.array([], dtype=float)\n",
    "    ea = g.edge_attr\n",
    "    try:\n",
    "        return ea.view(-1).detach().cpu().numpy()\n",
    "    except Exception:\n",
    "        return np.array([], dtype=float)\n",
    "\n",
    "def find_shards() -> list[Path]:\n",
    "    # Try multiple roots to be robust to current working directory\n",
    "    roots = [\n",
    "        PREFERRED_DIR,\n",
    "        Path(\"Graph_data\"),\n",
    "        Path.cwd() / \"WebScraper\" / \"Graph_data\",\n",
    "        Path.cwd() / \"Graph_data\",\n",
    "        Path(\"..\") / \"WebScraper\" / \"Graph_data\",\n",
    "        Path(\"..\") / \"Graph_data\",\n",
    "    ]\n",
    "    candidates: set[Path] = set()\n",
    "    for r in roots:\n",
    "        if r.exists():\n",
    "            candidates.update(r.glob(\"shard_*.pkl\"))\n",
    "    # Fallback: recursive search from CWD if still nothing\n",
    "    if not candidates:\n",
    "        candidates = set(Path(\".\").rglob(\"shard_*.pkl\"))\n",
    "    shards = sorted(candidates, key=lambda p: p.stat().st_mtime)\n",
    "    # Debug print\n",
    "    print(f\"CWD: {Path.cwd()}\")\n",
    "    print(\"Search roots checked:\")\n",
    "    for r in roots:\n",
    "        print(\" -\", r.resolve())\n",
    "    print(f\"Found {len(shards)} shards:\")\n",
    "    for s in shards[:5]:\n",
    "        print(\"  •\", s)\n",
    "    if len(shards) > 5:\n",
    "        print(\"  • ...\")\n",
    "    return shards\n",
    "\n",
    "# ---- Run ----\n",
    "shards = find_shards()\n",
    "\n",
    "results: Dict[int, Dict[str, Optional[List[float]]]] = {}\n",
    "count_shards = 0\n",
    "\n",
    "for shard in shards:\n",
    "    with open(shard, \"rb\") as f:\n",
    "        d = pickle.load(f)  # dict[int -> dict[class -> Data|None]]\n",
    "    count_shards += 1\n",
    "\n",
    "    for apt_id, class_map in d.items():\n",
    "        # ensure apartment dict exists\n",
    "        if apt_id not in results:\n",
    "            results[apt_id] = {cls: None for cls in CLASSES}\n",
    "\n",
    "        # compute per class\n",
    "        if isinstance(class_map, dict):\n",
    "            for cls in CLASSES:\n",
    "                g = class_map.get(cls, None)\n",
    "                if g is None:\n",
    "                    continue\n",
    "                meters = extract_poi_distances_from_graph(g)\n",
    "                vec = baseline_from_meters(meters)\n",
    "                results[apt_id][cls] = vec\n",
    "        else:\n",
    "            # Unexpected structure; skip safely\n",
    "            continue\n",
    "\n",
    "print(f\"Processed {count_shards} shards. Apartments gathered: {len(results)}\")\n",
    "\n",
    "rows = []\n",
    "for apt_id in sorted(results.keys()):\n",
    "    rec = {\"id\": int(apt_id)}\n",
    "    for cls in CLASSES:\n",
    "        vec = results[apt_id][cls]\n",
    "        rec[f\"emb_{cls}\"] = json.dumps(vec) if vec is not None else None\n",
    "    rows.append(rec)\n",
    "\n",
    "df = pd.DataFrame(rows, columns=[\"id\"] + [f\"emb_{c}\" for c in CLASSES])\n",
    "df.to_csv(OUT_CSV, index=False)\n",
    "print(\n",
    "    f\"✅ wrote {OUT_CSV} with {len(df)} rows. \"\n",
    "    + \"Non-null counts: \"\n",
    "    + \", \".join([f\"{c}={df[f'emb_{c}'].notna().sum()}\" for c in CLASSES])\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e22098",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
