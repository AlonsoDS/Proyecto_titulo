{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0be2a104",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import math\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import root_mean_squared_error, mean_absolute_error, r2_score\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c4c1bc",
   "metadata": {},
   "source": [
    "### Configuración común a los modelos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "edff1199",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config\n",
    "SEED = 42\n",
    "TEST_SIZE = 0.2\n",
    "VAL_SIZE = 0.2\n",
    "BATCH_SIZE = 512\n",
    "EPOCHS = 1000  \n",
    "LR = 2e-3\n",
    "D_TOKEN = 128\n",
    "N_HEAD = 8\n",
    "N_LAYERS = 2\n",
    "FF_MULT = 2  # dim_feedforward = D_TOKEN * FF_MULT\n",
    "DROPOUT = 0.1\n",
    "WEIGHT_DECAY = 1e-4\n",
    "\n",
    "# Early stop MUY AGRESIVO\n",
    "#EARLY_STOP_PATIENCE = 50     # épocas sin mejora antes de cortar\n",
    "#MIN_DELTA_R2 = 1e-3          # mejora mínima en R^2(log) para resetear paciencia\n",
    "#LR_PATIENCE = 10             # épocas sin mejora antes de reducir LR\n",
    "#LR_FACTOR = 0.5              # multiplicador del LR cuando no mejora\n",
    "#MIN_LR = 1e-6                # LR mínimo\n",
    "\n",
    "# Early stop\n",
    "EARLY_STOP_PATIENCE = 150     # épocas sin mejora antes de cortar\n",
    "MIN_DELTA_R2 = 5e-4          # mejora mínima en R^2(log) para resetear paciencia\n",
    "LR_PATIENCE = 25             # épocas sin mejora antes de reducir LR\n",
    "LR_FACTOR = 0.5              # multiplicador del LR cuando no mejora\n",
    "MIN_LR = 1e-5                # LR mínimo\n",
    "\n",
    "# Reproducibilidad\n",
    "def set_seed(seed: int) -> None:\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d86b1d2",
   "metadata": {},
   "source": [
    "### Primer modelo FT-Transformer --> Sin VCR ni coordenadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "8914754c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    25211.000000\n",
       "mean         8.395828\n",
       "std          0.830310\n",
       "min          5.950643\n",
       "25%          7.740664\n",
       "50%          8.242756\n",
       "75%          8.984694\n",
       "max         10.915088\n",
       "Name: log_monto, dtype: float64"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Configuración básica del dataset\n",
    "df_vcr_c = pd.read_csv('dataset_vcr_compact.csv')\n",
    "df_vcr_c = df_vcr_c[df_vcr_c['monto'] < 56000].copy()\n",
    "df_vcr_c['log_monto']=np.log(df_vcr_c['monto'])\n",
    "df_vcr_c['log_monto'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "294c4141",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 25211 entries, 0 to 25214\n",
      "Data columns (total 23 columns):\n",
      " #   Column                Non-Null Count  Dtype  \n",
      "---  ------                --------------  -----  \n",
      " 0   monto                 25211 non-null  int64  \n",
      " 1   superficie_t          25211 non-null  float64\n",
      " 2   dormitorios           25211 non-null  int64  \n",
      " 3   dormitorios_faltante  25211 non-null  int64  \n",
      " 4   banos                 25211 non-null  int64  \n",
      " 5   banos_faltante        25211 non-null  int64  \n",
      " 6   antiguedad            25211 non-null  int64  \n",
      " 7   antiguedad_faltante   25211 non-null  int64  \n",
      " 8   Or_N                  25211 non-null  int64  \n",
      " 9   Or_S                  25211 non-null  int64  \n",
      " 10  Or_E                  25211 non-null  int64  \n",
      " 11  Or_O                  25211 non-null  int64  \n",
      " 12  Or_Faltante           25211 non-null  int64  \n",
      " 13  terraza               25211 non-null  float64\n",
      " 14  estacionamiento       25211 non-null  int64  \n",
      " 15  bodegas               25211 non-null  int64  \n",
      " 16  flag_Departamento     25211 non-null  int64  \n",
      " 17  flag_Multinivel       25211 non-null  int64  \n",
      " 18  flag_Semipiso         25211 non-null  int64  \n",
      " 19  flag_Premium          25211 non-null  int64  \n",
      " 20  flag_Monoambiente     25211 non-null  int64  \n",
      " 21  flag_Loft             25211 non-null  int64  \n",
      " 22  log_monto             25211 non-null  float64\n",
      "dtypes: float64(3), int64(20)\n",
      "memory usage: 4.6 MB\n"
     ]
    }
   ],
   "source": [
    "#Configuración específica del modelo\n",
    "df_base =df_vcr_c.copy()\n",
    "obj_cols = df_base.select_dtypes(include=[\"object\"]).columns\n",
    "cols_to_drop = list(obj_cols) + [\"id\", \"latitud\", \"longitud\"]\n",
    "df_base = df_base.drop(columns=cols_to_drop)\n",
    "df_base.info(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "3c04f678",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_features: 21\n"
     ]
    }
   ],
   "source": [
    "# Split\n",
    "X_df = df_base.drop(columns=[\"monto\", \"log_monto\"]).copy()\n",
    "y = df_base[\"log_monto\"].values.astype(np.float32)\n",
    "\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "    X_df.values, y, test_size=TEST_SIZE, random_state=SEED\n",
    ")\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_full, y_train_full, test_size=VAL_SIZE, random_state=SEED\n",
    ")\n",
    "\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train = scaler.transform(X_train).astype(np.float32)\n",
    "X_val = scaler.transform(X_val).astype(np.float32)\n",
    "X_test = scaler.transform(X_test).astype(np.float32)\n",
    "\n",
    "n_features = X_train.shape[1]\n",
    "print(f\"n_features: {n_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "d46b2d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset & Loader\n",
    "class NpDataset(Dataset):\n",
    "    def __init__(self, X: np.ndarray, y: np.ndarray):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            torch.from_numpy(self.X[idx]),\n",
    "            torch.tensor(self.y[idx], dtype=torch.float32),\n",
    "        )\n",
    "\n",
    "train_dl = DataLoader(NpDataset(X_train, y_train), batch_size=BATCH_SIZE, shuffle=True, drop_last=False, pin_memory=True)\n",
    "val_dl = DataLoader(NpDataset(X_val, y_val), batch_size=BATCH_SIZE, shuffle=False, drop_last=False, pin_memory=True)\n",
    "test_dl = DataLoader(NpDataset(X_test, y_test), batch_size=BATCH_SIZE, shuffle=False, drop_last=False, pin_memory=True)\n",
    "\n",
    "# Modelo\n",
    "class NumericTokenizer(nn.Module):\n",
    "    \"\"\"Mapea features numéricas a tokens: token_i = x_i * W_i + b_i\"\"\"\n",
    "    def __init__(self, n_features: int, d_token: int):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.empty(n_features, d_token))\n",
    "        self.bias = nn.Parameter(torch.zeros(n_features, d_token))\n",
    "        nn.init.xavier_uniform_(self.weight)\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x: (B, F) -> (B, F, D)\n",
    "        return x.unsqueeze(-1) * self.weight + self.bias\n",
    "\n",
    "class FTTransformer(nn.Module):\n",
    "    def __init__(self, n_features: int, d_token: int, n_layers: int, n_head: int, ff_mult: int, dropout: float):\n",
    "        super().__init__()\n",
    "        self.tokenizer = NumericTokenizer(n_features, d_token)\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, d_token))\n",
    "        enc_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_token,\n",
    "            nhead=n_head,\n",
    "            dim_feedforward=d_token * ff_mult,\n",
    "            dropout=dropout,\n",
    "            batch_first=True,\n",
    "            activation=\"gelu\",\n",
    "            norm_first=True,\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(enc_layer, num_layers=n_layers)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.LayerNorm(d_token),\n",
    "            nn.Linear(d_token, d_token),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_token, 1),\n",
    "        )\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        B = x.size(0)\n",
    "        tokens = self.tokenizer(x)                  # (B, F, D)\n",
    "        cls = self.cls_token.expand(B, -1, -1)     # (B, 1, D)\n",
    "        x_tok = torch.cat([cls, tokens], dim=1)    # (B, 1+F, D)\n",
    "        x_enc = self.encoder(x_tok)                # (B, 1+F, D)\n",
    "        cls_out = x_enc[:, 0, :]                   # (B, D)\n",
    "        y = self.head(cls_out).squeeze(-1)         # (B,)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "312897f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01/1000 | train MSE: 8.71393 | val R2_log: -0.0527 | val RMSE: 6,543.30 | val MAE: 4,395.52 | val MAPE: 103.97%\n",
      "Epoch 02/1000 | train MSE: 0.76933 | val R2_log: 0.5966 | val RMSE: 4,792.93 | val MAE: 2,741.22 | val MAPE: 54.46%\n",
      "Epoch 03/1000 | train MSE: 0.38030 | val R2_log: 0.7546 | val RMSE: 3,600.20 | val MAE: 2,156.32 | val MAPE: 41.28%\n",
      "Epoch 04/1000 | train MSE: 0.33068 | val R2_log: 0.8038 | val RMSE: 3,113.88 | val MAE: 1,896.76 | val MAPE: 36.11%\n",
      "Epoch 05/1000 | train MSE: 0.29379 | val R2_log: 0.8360 | val RMSE: 2,947.72 | val MAE: 1,748.18 | val MAPE: 32.10%\n",
      "Epoch 06/1000 | train MSE: 0.26657 | val R2_log: 0.8587 | val RMSE: 2,596.58 | val MAE: 1,548.82 | val MAPE: 28.93%\n",
      "Epoch 07/1000 | train MSE: 0.25177 | val R2_log: 0.8675 | val RMSE: 2,360.86 | val MAE: 1,412.64 | val MAPE: 27.57%\n",
      "Epoch 08/1000 | train MSE: 0.24070 | val R2_log: 0.8584 | val RMSE: 2,428.89 | val MAE: 1,484.85 | val MAPE: 29.21%\n",
      "Epoch 09/1000 | train MSE: 0.23408 | val R2_log: 0.8573 | val RMSE: 2,561.65 | val MAE: 1,556.10 | val MAPE: 29.77%\n",
      "Epoch 10/1000 | train MSE: 0.23008 | val R2_log: 0.8582 | val RMSE: 2,376.64 | val MAE: 1,449.70 | val MAPE: 29.48%\n",
      "Epoch 11/1000 | train MSE: 0.22215 | val R2_log: 0.8778 | val RMSE: 2,400.74 | val MAE: 1,376.40 | val MAPE: 25.72%\n",
      "Epoch 12/1000 | train MSE: 0.21356 | val R2_log: 0.8899 | val RMSE: 2,267.19 | val MAE: 1,319.68 | val MAPE: 23.44%\n",
      "Epoch 13/1000 | train MSE: 0.21079 | val R2_log: 0.8699 | val RMSE: 2,606.41 | val MAE: 1,531.68 | val MAPE: 27.64%\n",
      "Epoch 14/1000 | train MSE: 0.21108 | val R2_log: 0.8136 | val RMSE: 2,754.63 | val MAE: 1,793.59 | val MAPE: 36.68%\n",
      "Epoch 15/1000 | train MSE: 0.21066 | val R2_log: 0.8862 | val RMSE: 2,412.07 | val MAE: 1,385.50 | val MAPE: 24.41%\n",
      "Epoch 16/1000 | train MSE: 0.21191 | val R2_log: 0.8821 | val RMSE: 2,356.75 | val MAE: 1,403.58 | val MAPE: 25.28%\n",
      "Epoch 17/1000 | train MSE: 0.20288 | val R2_log: 0.7910 | val RMSE: 2,618.65 | val MAE: 1,775.33 | val MAPE: 39.76%\n",
      "Epoch 18/1000 | train MSE: 0.20731 | val R2_log: 0.8897 | val RMSE: 2,426.63 | val MAE: 1,379.40 | val MAPE: 23.31%\n",
      "Epoch 19/1000 | train MSE: 0.20128 | val R2_log: 0.8511 | val RMSE: 2,689.92 | val MAE: 1,602.38 | val MAPE: 30.83%\n",
      "Epoch 20/1000 | train MSE: 0.19613 | val R2_log: 0.8627 | val RMSE: 3,005.67 | val MAE: 1,670.35 | val MAPE: 28.69%\n",
      "Epoch 21/1000 | train MSE: 0.20241 | val R2_log: 0.8967 | val RMSE: 2,193.06 | val MAE: 1,227.04 | val MAPE: 20.55%\n",
      "Epoch 22/1000 | train MSE: 0.20143 | val R2_log: 0.8657 | val RMSE: 2,588.00 | val MAE: 1,542.38 | val MAPE: 28.42%\n",
      "Epoch 23/1000 | train MSE: 0.19376 | val R2_log: 0.8833 | val RMSE: 2,507.07 | val MAE: 1,450.14 | val MAPE: 25.17%\n",
      "Epoch 24/1000 | train MSE: 0.19078 | val R2_log: 0.8404 | val RMSE: 2,626.86 | val MAE: 1,660.13 | val MAPE: 32.66%\n",
      "Epoch 25/1000 | train MSE: 0.19549 | val R2_log: 0.8853 | val RMSE: 2,572.23 | val MAE: 1,453.20 | val MAPE: 24.88%\n",
      "Epoch 26/1000 | train MSE: 0.18824 | val R2_log: 0.8186 | val RMSE: 3,263.05 | val MAE: 2,018.46 | val MAPE: 36.21%\n",
      "Epoch 27/1000 | train MSE: 0.19013 | val R2_log: 0.8620 | val RMSE: 2,885.31 | val MAE: 1,721.78 | val MAPE: 29.09%\n",
      "Epoch 28/1000 | train MSE: 0.18479 | val R2_log: 0.8852 | val RMSE: 2,564.52 | val MAE: 1,462.76 | val MAPE: 24.88%\n",
      "Epoch 29/1000 | train MSE: 0.18385 | val R2_log: 0.8463 | val RMSE: 2,795.24 | val MAE: 1,685.97 | val MAPE: 31.78%\n",
      "Epoch 30/1000 | train MSE: 0.18541 | val R2_log: 0.8986 | val RMSE: 2,158.86 | val MAE: 1,241.68 | val MAPE: 22.28%\n",
      "Epoch 31/1000 | train MSE: 0.18784 | val R2_log: 0.8583 | val RMSE: 2,726.50 | val MAE: 1,593.22 | val MAPE: 29.81%\n",
      "Epoch 32/1000 | train MSE: 0.19043 | val R2_log: 0.8289 | val RMSE: 3,425.80 | val MAE: 2,022.38 | val MAPE: 34.59%\n",
      "Epoch 33/1000 | train MSE: 0.18345 | val R2_log: 0.8988 | val RMSE: 2,211.85 | val MAE: 1,250.95 | val MAPE: 20.94%\n",
      "Epoch 34/1000 | train MSE: 0.17889 | val R2_log: 0.8601 | val RMSE: 2,729.35 | val MAE: 1,620.30 | val MAPE: 29.60%\n",
      "Epoch 35/1000 | train MSE: 0.18020 | val R2_log: 0.8382 | val RMSE: 2,925.90 | val MAE: 1,755.55 | val MAPE: 33.13%\n",
      "Epoch 36/1000 | train MSE: 0.17978 | val R2_log: 0.8692 | val RMSE: 2,558.11 | val MAE: 1,518.55 | val MAPE: 28.06%\n",
      "Epoch 37/1000 | train MSE: 0.18170 | val R2_log: 0.9001 | val RMSE: 2,211.02 | val MAE: 1,238.70 | val MAPE: 21.70%\n",
      "Epoch 38/1000 | train MSE: 0.17639 | val R2_log: 0.8670 | val RMSE: 2,209.76 | val MAE: 1,344.70 | val MAPE: 28.12%\n",
      "Epoch 39/1000 | train MSE: 0.17886 | val R2_log: 0.9027 | val RMSE: 2,151.28 | val MAE: 1,199.20 | val MAPE: 21.14%\n",
      "Epoch 40/1000 | train MSE: 0.17507 | val R2_log: 0.8856 | val RMSE: 2,247.76 | val MAE: 1,318.53 | val MAPE: 25.09%\n",
      "Epoch 41/1000 | train MSE: 0.17564 | val R2_log: 0.8849 | val RMSE: 2,751.39 | val MAE: 1,511.66 | val MAPE: 25.14%\n",
      "Epoch 42/1000 | train MSE: 0.17963 | val R2_log: 0.8965 | val RMSE: 2,360.05 | val MAE: 1,291.97 | val MAPE: 20.50%\n",
      "Epoch 43/1000 | train MSE: 0.17612 | val R2_log: 0.8335 | val RMSE: 2,858.89 | val MAE: 1,773.80 | val MAPE: 33.98%\n",
      "Epoch 44/1000 | train MSE: 0.17153 | val R2_log: 0.8586 | val RMSE: 2,896.55 | val MAE: 1,727.82 | val MAPE: 29.90%\n",
      "Epoch 45/1000 | train MSE: 0.17256 | val R2_log: 0.8941 | val RMSE: 2,318.93 | val MAE: 1,316.66 | val MAPE: 23.52%\n",
      "Epoch 46/1000 | train MSE: 0.16862 | val R2_log: 0.8850 | val RMSE: 2,465.56 | val MAE: 1,424.85 | val MAPE: 25.28%\n",
      "Epoch 47/1000 | train MSE: 0.17130 | val R2_log: 0.8970 | val RMSE: 2,421.03 | val MAE: 1,338.14 | val MAPE: 22.76%\n",
      "Epoch 48/1000 | train MSE: 0.17112 | val R2_log: 0.8672 | val RMSE: 2,364.70 | val MAE: 1,444.56 | val MAPE: 28.48%\n",
      "Epoch 49/1000 | train MSE: 0.16475 | val R2_log: 0.8983 | val RMSE: 2,425.72 | val MAE: 1,353.41 | val MAPE: 22.24%\n",
      "Epoch 50/1000 | train MSE: 0.16601 | val R2_log: 0.8840 | val RMSE: 2,558.39 | val MAE: 1,490.41 | val MAPE: 25.38%\n",
      "Epoch 51/1000 | train MSE: 0.16502 | val R2_log: 0.8531 | val RMSE: 2,657.90 | val MAE: 1,611.97 | val MAPE: 30.94%\n",
      "Epoch 52/1000 | train MSE: 0.16671 | val R2_log: 0.9008 | val RMSE: 2,278.37 | val MAE: 1,273.53 | val MAPE: 21.10%\n",
      "Epoch 53/1000 | train MSE: 0.16332 | val R2_log: 0.8963 | val RMSE: 2,300.95 | val MAE: 1,310.67 | val MAPE: 22.85%\n",
      "Epoch 54/1000 | train MSE: 0.16187 | val R2_log: 0.8874 | val RMSE: 2,506.11 | val MAE: 1,451.08 | val MAPE: 24.56%\n",
      "Epoch 55/1000 | train MSE: 0.16376 | val R2_log: 0.8463 | val RMSE: 2,459.85 | val MAE: 1,576.14 | val MAPE: 31.93%\n",
      "Epoch 56/1000 | train MSE: 0.16736 | val R2_log: 0.8317 | val RMSE: 3,366.66 | val MAE: 2,010.08 | val MAPE: 34.35%\n",
      "Epoch 57/1000 | train MSE: 0.16418 | val R2_log: 0.8775 | val RMSE: 2,593.19 | val MAE: 1,519.89 | val MAPE: 26.81%\n",
      "Epoch 58/1000 | train MSE: 0.15657 | val R2_log: 0.8888 | val RMSE: 2,282.52 | val MAE: 1,332.74 | val MAPE: 24.75%\n",
      "Epoch 59/1000 | train MSE: 0.15698 | val R2_log: 0.9029 | val RMSE: 2,165.70 | val MAE: 1,201.38 | val MAPE: 19.87%\n",
      "Epoch 60/1000 | train MSE: 0.15885 | val R2_log: 0.8589 | val RMSE: 2,798.89 | val MAE: 1,638.89 | val MAPE: 29.99%\n",
      "Epoch 61/1000 | train MSE: 0.15613 | val R2_log: 0.8985 | val RMSE: 2,526.89 | val MAE: 1,350.26 | val MAPE: 22.33%\n",
      "Epoch 62/1000 | train MSE: 0.15497 | val R2_log: 0.8763 | val RMSE: 2,660.13 | val MAE: 1,506.66 | val MAPE: 27.03%\n",
      "Epoch 63/1000 | train MSE: 0.15272 | val R2_log: 0.8967 | val RMSE: 2,374.72 | val MAE: 1,333.71 | val MAPE: 23.06%\n",
      "Epoch 64/1000 | train MSE: 0.15433 | val R2_log: 0.8919 | val RMSE: 2,679.07 | val MAE: 1,435.16 | val MAPE: 23.99%\n",
      "Epoch 65/1000 | train MSE: 0.15109 | val R2_log: 0.9013 | val RMSE: 2,093.79 | val MAE: 1,181.59 | val MAPE: 21.94%\n",
      "Epoch 66/1000 | train MSE: 0.14908 | val R2_log: 0.8901 | val RMSE: 2,383.70 | val MAE: 1,380.91 | val MAPE: 24.25%\n",
      "Epoch 67/1000 | train MSE: 0.15208 | val R2_log: 0.9036 | val RMSE: 2,143.90 | val MAE: 1,181.89 | val MAPE: 19.81%\n",
      "Epoch 68/1000 | train MSE: 0.15364 | val R2_log: 0.9033 | val RMSE: 2,153.19 | val MAE: 1,183.39 | val MAPE: 19.69%\n",
      "Epoch 69/1000 | train MSE: 0.14989 | val R2_log: 0.8908 | val RMSE: 2,185.70 | val MAE: 1,276.99 | val MAPE: 24.18%\n",
      "Epoch 70/1000 | train MSE: 0.15078 | val R2_log: 0.8867 | val RMSE: 2,499.91 | val MAE: 1,430.68 | val MAPE: 24.83%\n",
      "Epoch 71/1000 | train MSE: 0.14683 | val R2_log: 0.9041 | val RMSE: 2,133.11 | val MAE: 1,193.88 | val MAPE: 20.72%\n",
      "Epoch 72/1000 | train MSE: 0.14698 | val R2_log: 0.9024 | val RMSE: 2,287.50 | val MAE: 1,274.59 | val MAPE: 21.81%\n",
      "Epoch 73/1000 | train MSE: 0.14813 | val R2_log: 0.9039 | val RMSE: 2,280.41 | val MAE: 1,252.44 | val MAPE: 21.06%\n",
      "Epoch 74/1000 | train MSE: 0.14582 | val R2_log: 0.9000 | val RMSE: 2,166.22 | val MAE: 1,226.89 | val MAPE: 22.42%\n",
      "Epoch 75/1000 | train MSE: 0.14529 | val R2_log: 0.8951 | val RMSE: 2,527.62 | val MAE: 1,392.17 | val MAPE: 23.42%\n",
      "Epoch 76/1000 | train MSE: 0.14408 | val R2_log: 0.8903 | val RMSE: 2,183.83 | val MAE: 1,271.08 | val MAPE: 24.43%\n",
      "Epoch 77/1000 | train MSE: 0.14170 | val R2_log: 0.8854 | val RMSE: 2,412.56 | val MAE: 1,363.56 | val MAPE: 25.28%\n",
      "Epoch 78/1000 | train MSE: 0.14482 | val R2_log: 0.8840 | val RMSE: 2,143.99 | val MAE: 1,271.16 | val MAPE: 25.50%\n",
      "Epoch 79/1000 | train MSE: 0.14234 | val R2_log: 0.8908 | val RMSE: 2,278.99 | val MAE: 1,323.21 | val MAPE: 24.48%\n",
      "Epoch 80/1000 | train MSE: 0.14331 | val R2_log: 0.8570 | val RMSE: 2,549.54 | val MAE: 1,564.88 | val MAPE: 30.31%\n",
      "Epoch 81/1000 | train MSE: 0.14380 | val R2_log: 0.8901 | val RMSE: 2,204.93 | val MAE: 1,267.61 | val MAPE: 24.42%\n",
      "Epoch 82/1000 | train MSE: 0.14448 | val R2_log: 0.9044 | val RMSE: 2,258.08 | val MAE: 1,256.44 | val MAPE: 20.97%\n",
      "Epoch 83/1000 | train MSE: 0.13777 | val R2_log: 0.8846 | val RMSE: 2,305.01 | val MAE: 1,331.79 | val MAPE: 25.41%\n",
      "Epoch 84/1000 | train MSE: 0.13975 | val R2_log: 0.8795 | val RMSE: 2,699.06 | val MAE: 1,483.40 | val MAPE: 26.42%\n",
      "Epoch 85/1000 | train MSE: 0.13649 | val R2_log: 0.9046 | val RMSE: 2,233.39 | val MAE: 1,233.52 | val MAPE: 21.22%\n",
      "Epoch 86/1000 | train MSE: 0.13814 | val R2_log: 0.8754 | val RMSE: 2,426.70 | val MAE: 1,439.94 | val MAPE: 27.11%\n",
      "Epoch 87/1000 | train MSE: 0.14109 | val R2_log: 0.8693 | val RMSE: 2,514.09 | val MAE: 1,533.46 | val MAPE: 28.34%\n",
      "Epoch 88/1000 | train MSE: 0.14889 | val R2_log: 0.8886 | val RMSE: 2,277.69 | val MAE: 1,326.66 | val MAPE: 24.80%\n",
      "Epoch 89/1000 | train MSE: 0.14314 | val R2_log: 0.8944 | val RMSE: 2,244.72 | val MAE: 1,281.97 | val MAPE: 23.52%\n",
      "Epoch 90/1000 | train MSE: 0.13840 | val R2_log: 0.9031 | val RMSE: 2,178.25 | val MAE: 1,193.81 | val MAPE: 21.43%\n",
      "Epoch 91/1000 | train MSE: 0.13711 | val R2_log: 0.9005 | val RMSE: 2,123.74 | val MAE: 1,186.64 | val MAPE: 22.14%\n",
      "Epoch 92/1000 | train MSE: 0.13508 | val R2_log: 0.8829 | val RMSE: 2,688.26 | val MAE: 1,521.06 | val MAPE: 25.74%\n",
      "Epoch 93/1000 | train MSE: 0.13434 | val R2_log: 0.9007 | val RMSE: 2,189.33 | val MAE: 1,225.05 | val MAPE: 22.25%\n",
      "Epoch 94/1000 | train MSE: 0.13462 | val R2_log: 0.9036 | val RMSE: 2,368.39 | val MAE: 1,286.53 | val MAPE: 21.28%\n",
      "Epoch 95/1000 | train MSE: 0.13508 | val R2_log: 0.8717 | val RMSE: 2,924.19 | val MAE: 1,643.99 | val MAPE: 27.95%\n",
      "Epoch 96/1000 | train MSE: 0.13834 | val R2_log: 0.8897 | val RMSE: 2,346.59 | val MAE: 1,363.01 | val MAPE: 24.43%\n",
      "Epoch 97/1000 | train MSE: 0.13265 | val R2_log: 0.8906 | val RMSE: 2,537.12 | val MAE: 1,419.64 | val MAPE: 24.35%\n",
      "Epoch 98/1000 | train MSE: 0.13112 | val R2_log: 0.9042 | val RMSE: 2,177.40 | val MAE: 1,216.68 | val MAPE: 20.92%\n",
      "Epoch 99/1000 | train MSE: 0.13357 | val R2_log: 0.8865 | val RMSE: 2,571.13 | val MAE: 1,466.30 | val MAPE: 25.16%\n",
      "Epoch 100/1000 | train MSE: 0.13297 | val R2_log: 0.8822 | val RMSE: 2,814.25 | val MAE: 1,543.80 | val MAPE: 25.92%\n",
      "Epoch 101/1000 | train MSE: 0.13110 | val R2_log: 0.9045 | val RMSE: 2,304.92 | val MAE: 1,243.03 | val MAPE: 20.93%\n",
      "Epoch 102/1000 | train MSE: 0.13108 | val R2_log: 0.8530 | val RMSE: 2,827.74 | val MAE: 1,686.28 | val MAPE: 31.15%\n",
      "Epoch 103/1000 | train MSE: 0.13287 | val R2_log: 0.8969 | val RMSE: 2,358.02 | val MAE: 1,305.25 | val MAPE: 22.95%\n",
      "Epoch 104/1000 | train MSE: 0.13028 | val R2_log: 0.9023 | val RMSE: 2,128.49 | val MAE: 1,192.82 | val MAPE: 21.86%\n",
      "Epoch 105/1000 | train MSE: 0.12840 | val R2_log: 0.8906 | val RMSE: 2,262.52 | val MAE: 1,323.91 | val MAPE: 24.40%\n",
      "Epoch 106/1000 | train MSE: 0.13083 | val R2_log: 0.8830 | val RMSE: 2,566.98 | val MAE: 1,446.96 | val MAPE: 25.72%\n",
      "Epoch 107/1000 | train MSE: 0.13336 | val R2_log: 0.9056 | val RMSE: 2,120.15 | val MAE: 1,187.46 | val MAPE: 20.55%\n",
      "Epoch 108/1000 | train MSE: 0.13015 | val R2_log: 0.8475 | val RMSE: 2,492.40 | val MAE: 1,575.24 | val MAPE: 31.85%\n",
      "Epoch 109/1000 | train MSE: 0.13038 | val R2_log: 0.9052 | val RMSE: 2,104.67 | val MAE: 1,160.31 | val MAPE: 20.77%\n",
      "Epoch 110/1000 | train MSE: 0.12647 | val R2_log: 0.9009 | val RMSE: 2,234.95 | val MAE: 1,237.98 | val MAPE: 22.19%\n",
      "Epoch 111/1000 | train MSE: 0.12881 | val R2_log: 0.9048 | val RMSE: 2,171.79 | val MAE: 1,209.96 | val MAPE: 21.06%\n",
      "Epoch 112/1000 | train MSE: 0.12673 | val R2_log: 0.8676 | val RMSE: 2,580.59 | val MAE: 1,510.22 | val MAPE: 28.45%\n",
      "Epoch 113/1000 | train MSE: 0.12741 | val R2_log: 0.8925 | val RMSE: 2,629.59 | val MAE: 1,463.85 | val MAPE: 23.97%\n",
      "Epoch 114/1000 | train MSE: 0.12586 | val R2_log: 0.8896 | val RMSE: 2,371.27 | val MAE: 1,331.50 | val MAPE: 24.52%\n",
      "Epoch 115/1000 | train MSE: 0.12756 | val R2_log: 0.8826 | val RMSE: 2,180.58 | val MAE: 1,295.18 | val MAPE: 25.73%\n",
      "Epoch 116/1000 | train MSE: 0.12579 | val R2_log: 0.8976 | val RMSE: 2,232.26 | val MAE: 1,271.20 | val MAPE: 22.94%\n",
      "Epoch 117/1000 | train MSE: 0.12562 | val R2_log: 0.9019 | val RMSE: 2,246.58 | val MAE: 1,267.24 | val MAPE: 21.44%\n",
      "Epoch 118/1000 | train MSE: 0.12484 | val R2_log: 0.8896 | val RMSE: 2,929.06 | val MAE: 1,569.48 | val MAPE: 24.14%\n",
      "Epoch 119/1000 | train MSE: 0.12733 | val R2_log: 0.8929 | val RMSE: 2,228.03 | val MAE: 1,269.45 | val MAPE: 23.65%\n",
      "Epoch 120/1000 | train MSE: 0.12348 | val R2_log: 0.8843 | val RMSE: 2,604.30 | val MAE: 1,496.48 | val MAPE: 25.61%\n",
      "Epoch 121/1000 | train MSE: 0.12776 | val R2_log: 0.9017 | val RMSE: 2,237.90 | val MAE: 1,246.91 | val MAPE: 22.01%\n",
      "Epoch 122/1000 | train MSE: 0.12894 | val R2_log: 0.9065 | val RMSE: 2,141.51 | val MAE: 1,163.51 | val MAPE: 19.65%\n",
      "Epoch 123/1000 | train MSE: 0.12670 | val R2_log: 0.8949 | val RMSE: 2,439.03 | val MAE: 1,379.09 | val MAPE: 22.26%\n",
      "Epoch 124/1000 | train MSE: 0.13037 | val R2_log: 0.9005 | val RMSE: 2,437.41 | val MAE: 1,329.63 | val MAPE: 21.98%\n",
      "Epoch 125/1000 | train MSE: 0.12517 | val R2_log: 0.8955 | val RMSE: 2,312.04 | val MAE: 1,302.07 | val MAPE: 23.48%\n",
      "Epoch 126/1000 | train MSE: 0.12362 | val R2_log: 0.8964 | val RMSE: 2,277.48 | val MAE: 1,304.17 | val MAPE: 23.07%\n",
      "Epoch 127/1000 | train MSE: 0.12530 | val R2_log: 0.8818 | val RMSE: 2,520.84 | val MAE: 1,457.78 | val MAPE: 26.16%\n",
      "Epoch 128/1000 | train MSE: 0.12517 | val R2_log: 0.9050 | val RMSE: 2,274.67 | val MAE: 1,247.13 | val MAPE: 21.04%\n",
      "Epoch 129/1000 | train MSE: 0.12494 | val R2_log: 0.8893 | val RMSE: 2,917.45 | val MAE: 1,573.30 | val MAPE: 24.41%\n",
      "Epoch 130/1000 | train MSE: 0.12534 | val R2_log: 0.8861 | val RMSE: 2,436.96 | val MAE: 1,420.87 | val MAPE: 25.03%\n",
      "Epoch 131/1000 | train MSE: 0.12693 | val R2_log: 0.9063 | val RMSE: 2,133.22 | val MAE: 1,167.54 | val MAPE: 20.44%\n",
      "Epoch 132/1000 | train MSE: 0.12752 | val R2_log: 0.9006 | val RMSE: 2,125.71 | val MAE: 1,187.23 | val MAPE: 22.16%\n",
      "Epoch 133/1000 | train MSE: 0.12300 | val R2_log: 0.9010 | val RMSE: 2,439.79 | val MAE: 1,331.97 | val MAPE: 22.23%\n",
      "Epoch 134/1000 | train MSE: 0.11957 | val R2_log: 0.8865 | val RMSE: 2,240.08 | val MAE: 1,265.21 | val MAPE: 20.35%\n",
      "Epoch 135/1000 | train MSE: 0.12727 | val R2_log: 0.9009 | val RMSE: 2,155.49 | val MAE: 1,227.01 | val MAPE: 22.06%\n",
      "Epoch 136/1000 | train MSE: 0.12299 | val R2_log: 0.8826 | val RMSE: 2,201.56 | val MAE: 1,294.16 | val MAPE: 25.64%\n",
      "Epoch 137/1000 | train MSE: 0.12202 | val R2_log: 0.8649 | val RMSE: 2,794.58 | val MAE: 1,601.78 | val MAPE: 29.06%\n",
      "Epoch 138/1000 | train MSE: 0.12436 | val R2_log: 0.8867 | val RMSE: 2,526.00 | val MAE: 1,420.21 | val MAPE: 25.06%\n",
      "Epoch 139/1000 | train MSE: 0.12379 | val R2_log: 0.9066 | val RMSE: 2,174.55 | val MAE: 1,189.20 | val MAPE: 20.08%\n",
      "Epoch 140/1000 | train MSE: 0.12188 | val R2_log: 0.9059 | val RMSE: 2,168.67 | val MAE: 1,171.05 | val MAPE: 20.12%\n",
      "Epoch 141/1000 | train MSE: 0.12474 | val R2_log: 0.9014 | val RMSE: 2,244.46 | val MAE: 1,198.92 | val MAPE: 19.71%\n",
      "Epoch 142/1000 | train MSE: 0.12297 | val R2_log: 0.8955 | val RMSE: 2,184.77 | val MAE: 1,216.12 | val MAPE: 23.22%\n",
      "Epoch 143/1000 | train MSE: 0.12289 | val R2_log: 0.8702 | val RMSE: 3,267.52 | val MAE: 1,820.76 | val MAPE: 27.92%\n",
      "Epoch 144/1000 | train MSE: 0.12001 | val R2_log: 0.8890 | val RMSE: 2,198.04 | val MAE: 1,285.79 | val MAPE: 24.76%\n",
      "Epoch 145/1000 | train MSE: 0.12319 | val R2_log: 0.8975 | val RMSE: 2,519.71 | val MAE: 1,367.00 | val MAPE: 22.99%\n",
      "Epoch 146/1000 | train MSE: 0.12139 | val R2_log: 0.9027 | val RMSE: 2,107.69 | val MAE: 1,175.57 | val MAPE: 21.80%\n",
      "Epoch 147/1000 | train MSE: 0.12327 | val R2_log: 0.9002 | val RMSE: 2,132.32 | val MAE: 1,210.05 | val MAPE: 22.20%\n",
      "Epoch 148/1000 | train MSE: 0.12096 | val R2_log: 0.8894 | val RMSE: 2,618.14 | val MAE: 1,440.48 | val MAPE: 24.56%\n",
      "Epoch 149/1000 | train MSE: 0.12306 | val R2_log: 0.8634 | val RMSE: 2,859.57 | val MAE: 1,702.86 | val MAPE: 29.27%\n",
      "Epoch 150/1000 | train MSE: 0.12509 | val R2_log: 0.8906 | val RMSE: 2,162.09 | val MAE: 1,247.93 | val MAPE: 24.21%\n",
      "Epoch 151/1000 | train MSE: 0.11783 | val R2_log: 0.9066 | val RMSE: 2,148.06 | val MAE: 1,162.11 | val MAPE: 20.08%\n",
      "Epoch 152/1000 | train MSE: 0.12157 | val R2_log: 0.9055 | val RMSE: 2,155.64 | val MAE: 1,179.22 | val MAPE: 19.68%\n",
      "Epoch 153/1000 | train MSE: 0.11975 | val R2_log: 0.8980 | val RMSE: 2,319.49 | val MAE: 1,323.76 | val MAPE: 22.15%\n",
      "Epoch 154/1000 | train MSE: 0.11991 | val R2_log: 0.9082 | val RMSE: 2,102.33 | val MAE: 1,151.59 | val MAPE: 19.87%\n",
      "Epoch 155/1000 | train MSE: 0.12174 | val R2_log: 0.8805 | val RMSE: 2,382.73 | val MAE: 1,431.30 | val MAPE: 26.22%\n",
      "Epoch 156/1000 | train MSE: 0.12189 | val R2_log: 0.9043 | val RMSE: 2,172.45 | val MAE: 1,223.38 | val MAPE: 20.96%\n",
      "Epoch 157/1000 | train MSE: 0.12317 | val R2_log: 0.8750 | val RMSE: 3,440.35 | val MAE: 1,869.45 | val MAPE: 26.78%\n",
      "Epoch 158/1000 | train MSE: 0.12074 | val R2_log: 0.9016 | val RMSE: 2,171.27 | val MAE: 1,232.77 | val MAPE: 22.09%\n",
      "Epoch 159/1000 | train MSE: 0.11967 | val R2_log: 0.9057 | val RMSE: 2,146.35 | val MAE: 1,165.18 | val MAPE: 20.79%\n",
      "Epoch 160/1000 | train MSE: 0.11947 | val R2_log: 0.8950 | val RMSE: 2,393.05 | val MAE: 1,357.32 | val MAPE: 23.09%\n",
      "Epoch 161/1000 | train MSE: 0.11837 | val R2_log: 0.9054 | val RMSE: 2,107.51 | val MAE: 1,165.44 | val MAPE: 21.06%\n",
      "Epoch 162/1000 | train MSE: 0.12011 | val R2_log: 0.8812 | val RMSE: 2,426.68 | val MAE: 1,425.16 | val MAPE: 26.01%\n",
      "Epoch 163/1000 | train MSE: 0.11823 | val R2_log: 0.8750 | val RMSE: 2,936.17 | val MAE: 1,627.74 | val MAPE: 27.03%\n",
      "Epoch 164/1000 | train MSE: 0.11992 | val R2_log: 0.8860 | val RMSE: 2,130.58 | val MAE: 1,218.05 | val MAPE: 24.87%\n",
      "Epoch 165/1000 | train MSE: 0.11759 | val R2_log: 0.8961 | val RMSE: 2,500.86 | val MAE: 1,375.64 | val MAPE: 22.96%\n",
      "Epoch 166/1000 | train MSE: 0.11569 | val R2_log: 0.8977 | val RMSE: 2,365.08 | val MAE: 1,333.40 | val MAPE: 22.85%\n",
      "Epoch 167/1000 | train MSE: 0.11783 | val R2_log: 0.9079 | val RMSE: 2,097.14 | val MAE: 1,141.74 | val MAPE: 19.94%\n",
      "Epoch 168/1000 | train MSE: 0.12080 | val R2_log: 0.8674 | val RMSE: 2,694.26 | val MAE: 1,575.58 | val MAPE: 28.60%\n",
      "Epoch 169/1000 | train MSE: 0.11994 | val R2_log: 0.9070 | val RMSE: 2,137.44 | val MAE: 1,158.16 | val MAPE: 19.61%\n",
      "Epoch 170/1000 | train MSE: 0.12085 | val R2_log: 0.9072 | val RMSE: 2,115.21 | val MAE: 1,152.79 | val MAPE: 19.62%\n",
      "Epoch 171/1000 | train MSE: 0.11841 | val R2_log: 0.8955 | val RMSE: 2,253.28 | val MAE: 1,282.67 | val MAPE: 23.27%\n",
      "Epoch 172/1000 | train MSE: 0.11628 | val R2_log: 0.8980 | val RMSE: 2,654.74 | val MAE: 1,414.01 | val MAPE: 22.46%\n",
      "Epoch 173/1000 | train MSE: 0.11653 | val R2_log: 0.8947 | val RMSE: 2,224.82 | val MAE: 1,258.67 | val MAPE: 23.63%\n",
      "Epoch 174/1000 | train MSE: 0.11594 | val R2_log: 0.9010 | val RMSE: 2,096.03 | val MAE: 1,177.43 | val MAPE: 22.01%\n",
      "Epoch 175/1000 | train MSE: 0.11950 | val R2_log: 0.9087 | val RMSE: 2,161.88 | val MAE: 1,162.23 | val MAPE: 19.79%\n",
      "Epoch 176/1000 | train MSE: 0.11524 | val R2_log: 0.8967 | val RMSE: 2,473.27 | val MAE: 1,384.43 | val MAPE: 22.55%\n",
      "Epoch 177/1000 | train MSE: 0.11637 | val R2_log: 0.9048 | val RMSE: 2,237.83 | val MAE: 1,239.67 | val MAPE: 20.09%\n",
      "Epoch 178/1000 | train MSE: 0.11623 | val R2_log: 0.8977 | val RMSE: 2,434.46 | val MAE: 1,372.87 | val MAPE: 22.73%\n",
      "Epoch 179/1000 | train MSE: 0.11846 | val R2_log: 0.9013 | val RMSE: 2,268.74 | val MAE: 1,266.24 | val MAPE: 21.33%\n",
      "Epoch 180/1000 | train MSE: 0.12074 | val R2_log: 0.9064 | val RMSE: 2,169.84 | val MAE: 1,192.51 | val MAPE: 19.76%\n",
      "Epoch 181/1000 | train MSE: 0.11611 | val R2_log: 0.9040 | val RMSE: 2,131.63 | val MAE: 1,173.41 | val MAPE: 21.50%\n",
      "Epoch 182/1000 | train MSE: 0.11445 | val R2_log: 0.9058 | val RMSE: 2,150.11 | val MAE: 1,167.39 | val MAPE: 19.44%\n",
      "Epoch 183/1000 | train MSE: 0.11413 | val R2_log: 0.9038 | val RMSE: 2,391.58 | val MAE: 1,310.96 | val MAPE: 21.38%\n",
      "Epoch 184/1000 | train MSE: 0.11416 | val R2_log: 0.9090 | val RMSE: 2,082.55 | val MAE: 1,149.04 | val MAPE: 19.56%\n",
      "Epoch 185/1000 | train MSE: 0.11504 | val R2_log: 0.9054 | val RMSE: 2,090.87 | val MAE: 1,171.87 | val MAPE: 21.13%\n",
      "Epoch 186/1000 | train MSE: 0.11580 | val R2_log: 0.8646 | val RMSE: 2,622.91 | val MAE: 1,590.16 | val MAPE: 29.15%\n",
      "Epoch 187/1000 | train MSE: 0.12006 | val R2_log: 0.9030 | val RMSE: 2,199.04 | val MAE: 1,236.37 | val MAPE: 21.90%\n",
      "Epoch 188/1000 | train MSE: 0.11495 | val R2_log: 0.9057 | val RMSE: 2,257.96 | val MAE: 1,189.20 | val MAPE: 19.80%\n",
      "Epoch 189/1000 | train MSE: 0.11875 | val R2_log: 0.8985 | val RMSE: 2,281.61 | val MAE: 1,277.15 | val MAPE: 22.61%\n",
      "Epoch 190/1000 | train MSE: 0.11378 | val R2_log: 0.8769 | val RMSE: 2,257.55 | val MAE: 1,366.51 | val MAPE: 26.94%\n",
      "Epoch 191/1000 | train MSE: 0.11716 | val R2_log: 0.8975 | val RMSE: 2,154.96 | val MAE: 1,217.65 | val MAPE: 22.83%\n",
      "Epoch 192/1000 | train MSE: 0.11507 | val R2_log: 0.9064 | val RMSE: 2,156.91 | val MAE: 1,184.16 | val MAPE: 20.44%\n",
      "Epoch 193/1000 | train MSE: 0.12023 | val R2_log: 0.9068 | val RMSE: 2,120.23 | val MAE: 1,174.40 | val MAPE: 20.89%\n",
      "Epoch 194/1000 | train MSE: 0.11506 | val R2_log: 0.8817 | val RMSE: 2,205.34 | val MAE: 1,310.01 | val MAPE: 26.04%\n",
      "Epoch 195/1000 | train MSE: 0.11269 | val R2_log: 0.9061 | val RMSE: 2,254.49 | val MAE: 1,215.08 | val MAPE: 19.69%\n",
      "Epoch 196/1000 | train MSE: 0.11641 | val R2_log: 0.9050 | val RMSE: 2,101.42 | val MAE: 1,150.69 | val MAPE: 21.09%\n",
      "Epoch 197/1000 | train MSE: 0.11395 | val R2_log: 0.8821 | val RMSE: 2,666.13 | val MAE: 1,502.34 | val MAPE: 26.06%\n",
      "Epoch 198/1000 | train MSE: 0.11504 | val R2_log: 0.8899 | val RMSE: 2,765.69 | val MAE: 1,530.93 | val MAPE: 23.90%\n",
      "Epoch 199/1000 | train MSE: 0.11799 | val R2_log: 0.8991 | val RMSE: 2,276.46 | val MAE: 1,276.74 | val MAPE: 22.54%\n",
      "Epoch 200/1000 | train MSE: 0.11206 | val R2_log: 0.9046 | val RMSE: 2,097.29 | val MAE: 1,171.18 | val MAPE: 21.02%\n",
      "Epoch 201/1000 | train MSE: 0.11559 | val R2_log: 0.8956 | val RMSE: 2,207.81 | val MAE: 1,242.96 | val MAPE: 23.14%\n",
      "Epoch 202/1000 | train MSE: 0.11433 | val R2_log: 0.8831 | val RMSE: 2,876.29 | val MAE: 1,635.61 | val MAPE: 25.49%\n",
      "Epoch 203/1000 | train MSE: 0.11678 | val R2_log: 0.9018 | val RMSE: 2,169.76 | val MAE: 1,244.51 | val MAPE: 22.00%\n",
      "Epoch 204/1000 | train MSE: 0.11391 | val R2_log: 0.8890 | val RMSE: 2,159.85 | val MAE: 1,270.16 | val MAPE: 24.48%\n",
      "Epoch 205/1000 | train MSE: 0.11581 | val R2_log: 0.9002 | val RMSE: 2,157.18 | val MAE: 1,229.82 | val MAPE: 22.38%\n",
      "Epoch 206/1000 | train MSE: 0.11286 | val R2_log: 0.9045 | val RMSE: 2,079.43 | val MAE: 1,145.28 | val MAPE: 21.34%\n",
      "Epoch 207/1000 | train MSE: 0.11316 | val R2_log: 0.8966 | val RMSE: 2,716.97 | val MAE: 1,462.64 | val MAPE: 22.15%\n",
      "Epoch 208/1000 | train MSE: 0.11151 | val R2_log: 0.9000 | val RMSE: 2,116.62 | val MAE: 1,179.29 | val MAPE: 22.06%\n",
      "Epoch 209/1000 | train MSE: 0.11439 | val R2_log: 0.9075 | val RMSE: 2,088.61 | val MAE: 1,144.65 | val MAPE: 20.24%\n",
      "Epoch 210/1000 | train MSE: 0.11256 | val R2_log: 0.9052 | val RMSE: 2,266.21 | val MAE: 1,246.52 | val MAPE: 21.22%\n",
      "Epoch 00210: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch 211/1000 | train MSE: 0.11210 | val R2_log: 0.9038 | val RMSE: 2,215.96 | val MAE: 1,241.11 | val MAPE: 21.66%\n",
      "Epoch 212/1000 | train MSE: 0.10887 | val R2_log: 0.9040 | val RMSE: 2,189.24 | val MAE: 1,221.25 | val MAPE: 21.33%\n",
      "Epoch 213/1000 | train MSE: 0.10830 | val R2_log: 0.9070 | val RMSE: 2,102.61 | val MAE: 1,156.17 | val MAPE: 20.47%\n",
      "Epoch 214/1000 | train MSE: 0.10889 | val R2_log: 0.9064 | val RMSE: 2,197.88 | val MAE: 1,175.37 | val MAPE: 20.51%\n",
      "Epoch 215/1000 | train MSE: 0.10885 | val R2_log: 0.8976 | val RMSE: 2,339.23 | val MAE: 1,311.78 | val MAPE: 22.73%\n",
      "Epoch 216/1000 | train MSE: 0.10920 | val R2_log: 0.8988 | val RMSE: 2,240.42 | val MAE: 1,262.95 | val MAPE: 22.67%\n",
      "Epoch 217/1000 | train MSE: 0.10793 | val R2_log: 0.9078 | val RMSE: 2,090.36 | val MAE: 1,133.47 | val MAPE: 19.30%\n",
      "Epoch 218/1000 | train MSE: 0.10834 | val R2_log: 0.9083 | val RMSE: 2,067.31 | val MAE: 1,139.20 | val MAPE: 20.27%\n",
      "Epoch 219/1000 | train MSE: 0.10734 | val R2_log: 0.9084 | val RMSE: 2,083.99 | val MAE: 1,136.38 | val MAPE: 19.66%\n",
      "Epoch 220/1000 | train MSE: 0.10868 | val R2_log: 0.9088 | val RMSE: 2,080.81 | val MAE: 1,143.86 | val MAPE: 19.80%\n",
      "Epoch 221/1000 | train MSE: 0.10994 | val R2_log: 0.9056 | val RMSE: 2,141.22 | val MAE: 1,181.72 | val MAPE: 20.81%\n",
      "Epoch 222/1000 | train MSE: 0.11001 | val R2_log: 0.9080 | val RMSE: 2,102.10 | val MAE: 1,154.03 | val MAPE: 19.89%\n",
      "Epoch 223/1000 | train MSE: 0.10973 | val R2_log: 0.9013 | val RMSE: 2,414.29 | val MAE: 1,318.30 | val MAPE: 21.10%\n",
      "Epoch 224/1000 | train MSE: 0.10659 | val R2_log: 0.8982 | val RMSE: 2,181.73 | val MAE: 1,232.06 | val MAPE: 22.74%\n",
      "Epoch 225/1000 | train MSE: 0.10831 | val R2_log: 0.9060 | val RMSE: 2,120.50 | val MAE: 1,173.88 | val MAPE: 20.58%\n",
      "Epoch 226/1000 | train MSE: 0.10934 | val R2_log: 0.8969 | val RMSE: 2,407.53 | val MAE: 1,338.79 | val MAPE: 22.91%\n",
      "Epoch 227/1000 | train MSE: 0.10771 | val R2_log: 0.9059 | val RMSE: 2,183.54 | val MAE: 1,198.43 | val MAPE: 20.04%\n",
      "Epoch 228/1000 | train MSE: 0.10875 | val R2_log: 0.9073 | val RMSE: 2,121.55 | val MAE: 1,176.22 | val MAPE: 20.58%\n",
      "Epoch 229/1000 | train MSE: 0.11011 | val R2_log: 0.8976 | val RMSE: 2,480.26 | val MAE: 1,348.72 | val MAPE: 22.95%\n",
      "Epoch 230/1000 | train MSE: 0.10476 | val R2_log: 0.9077 | val RMSE: 2,125.36 | val MAE: 1,155.90 | val MAPE: 19.59%\n",
      "Epoch 231/1000 | train MSE: 0.10835 | val R2_log: 0.9014 | val RMSE: 2,341.75 | val MAE: 1,284.96 | val MAPE: 21.99%\n",
      "Epoch 232/1000 | train MSE: 0.10509 | val R2_log: 0.9047 | val RMSE: 2,143.76 | val MAE: 1,191.86 | val MAPE: 21.15%\n",
      "Epoch 233/1000 | train MSE: 0.10734 | val R2_log: 0.9089 | val RMSE: 2,076.84 | val MAE: 1,138.18 | val MAPE: 20.00%\n",
      "Epoch 234/1000 | train MSE: 0.10694 | val R2_log: 0.9015 | val RMSE: 2,172.15 | val MAE: 1,227.59 | val MAPE: 22.00%\n",
      "Epoch 235/1000 | train MSE: 0.10720 | val R2_log: 0.8967 | val RMSE: 2,293.79 | val MAE: 1,291.93 | val MAPE: 23.00%\n",
      "Epoch 236/1000 | train MSE: 0.10666 | val R2_log: 0.9053 | val RMSE: 2,145.97 | val MAE: 1,190.01 | val MAPE: 21.18%\n",
      "Epoch 00236: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 237/1000 | train MSE: 0.10719 | val R2_log: 0.9071 | val RMSE: 2,161.47 | val MAE: 1,189.53 | val MAPE: 20.34%\n",
      "Epoch 238/1000 | train MSE: 0.10506 | val R2_log: 0.9074 | val RMSE: 2,113.30 | val MAE: 1,168.71 | val MAPE: 20.33%\n",
      "Epoch 239/1000 | train MSE: 0.10520 | val R2_log: 0.9018 | val RMSE: 2,251.79 | val MAE: 1,266.09 | val MAPE: 21.94%\n",
      "Epoch 240/1000 | train MSE: 0.10290 | val R2_log: 0.9076 | val RMSE: 2,106.96 | val MAE: 1,151.58 | val MAPE: 20.53%\n",
      "Epoch 241/1000 | train MSE: 0.10506 | val R2_log: 0.9057 | val RMSE: 2,199.29 | val MAE: 1,218.33 | val MAPE: 20.74%\n",
      "Epoch 242/1000 | train MSE: 0.10469 | val R2_log: 0.9060 | val RMSE: 2,183.66 | val MAE: 1,201.46 | val MAPE: 20.79%\n",
      "Epoch 243/1000 | train MSE: 0.10403 | val R2_log: 0.9044 | val RMSE: 2,221.98 | val MAE: 1,224.39 | val MAPE: 20.95%\n",
      "Epoch 244/1000 | train MSE: 0.10496 | val R2_log: 0.9055 | val RMSE: 2,219.59 | val MAE: 1,223.74 | val MAPE: 20.76%\n",
      "Epoch 245/1000 | train MSE: 0.10558 | val R2_log: 0.9064 | val RMSE: 2,242.60 | val MAE: 1,217.21 | val MAPE: 20.49%\n",
      "Epoch 246/1000 | train MSE: 0.10363 | val R2_log: 0.9071 | val RMSE: 2,122.39 | val MAE: 1,166.79 | val MAPE: 20.61%\n",
      "Epoch 247/1000 | train MSE: 0.10495 | val R2_log: 0.9026 | val RMSE: 2,214.39 | val MAE: 1,233.13 | val MAPE: 20.41%\n",
      "Epoch 248/1000 | train MSE: 0.10469 | val R2_log: 0.9067 | val RMSE: 2,139.07 | val MAE: 1,174.51 | val MAPE: 20.41%\n",
      "Epoch 249/1000 | train MSE: 0.10398 | val R2_log: 0.9047 | val RMSE: 2,204.64 | val MAE: 1,219.87 | val MAPE: 20.24%\n",
      "Epoch 250/1000 | train MSE: 0.10324 | val R2_log: 0.8963 | val RMSE: 2,280.19 | val MAE: 1,293.40 | val MAPE: 23.16%\n",
      "Epoch 251/1000 | train MSE: 0.10306 | val R2_log: 0.9066 | val RMSE: 2,115.18 | val MAE: 1,164.89 | val MAPE: 20.59%\n",
      "Epoch 252/1000 | train MSE: 0.10403 | val R2_log: 0.9072 | val RMSE: 2,133.55 | val MAE: 1,165.36 | val MAPE: 20.08%\n",
      "Epoch 253/1000 | train MSE: 0.10260 | val R2_log: 0.9033 | val RMSE: 2,194.37 | val MAE: 1,220.05 | val MAPE: 21.23%\n",
      "Epoch 254/1000 | train MSE: 0.10263 | val R2_log: 0.9024 | val RMSE: 2,174.02 | val MAE: 1,220.53 | val MAPE: 21.42%\n",
      "Epoch 255/1000 | train MSE: 0.10370 | val R2_log: 0.8939 | val RMSE: 2,644.03 | val MAE: 1,437.94 | val MAPE: 23.57%\n",
      "Epoch 256/1000 | train MSE: 0.10367 | val R2_log: 0.9033 | val RMSE: 2,272.53 | val MAE: 1,250.03 | val MAPE: 20.53%\n",
      "Epoch 257/1000 | train MSE: 0.10323 | val R2_log: 0.9053 | val RMSE: 2,155.22 | val MAE: 1,185.46 | val MAPE: 20.29%\n",
      "Epoch 258/1000 | train MSE: 0.10281 | val R2_log: 0.9089 | val RMSE: 2,100.79 | val MAE: 1,136.82 | val MAPE: 19.66%\n",
      "Epoch 259/1000 | train MSE: 0.10183 | val R2_log: 0.9010 | val RMSE: 2,260.16 | val MAE: 1,265.81 | val MAPE: 21.84%\n",
      "Epoch 260/1000 | train MSE: 0.10173 | val R2_log: 0.9066 | val RMSE: 2,166.01 | val MAE: 1,175.22 | val MAPE: 19.85%\n",
      "Epoch 261/1000 | train MSE: 0.10402 | val R2_log: 0.9077 | val RMSE: 2,147.20 | val MAE: 1,173.58 | val MAPE: 20.07%\n",
      "Epoch 262/1000 | train MSE: 0.10407 | val R2_log: 0.8973 | val RMSE: 2,243.23 | val MAE: 1,277.10 | val MAPE: 23.05%\n",
      "Epoch 00262: reducing learning rate of group 0 to 2.5000e-04.\n",
      "Epoch 263/1000 | train MSE: 0.10404 | val R2_log: 0.9071 | val RMSE: 2,143.60 | val MAE: 1,181.31 | val MAPE: 20.31%\n",
      "Epoch 264/1000 | train MSE: 0.10147 | val R2_log: 0.9074 | val RMSE: 2,110.51 | val MAE: 1,153.97 | val MAPE: 19.99%\n",
      "Epoch 265/1000 | train MSE: 0.10096 | val R2_log: 0.9010 | val RMSE: 2,191.84 | val MAE: 1,232.10 | val MAPE: 22.04%\n",
      "Epoch 266/1000 | train MSE: 0.10232 | val R2_log: 0.9054 | val RMSE: 2,203.30 | val MAE: 1,214.81 | val MAPE: 20.61%\n",
      "Epoch 267/1000 | train MSE: 0.10040 | val R2_log: 0.9046 | val RMSE: 2,230.22 | val MAE: 1,222.31 | val MAPE: 21.16%\n",
      "Epoch 268/1000 | train MSE: 0.10209 | val R2_log: 0.9053 | val RMSE: 2,129.48 | val MAE: 1,170.35 | val MAPE: 20.77%\n",
      "Epoch 269/1000 | train MSE: 0.10092 | val R2_log: 0.9043 | val RMSE: 2,188.35 | val MAE: 1,208.27 | val MAPE: 21.02%\n",
      "Epoch 270/1000 | train MSE: 0.10261 | val R2_log: 0.8992 | val RMSE: 2,260.14 | val MAE: 1,256.45 | val MAPE: 22.49%\n",
      "Epoch 271/1000 | train MSE: 0.10271 | val R2_log: 0.9049 | val RMSE: 2,224.92 | val MAE: 1,219.18 | val MAPE: 20.70%\n",
      "Epoch 272/1000 | train MSE: 0.10477 | val R2_log: 0.9031 | val RMSE: 2,198.33 | val MAE: 1,222.83 | val MAPE: 20.90%\n",
      "Epoch 273/1000 | train MSE: 0.10131 | val R2_log: 0.9065 | val RMSE: 2,118.38 | val MAE: 1,164.57 | val MAPE: 20.56%\n",
      "Epoch 274/1000 | train MSE: 0.10032 | val R2_log: 0.9059 | val RMSE: 2,190.30 | val MAE: 1,202.38 | val MAPE: 20.56%\n",
      "Epoch 275/1000 | train MSE: 0.10262 | val R2_log: 0.9065 | val RMSE: 2,160.92 | val MAE: 1,173.03 | val MAPE: 19.88%\n",
      "Epoch 276/1000 | train MSE: 0.09965 | val R2_log: 0.9024 | val RMSE: 2,296.99 | val MAE: 1,274.03 | val MAPE: 21.26%\n",
      "Epoch 277/1000 | train MSE: 0.10002 | val R2_log: 0.9034 | val RMSE: 2,347.60 | val MAE: 1,274.97 | val MAPE: 20.87%\n",
      "Epoch 278/1000 | train MSE: 0.10142 | val R2_log: 0.9084 | val RMSE: 2,093.20 | val MAE: 1,135.75 | val MAPE: 19.95%\n",
      "Epoch 279/1000 | train MSE: 0.10180 | val R2_log: 0.9079 | val RMSE: 2,115.22 | val MAE: 1,148.12 | val MAPE: 19.67%\n",
      "Epoch 280/1000 | train MSE: 0.10254 | val R2_log: 0.9070 | val RMSE: 2,123.16 | val MAE: 1,156.48 | val MAPE: 20.06%\n",
      "Epoch 281/1000 | train MSE: 0.09967 | val R2_log: 0.9040 | val RMSE: 2,240.73 | val MAE: 1,236.49 | val MAPE: 21.10%\n",
      "Epoch 282/1000 | train MSE: 0.10052 | val R2_log: 0.9039 | val RMSE: 2,192.26 | val MAE: 1,209.77 | val MAPE: 21.09%\n",
      "Epoch 283/1000 | train MSE: 0.10056 | val R2_log: 0.9035 | val RMSE: 2,213.97 | val MAE: 1,215.57 | val MAPE: 20.35%\n",
      "Epoch 284/1000 | train MSE: 0.10131 | val R2_log: 0.9001 | val RMSE: 2,246.99 | val MAE: 1,257.11 | val MAPE: 21.72%\n",
      "Epoch 285/1000 | train MSE: 0.10020 | val R2_log: 0.9059 | val RMSE: 2,129.97 | val MAE: 1,151.64 | val MAPE: 19.38%\n",
      "Epoch 286/1000 | train MSE: 0.10171 | val R2_log: 0.9058 | val RMSE: 2,169.23 | val MAE: 1,187.23 | val MAPE: 20.25%\n",
      "Epoch 287/1000 | train MSE: 0.10003 | val R2_log: 0.9056 | val RMSE: 2,139.88 | val MAE: 1,175.19 | val MAPE: 20.03%\n",
      "Epoch 288/1000 | train MSE: 0.10193 | val R2_log: 0.9000 | val RMSE: 2,270.68 | val MAE: 1,271.05 | val MAPE: 22.16%\n",
      "Epoch 00288: reducing learning rate of group 0 to 1.2500e-04.\n",
      "Epoch 289/1000 | train MSE: 0.10099 | val R2_log: 0.9044 | val RMSE: 2,219.28 | val MAE: 1,219.34 | val MAPE: 20.67%\n",
      "Epoch 290/1000 | train MSE: 0.09999 | val R2_log: 0.9069 | val RMSE: 2,125.25 | val MAE: 1,159.45 | val MAPE: 19.88%\n",
      "Epoch 291/1000 | train MSE: 0.09928 | val R2_log: 0.9058 | val RMSE: 2,190.77 | val MAE: 1,197.12 | val MAPE: 20.24%\n",
      "Epoch 292/1000 | train MSE: 0.09966 | val R2_log: 0.9037 | val RMSE: 2,244.29 | val MAE: 1,225.65 | val MAPE: 20.69%\n",
      "Epoch 293/1000 | train MSE: 0.09923 | val R2_log: 0.9037 | val RMSE: 2,200.02 | val MAE: 1,216.29 | val MAPE: 21.05%\n",
      "Epoch 294/1000 | train MSE: 0.09973 | val R2_log: 0.9037 | val RMSE: 2,181.84 | val MAE: 1,209.41 | val MAPE: 21.16%\n",
      "Epoch 295/1000 | train MSE: 0.09778 | val R2_log: 0.9031 | val RMSE: 2,235.21 | val MAE: 1,240.28 | val MAPE: 21.25%\n",
      "Epoch 296/1000 | train MSE: 0.09956 | val R2_log: 0.9047 | val RMSE: 2,188.37 | val MAE: 1,202.95 | val MAPE: 20.49%\n",
      "Epoch 297/1000 | train MSE: 0.10085 | val R2_log: 0.9048 | val RMSE: 2,229.96 | val MAE: 1,217.23 | val MAPE: 20.60%\n",
      "Epoch 298/1000 | train MSE: 0.10023 | val R2_log: 0.9048 | val RMSE: 2,231.80 | val MAE: 1,214.36 | val MAPE: 20.60%\n",
      "Epoch 299/1000 | train MSE: 0.10038 | val R2_log: 0.9033 | val RMSE: 2,254.62 | val MAE: 1,234.04 | val MAPE: 20.75%\n",
      "Epoch 300/1000 | train MSE: 0.10007 | val R2_log: 0.9071 | val RMSE: 2,107.91 | val MAE: 1,142.27 | val MAPE: 19.90%\n",
      "Epoch 301/1000 | train MSE: 0.10056 | val R2_log: 0.9050 | val RMSE: 2,145.44 | val MAE: 1,181.53 | val MAPE: 20.83%\n",
      "Epoch 302/1000 | train MSE: 0.09997 | val R2_log: 0.9060 | val RMSE: 2,148.10 | val MAE: 1,175.54 | val MAPE: 20.55%\n",
      "Epoch 303/1000 | train MSE: 0.09994 | val R2_log: 0.9016 | val RMSE: 2,253.81 | val MAE: 1,245.75 | val MAPE: 21.67%\n",
      "Epoch 304/1000 | train MSE: 0.09842 | val R2_log: 0.9052 | val RMSE: 2,191.08 | val MAE: 1,194.99 | val MAPE: 20.13%\n",
      "Epoch 305/1000 | train MSE: 0.10008 | val R2_log: 0.9040 | val RMSE: 2,173.54 | val MAE: 1,199.90 | val MAPE: 20.85%\n",
      "Epoch 306/1000 | train MSE: 0.09798 | val R2_log: 0.9070 | val RMSE: 2,115.80 | val MAE: 1,153.70 | val MAPE: 19.95%\n",
      "Epoch 307/1000 | train MSE: 0.09997 | val R2_log: 0.9050 | val RMSE: 2,172.55 | val MAE: 1,195.15 | val MAPE: 20.81%\n",
      "Epoch 308/1000 | train MSE: 0.10153 | val R2_log: 0.9042 | val RMSE: 2,222.04 | val MAE: 1,219.21 | val MAPE: 20.67%\n",
      "Epoch 309/1000 | train MSE: 0.09940 | val R2_log: 0.9057 | val RMSE: 2,155.05 | val MAE: 1,180.46 | val MAPE: 20.60%\n",
      "Epoch 310/1000 | train MSE: 0.10163 | val R2_log: 0.9038 | val RMSE: 2,176.52 | val MAE: 1,207.26 | val MAPE: 20.71%\n",
      "Epoch 311/1000 | train MSE: 0.09882 | val R2_log: 0.9046 | val RMSE: 2,223.64 | val MAE: 1,216.90 | val MAPE: 20.52%\n",
      "Epoch 312/1000 | train MSE: 0.10080 | val R2_log: 0.9066 | val RMSE: 2,131.74 | val MAE: 1,167.95 | val MAPE: 20.04%\n",
      "Epoch 313/1000 | train MSE: 0.09951 | val R2_log: 0.9057 | val RMSE: 2,126.51 | val MAE: 1,170.02 | val MAPE: 20.39%\n",
      "Epoch 314/1000 | train MSE: 0.09905 | val R2_log: 0.9021 | val RMSE: 2,195.77 | val MAE: 1,220.31 | val MAPE: 21.11%\n",
      "Epoch 00314: reducing learning rate of group 0 to 6.2500e-05.\n",
      "Epoch 315/1000 | train MSE: 0.09896 | val R2_log: 0.9042 | val RMSE: 2,176.38 | val MAE: 1,196.58 | val MAPE: 20.73%\n",
      "Epoch 316/1000 | train MSE: 0.09778 | val R2_log: 0.9040 | val RMSE: 2,251.15 | val MAE: 1,228.14 | val MAPE: 20.67%\n",
      "Epoch 317/1000 | train MSE: 0.09927 | val R2_log: 0.9040 | val RMSE: 2,223.41 | val MAE: 1,223.51 | val MAPE: 20.75%\n",
      "Epoch 318/1000 | train MSE: 0.09972 | val R2_log: 0.9062 | val RMSE: 2,120.89 | val MAE: 1,157.56 | val MAPE: 19.93%\n",
      "Epoch 319/1000 | train MSE: 0.09856 | val R2_log: 0.9048 | val RMSE: 2,181.79 | val MAE: 1,200.13 | val MAPE: 20.58%\n",
      "Epoch 320/1000 | train MSE: 0.09999 | val R2_log: 0.9039 | val RMSE: 2,179.53 | val MAE: 1,204.52 | val MAPE: 20.90%\n",
      "Epoch 321/1000 | train MSE: 0.09854 | val R2_log: 0.9049 | val RMSE: 2,163.42 | val MAE: 1,189.75 | val MAPE: 20.68%\n",
      "Epoch 322/1000 | train MSE: 0.09846 | val R2_log: 0.9001 | val RMSE: 2,254.95 | val MAE: 1,260.20 | val MAPE: 21.91%\n",
      "Epoch 323/1000 | train MSE: 0.09866 | val R2_log: 0.8997 | val RMSE: 2,260.54 | val MAE: 1,262.46 | val MAPE: 22.14%\n",
      "Epoch 324/1000 | train MSE: 0.09834 | val R2_log: 0.9056 | val RMSE: 2,150.27 | val MAE: 1,178.47 | val MAPE: 20.52%\n",
      "Epoch 325/1000 | train MSE: 0.09898 | val R2_log: 0.9055 | val RMSE: 2,118.70 | val MAE: 1,156.54 | val MAPE: 20.06%\n",
      "Early stopping at epoch 325 (best @ 175 with R2_log=0.9087).\n",
      "\n",
      "Entrenamiento terminado en 377.34 s (device=cuda).\n",
      "Restored best model from epoch 175 (R2_log=0.9087).\n",
      "\n",
      "=== FT-Transformer No coords No VCR - Test ===\n",
      "R^2 (log): 0.9086\n",
      "RMSE ($): 2,110.31\n",
      "MAE  ($): 1,129.95\n",
      "MAPE (%): 19.75\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Entrenamiento\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = FTTransformer(\n",
    "    n_features=n_features,\n",
    "    d_token=D_TOKEN,\n",
    "    n_layers=N_LAYERS,\n",
    "    n_head=N_HEAD,\n",
    "    ff_mult=FF_MULT,\n",
    "    dropout=DROPOUT,\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode=\"max\",               # maximizamos R^2 en validación\n",
    "    factor=LR_FACTOR,\n",
    "    patience=LR_PATIENCE,\n",
    "    min_lr=MIN_LR,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "best_r2 = -float(\"inf\")\n",
    "best_state = None\n",
    "best_epoch = 0\n",
    "no_improve = 0\n",
    "\n",
    "def evaluate(dl: DataLoader) -> tuple[float, float, float, float]:\n",
    "    model.eval()\n",
    "    y_true_log, y_pred_log = [], []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in dl:\n",
    "            xb = xb.to(device, non_blocking=True)\n",
    "            yb = yb.to(device, non_blocking=True)\n",
    "            pred = model(xb)\n",
    "            y_true_log.append(yb.detach().cpu().numpy())\n",
    "            y_pred_log.append(pred.detach().cpu().numpy())\n",
    "    y_true_log = np.concatenate(y_true_log)\n",
    "    y_pred_log = np.concatenate(y_pred_log)\n",
    "    # Métricas\n",
    "    r2_log = r2_score(y_true_log, y_pred_log)\n",
    "    y_true_price = np.exp(y_true_log)\n",
    "    y_pred_price = np.exp(y_pred_log)\n",
    "    rmse = root_mean_squared_error(y_true_price, y_pred_price)\n",
    "    mae = mean_absolute_error(y_true_price, y_pred_price)\n",
    "    mape = np.mean(np.abs((y_true_price - y_pred_price) / np.clip(y_true_price, 1e-9, None))) * 100\n",
    "    return r2_log, rmse, mae, mape\n",
    "\n",
    "start = time.perf_counter()\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for xb, yb in train_dl:\n",
    "        xb = xb.to(device, non_blocking=True)\n",
    "        yb = yb.to(device, non_blocking=True)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        pred = model(xb)\n",
    "        loss = criterion(pred, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * xb.size(0)\n",
    "    train_loss = total_loss / len(train_dl.dataset)\n",
    "\n",
    "    r2_log_val, rmse_val, mae_val, mape_val = evaluate(val_dl)\n",
    "    print(\n",
    "        f\"Epoch {epoch:02d}/{EPOCHS} | train MSE: {train_loss:.5f} | \"\n",
    "        f\"val R2_log: {r2_log_val:.4f} | val RMSE: {rmse_val:,.2f} | val MAE: {mae_val:,.2f} | val MAPE: {mape_val:.2f}%\"\n",
    "    )\n",
    "    # Early stopping + ReduceLROnPlateau (monitor: R2_log de validación)\n",
    "    improved = (r2_log_val - best_r2) > MIN_DELTA_R2\n",
    "    if improved:\n",
    "        best_r2 = r2_log_val\n",
    "        best_state = deepcopy(model.state_dict())   # restore best weights\n",
    "        best_epoch = epoch\n",
    "        no_improve = 0\n",
    "    else:\n",
    "        no_improve += 1\n",
    "\n",
    "    scheduler.step(r2_log_val)\n",
    "\n",
    "    if no_improve >= EARLY_STOP_PATIENCE:\n",
    "        print(f\"Early stopping at epoch {epoch} (best @ {best_epoch} with R2_log={best_r2:.4f}).\")\n",
    "        break\n",
    "\n",
    "elapsed = time.perf_counter() - start\n",
    "print(f\"\\nEntrenamiento terminado en {elapsed:.2f} s (device={device}).\")\n",
    "\n",
    "# Restaurar los mejores pesos\n",
    "if best_state is not None:\n",
    "    model.load_state_dict(best_state)\n",
    "    print(f\"Restored best model from epoch {best_epoch} (R2_log={best_r2:.4f}).\")\n",
    "\n",
    "# Evaluación final en test\n",
    "r2_log_test, rmse_test, mae_test, mape_test = evaluate(test_dl)\n",
    "print(\"\\n=== FT-Transformer No coords No VCR - Test ===\")\n",
    "print(f\"R^2 (log): {r2_log_test:.4f}\")\n",
    "print(f\"RMSE ($): {rmse_test:,.2f}\")\n",
    "print(f\"MAE  ($): {mae_test:,.2f}\")\n",
    "print(f\"MAPE (%): {mape_test:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6311f5ae",
   "metadata": {},
   "source": [
    "### Segundo modelo FT-Transformer --> Con coordenadas, sin VCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "5071ec6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 25211 entries, 0 to 25214\n",
      "Data columns (total 25 columns):\n",
      " #   Column                Non-Null Count  Dtype  \n",
      "---  ------                --------------  -----  \n",
      " 0   monto                 25211 non-null  int64  \n",
      " 1   superficie_t          25211 non-null  float64\n",
      " 2   dormitorios           25211 non-null  int64  \n",
      " 3   dormitorios_faltante  25211 non-null  int64  \n",
      " 4   banos                 25211 non-null  int64  \n",
      " 5   banos_faltante        25211 non-null  int64  \n",
      " 6   antiguedad            25211 non-null  int64  \n",
      " 7   antiguedad_faltante   25211 non-null  int64  \n",
      " 8   Or_N                  25211 non-null  int64  \n",
      " 9   Or_S                  25211 non-null  int64  \n",
      " 10  Or_E                  25211 non-null  int64  \n",
      " 11  Or_O                  25211 non-null  int64  \n",
      " 12  Or_Faltante           25211 non-null  int64  \n",
      " 13  terraza               25211 non-null  float64\n",
      " 14  estacionamiento       25211 non-null  int64  \n",
      " 15  bodegas               25211 non-null  int64  \n",
      " 16  flag_Departamento     25211 non-null  int64  \n",
      " 17  flag_Multinivel       25211 non-null  int64  \n",
      " 18  flag_Semipiso         25211 non-null  int64  \n",
      " 19  flag_Premium          25211 non-null  int64  \n",
      " 20  flag_Monoambiente     25211 non-null  int64  \n",
      " 21  flag_Loft             25211 non-null  int64  \n",
      " 22  latitud               25211 non-null  float64\n",
      " 23  longitud              25211 non-null  float64\n",
      " 24  log_monto             25211 non-null  float64\n",
      "dtypes: float64(5), int64(20)\n",
      "memory usage: 5.0 MB\n"
     ]
    }
   ],
   "source": [
    "# Configuración específica del modelo\n",
    "df_coord =df_vcr_c.copy()\n",
    "obj_cols = df_coord.select_dtypes(include=[\"object\"]).columns\n",
    "cols_to_drop = list(obj_cols)\n",
    "cols_to_drop.append(\"id\")\n",
    "df_coord = df_coord.drop(columns=cols_to_drop)\n",
    "df_coord.info(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_features: 23\n"
     ]
    }
   ],
   "source": [
    "# Split\n",
    "X_df = df_coord.drop(columns=[\"monto\", \"log_monto\"]).copy()\n",
    "y = df_coord[\"log_monto\"].values.astype(np.float32)\n",
    "\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "    X_df.values, y, test_size=TEST_SIZE, random_state=SEED\n",
    ")\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_full, y_train_full, test_size=VAL_SIZE, random_state=SEED\n",
    ")\n",
    "\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train = scaler.transform(X_train).astype(np.float32)\n",
    "X_val = scaler.transform(X_val).astype(np.float32)\n",
    "X_test = scaler.transform(X_test).astype(np.float32)\n",
    "\n",
    "n_features = X_train.shape[1]\n",
    "print(f\"n_features: {n_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "4eb4e186",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset & Loader\n",
    "class NpDataset(Dataset):\n",
    "    def __init__(self, X: np.ndarray, y: np.ndarray):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            torch.from_numpy(self.X[idx]),\n",
    "            torch.tensor(self.y[idx], dtype=torch.float32),\n",
    "        )\n",
    "\n",
    "train_dl = DataLoader(NpDataset(X_train, y_train), batch_size=BATCH_SIZE, shuffle=True, drop_last=False, pin_memory=True)\n",
    "val_dl = DataLoader(NpDataset(X_val, y_val), batch_size=BATCH_SIZE, shuffle=False, drop_last=False, pin_memory=True)\n",
    "test_dl = DataLoader(NpDataset(X_test, y_test), batch_size=BATCH_SIZE, shuffle=False, drop_last=False, pin_memory=True)\n",
    "\n",
    "# Modelo\n",
    "class NumericTokenizer(nn.Module):\n",
    "    \"\"\"Mapea features numéricas a tokens: token_i = x_i * W_i + b_i\"\"\"\n",
    "    def __init__(self, n_features: int, d_token: int):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.empty(n_features, d_token))\n",
    "        self.bias = nn.Parameter(torch.zeros(n_features, d_token))\n",
    "        nn.init.xavier_uniform_(self.weight)\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x: (B, F) -> (B, F, D)\n",
    "        return x.unsqueeze(-1) * self.weight + self.bias\n",
    "\n",
    "class FTTransformer(nn.Module):\n",
    "    def __init__(self, n_features: int, d_token: int, n_layers: int, n_head: int, ff_mult: int, dropout: float):\n",
    "        super().__init__()\n",
    "        self.tokenizer = NumericTokenizer(n_features, d_token)\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, d_token))\n",
    "        enc_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_token,\n",
    "            nhead=n_head,\n",
    "            dim_feedforward=d_token * ff_mult,\n",
    "            dropout=dropout,\n",
    "            batch_first=True,\n",
    "            activation=\"gelu\",\n",
    "            norm_first=True,\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(enc_layer, num_layers=n_layers)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.LayerNorm(d_token),\n",
    "            nn.Linear(d_token, d_token),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_token, 1),\n",
    "        )\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        B = x.size(0)\n",
    "        tokens = self.tokenizer(x)                  # (B, F, D)\n",
    "        cls = self.cls_token.expand(B, -1, -1)     # (B, 1, D)\n",
    "        x_tok = torch.cat([cls, tokens], dim=1)    # (B, 1+F, D)\n",
    "        x_enc = self.encoder(x_tok)                # (B, 1+F, D)\n",
    "        cls_out = x_enc[:, 0, :]                   # (B, D)\n",
    "        y = self.head(cls_out).squeeze(-1)         # (B,)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "2d299048",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01/1000 | train MSE: 8.07576 | val R2_log: 0.1173 | val RMSE: 6,961.54 | val MAE: 3,953.49 | val MAPE: 57.47%\n",
      "Epoch 02/1000 | train MSE: 0.50069 | val R2_log: 0.7127 | val RMSE: 3,866.47 | val MAE: 2,406.59 | val MAPE: 47.96%\n",
      "Epoch 03/1000 | train MSE: 0.30102 | val R2_log: 0.8099 | val RMSE: 3,220.38 | val MAE: 1,904.70 | val MAPE: 36.27%\n",
      "Epoch 04/1000 | train MSE: 0.26536 | val R2_log: 0.8601 | val RMSE: 3,057.80 | val MAE: 1,808.42 | val MAPE: 30.00%\n",
      "Epoch 05/1000 | train MSE: 0.23735 | val R2_log: 0.8675 | val RMSE: 2,432.61 | val MAE: 1,487.85 | val MAPE: 29.13%\n",
      "Epoch 06/1000 | train MSE: 0.22839 | val R2_log: 0.8624 | val RMSE: 2,569.84 | val MAE: 1,606.06 | val MAPE: 30.42%\n",
      "Epoch 07/1000 | train MSE: 0.21992 | val R2_log: 0.9058 | val RMSE: 2,303.92 | val MAE: 1,323.78 | val MAPE: 22.94%\n",
      "Epoch 08/1000 | train MSE: 0.20885 | val R2_log: 0.8948 | val RMSE: 2,446.28 | val MAE: 1,416.91 | val MAPE: 25.06%\n",
      "Epoch 09/1000 | train MSE: 0.19538 | val R2_log: 0.8858 | val RMSE: 2,498.34 | val MAE: 1,471.46 | val MAPE: 26.65%\n",
      "Epoch 10/1000 | train MSE: 0.19687 | val R2_log: 0.8841 | val RMSE: 2,577.20 | val MAE: 1,423.83 | val MAPE: 26.49%\n",
      "Epoch 11/1000 | train MSE: 0.19452 | val R2_log: 0.9269 | val RMSE: 2,057.16 | val MAE: 1,094.09 | val MAPE: 18.80%\n",
      "Epoch 12/1000 | train MSE: 0.18999 | val R2_log: 0.8989 | val RMSE: 2,140.55 | val MAE: 1,264.32 | val MAPE: 24.40%\n",
      "Epoch 13/1000 | train MSE: 0.18050 | val R2_log: 0.9211 | val RMSE: 2,199.50 | val MAE: 1,194.30 | val MAPE: 20.19%\n",
      "Epoch 14/1000 | train MSE: 0.18412 | val R2_log: 0.9090 | val RMSE: 2,572.74 | val MAE: 1,415.43 | val MAPE: 22.61%\n",
      "Epoch 15/1000 | train MSE: 0.18070 | val R2_log: 0.9344 | val RMSE: 2,081.09 | val MAE: 1,055.18 | val MAPE: 16.79%\n",
      "Epoch 16/1000 | train MSE: 0.17490 | val R2_log: 0.8838 | val RMSE: 2,658.23 | val MAE: 1,563.48 | val MAPE: 27.32%\n",
      "Epoch 17/1000 | train MSE: 0.17461 | val R2_log: 0.9166 | val RMSE: 2,121.05 | val MAE: 1,200.51 | val MAPE: 21.27%\n",
      "Epoch 18/1000 | train MSE: 0.17350 | val R2_log: 0.9068 | val RMSE: 2,173.53 | val MAE: 1,262.92 | val MAPE: 23.30%\n",
      "Epoch 19/1000 | train MSE: 0.17757 | val R2_log: 0.8289 | val RMSE: 3,050.68 | val MAE: 1,950.17 | val MAPE: 36.38%\n",
      "Epoch 20/1000 | train MSE: 0.17707 | val R2_log: 0.9148 | val RMSE: 2,365.70 | val MAE: 1,284.46 | val MAPE: 21.74%\n",
      "Epoch 21/1000 | train MSE: 0.16747 | val R2_log: 0.9206 | val RMSE: 2,162.00 | val MAE: 1,208.35 | val MAPE: 20.68%\n",
      "Epoch 22/1000 | train MSE: 0.16313 | val R2_log: 0.9364 | val RMSE: 2,111.70 | val MAE: 1,063.28 | val MAPE: 16.71%\n",
      "Epoch 23/1000 | train MSE: 0.16942 | val R2_log: 0.9011 | val RMSE: 2,456.72 | val MAE: 1,387.93 | val MAPE: 24.41%\n",
      "Epoch 24/1000 | train MSE: 0.16132 | val R2_log: 0.9014 | val RMSE: 2,553.31 | val MAE: 1,451.30 | val MAPE: 24.52%\n",
      "Epoch 25/1000 | train MSE: 0.16766 | val R2_log: 0.9360 | val RMSE: 2,010.16 | val MAE: 1,075.43 | val MAPE: 17.22%\n",
      "Epoch 26/1000 | train MSE: 0.16231 | val R2_log: 0.9276 | val RMSE: 2,433.11 | val MAE: 1,283.43 | val MAPE: 19.12%\n",
      "Epoch 27/1000 | train MSE: 0.15732 | val R2_log: 0.9129 | val RMSE: 2,377.64 | val MAE: 1,316.70 | val MAPE: 22.28%\n",
      "Epoch 28/1000 | train MSE: 0.15547 | val R2_log: 0.9024 | val RMSE: 2,489.51 | val MAE: 1,451.39 | val MAPE: 24.53%\n",
      "Epoch 29/1000 | train MSE: 0.15534 | val R2_log: 0.9268 | val RMSE: 2,349.40 | val MAE: 1,236.46 | val MAPE: 19.58%\n",
      "Epoch 30/1000 | train MSE: 0.15479 | val R2_log: 0.9292 | val RMSE: 2,075.31 | val MAE: 1,124.62 | val MAPE: 19.11%\n",
      "Epoch 31/1000 | train MSE: 0.14926 | val R2_log: 0.9311 | val RMSE: 2,186.06 | val MAE: 1,169.20 | val MAPE: 18.60%\n",
      "Epoch 32/1000 | train MSE: 0.14929 | val R2_log: 0.8850 | val RMSE: 2,691.21 | val MAE: 1,653.53 | val MAPE: 27.86%\n",
      "Epoch 33/1000 | train MSE: 0.15012 | val R2_log: 0.9183 | val RMSE: 2,447.57 | val MAE: 1,372.63 | val MAPE: 21.49%\n",
      "Epoch 34/1000 | train MSE: 0.14885 | val R2_log: 0.9303 | val RMSE: 2,187.60 | val MAE: 1,127.28 | val MAPE: 18.76%\n",
      "Epoch 35/1000 | train MSE: 0.14856 | val R2_log: 0.8554 | val RMSE: 3,211.11 | val MAE: 1,924.75 | val MAPE: 32.64%\n",
      "Epoch 36/1000 | train MSE: 0.14677 | val R2_log: 0.9088 | val RMSE: 2,498.41 | val MAE: 1,404.17 | val MAPE: 23.38%\n",
      "Epoch 37/1000 | train MSE: 0.14684 | val R2_log: 0.9328 | val RMSE: 2,200.28 | val MAE: 1,145.06 | val MAPE: 18.30%\n",
      "Epoch 38/1000 | train MSE: 0.14401 | val R2_log: 0.9398 | val RMSE: 2,200.13 | val MAE: 1,119.64 | val MAPE: 16.61%\n",
      "Epoch 39/1000 | train MSE: 0.14465 | val R2_log: 0.9158 | val RMSE: 2,504.54 | val MAE: 1,371.00 | val MAPE: 21.91%\n",
      "Epoch 40/1000 | train MSE: 0.14373 | val R2_log: 0.9260 | val RMSE: 2,047.14 | val MAE: 1,128.77 | val MAPE: 19.90%\n",
      "Epoch 41/1000 | train MSE: 0.14506 | val R2_log: 0.9374 | val RMSE: 2,074.22 | val MAE: 1,062.58 | val MAPE: 17.22%\n",
      "Epoch 42/1000 | train MSE: 0.14682 | val R2_log: 0.9238 | val RMSE: 2,090.90 | val MAE: 1,129.21 | val MAPE: 20.24%\n",
      "Epoch 43/1000 | train MSE: 0.14101 | val R2_log: 0.8941 | val RMSE: 2,808.60 | val MAE: 1,641.59 | val MAPE: 26.30%\n",
      "Epoch 44/1000 | train MSE: 0.14072 | val R2_log: 0.9126 | val RMSE: 2,615.68 | val MAE: 1,423.79 | val MAPE: 22.63%\n",
      "Epoch 45/1000 | train MSE: 0.13560 | val R2_log: 0.9348 | val RMSE: 2,221.77 | val MAE: 1,173.17 | val MAPE: 18.12%\n",
      "Epoch 46/1000 | train MSE: 0.13817 | val R2_log: 0.9410 | val RMSE: 2,028.55 | val MAE: 1,020.24 | val MAPE: 16.50%\n",
      "Epoch 47/1000 | train MSE: 0.13588 | val R2_log: 0.9306 | val RMSE: 2,284.56 | val MAE: 1,167.98 | val MAPE: 18.81%\n",
      "Epoch 48/1000 | train MSE: 0.13714 | val R2_log: 0.9451 | val RMSE: 1,888.63 | val MAE: 941.31 | val MAPE: 15.02%\n",
      "Epoch 49/1000 | train MSE: 0.13766 | val R2_log: 0.9249 | val RMSE: 2,621.56 | val MAE: 1,350.06 | val MAPE: 20.19%\n",
      "Epoch 50/1000 | train MSE: 0.13329 | val R2_log: 0.9235 | val RMSE: 2,449.67 | val MAE: 1,319.53 | val MAPE: 20.57%\n",
      "Epoch 51/1000 | train MSE: 0.13371 | val R2_log: 0.8866 | val RMSE: 2,574.85 | val MAE: 1,624.24 | val MAPE: 27.84%\n",
      "Epoch 52/1000 | train MSE: 0.13222 | val R2_log: 0.9473 | val RMSE: 1,878.74 | val MAE: 931.72 | val MAPE: 14.65%\n",
      "Epoch 53/1000 | train MSE: 0.13341 | val R2_log: 0.9218 | val RMSE: 2,465.78 | val MAE: 1,371.06 | val MAPE: 20.97%\n",
      "Epoch 54/1000 | train MSE: 0.12955 | val R2_log: 0.9217 | val RMSE: 2,245.01 | val MAE: 1,196.74 | val MAPE: 20.73%\n",
      "Epoch 55/1000 | train MSE: 0.12906 | val R2_log: 0.9343 | val RMSE: 2,184.45 | val MAE: 1,171.12 | val MAPE: 17.99%\n",
      "Epoch 56/1000 | train MSE: 0.12779 | val R2_log: 0.9196 | val RMSE: 2,293.03 | val MAE: 1,301.17 | val MAPE: 21.53%\n",
      "Epoch 57/1000 | train MSE: 0.12478 | val R2_log: 0.9446 | val RMSE: 2,090.67 | val MAE: 1,046.32 | val MAPE: 15.38%\n",
      "Epoch 58/1000 | train MSE: 0.12893 | val R2_log: 0.9426 | val RMSE: 1,850.43 | val MAE: 942.62 | val MAPE: 16.16%\n",
      "Epoch 59/1000 | train MSE: 0.12419 | val R2_log: 0.9090 | val RMSE: 2,596.80 | val MAE: 1,449.86 | val MAPE: 23.60%\n",
      "Epoch 60/1000 | train MSE: 0.12441 | val R2_log: 0.8914 | val RMSE: 2,475.11 | val MAE: 1,512.23 | val MAPE: 27.04%\n",
      "Epoch 61/1000 | train MSE: 0.11951 | val R2_log: 0.9349 | val RMSE: 1,993.09 | val MAE: 1,076.14 | val MAPE: 18.30%\n",
      "Epoch 62/1000 | train MSE: 0.13242 | val R2_log: 0.9055 | val RMSE: 2,293.08 | val MAE: 1,339.88 | val MAPE: 24.16%\n",
      "Epoch 63/1000 | train MSE: 0.12175 | val R2_log: 0.9358 | val RMSE: 1,990.28 | val MAE: 1,044.23 | val MAPE: 17.83%\n",
      "Epoch 64/1000 | train MSE: 0.12490 | val R2_log: 0.9339 | val RMSE: 2,059.28 | val MAE: 1,113.97 | val MAPE: 18.48%\n",
      "Epoch 65/1000 | train MSE: 0.11812 | val R2_log: 0.9142 | val RMSE: 2,372.30 | val MAE: 1,336.09 | val MAPE: 22.61%\n",
      "Epoch 66/1000 | train MSE: 0.12003 | val R2_log: 0.9190 | val RMSE: 2,481.45 | val MAE: 1,370.19 | val MAPE: 21.76%\n",
      "Epoch 67/1000 | train MSE: 0.11511 | val R2_log: 0.9006 | val RMSE: 2,449.63 | val MAE: 1,432.87 | val MAPE: 25.17%\n",
      "Epoch 68/1000 | train MSE: 0.11874 | val R2_log: 0.9429 | val RMSE: 1,899.88 | val MAE: 1,015.08 | val MAPE: 16.33%\n",
      "Epoch 69/1000 | train MSE: 0.11787 | val R2_log: 0.9276 | val RMSE: 2,053.50 | val MAE: 1,134.05 | val MAPE: 19.89%\n",
      "Epoch 70/1000 | train MSE: 0.11835 | val R2_log: 0.9399 | val RMSE: 1,849.58 | val MAE: 965.47 | val MAPE: 16.82%\n",
      "Epoch 71/1000 | train MSE: 0.11788 | val R2_log: 0.9422 | val RMSE: 1,879.60 | val MAE: 984.32 | val MAPE: 16.54%\n",
      "Epoch 72/1000 | train MSE: 0.11670 | val R2_log: 0.9437 | val RMSE: 1,886.22 | val MAE: 980.20 | val MAPE: 16.17%\n",
      "Epoch 73/1000 | train MSE: 0.11529 | val R2_log: 0.9070 | val RMSE: 2,367.16 | val MAE: 1,387.49 | val MAPE: 24.10%\n",
      "Epoch 74/1000 | train MSE: 0.11785 | val R2_log: 0.9372 | val RMSE: 2,255.89 | val MAE: 1,200.76 | val MAPE: 17.48%\n",
      "Epoch 75/1000 | train MSE: 0.11358 | val R2_log: 0.9285 | val RMSE: 1,850.17 | val MAE: 1,044.39 | val MAPE: 19.64%\n",
      "Epoch 76/1000 | train MSE: 0.11520 | val R2_log: 0.9486 | val RMSE: 1,833.04 | val MAE: 919.18 | val MAPE: 14.56%\n",
      "Epoch 77/1000 | train MSE: 0.11382 | val R2_log: 0.9367 | val RMSE: 1,945.96 | val MAE: 1,075.92 | val MAPE: 17.91%\n",
      "Epoch 78/1000 | train MSE: 0.11133 | val R2_log: 0.9460 | val RMSE: 1,797.34 | val MAE: 920.03 | val MAPE: 15.54%\n",
      "Epoch 79/1000 | train MSE: 0.11891 | val R2_log: 0.9448 | val RMSE: 2,056.35 | val MAE: 1,048.61 | val MAPE: 15.50%\n",
      "Epoch 80/1000 | train MSE: 0.10918 | val R2_log: 0.9353 | val RMSE: 1,800.13 | val MAE: 963.38 | val MAPE: 18.00%\n",
      "Epoch 81/1000 | train MSE: 0.11011 | val R2_log: 0.9145 | val RMSE: 2,067.31 | val MAE: 1,224.90 | val MAPE: 22.59%\n",
      "Epoch 82/1000 | train MSE: 0.11137 | val R2_log: 0.9308 | val RMSE: 2,053.64 | val MAE: 1,155.39 | val MAPE: 19.26%\n",
      "Epoch 83/1000 | train MSE: 0.10753 | val R2_log: 0.9449 | val RMSE: 1,821.79 | val MAE: 956.54 | val MAPE: 15.96%\n",
      "Epoch 84/1000 | train MSE: 0.10586 | val R2_log: 0.9472 | val RMSE: 1,874.99 | val MAE: 932.83 | val MAPE: 15.07%\n",
      "Epoch 85/1000 | train MSE: 0.10882 | val R2_log: 0.9392 | val RMSE: 2,207.05 | val MAE: 1,160.82 | val MAPE: 17.22%\n",
      "Epoch 86/1000 | train MSE: 0.10843 | val R2_log: 0.9473 | val RMSE: 1,920.06 | val MAE: 963.85 | val MAPE: 14.81%\n",
      "Epoch 87/1000 | train MSE: 0.10813 | val R2_log: 0.9487 | val RMSE: 1,800.12 | val MAE: 898.54 | val MAPE: 14.66%\n",
      "Epoch 88/1000 | train MSE: 0.10412 | val R2_log: 0.9367 | val RMSE: 2,101.46 | val MAE: 1,100.77 | val MAPE: 17.83%\n",
      "Epoch 89/1000 | train MSE: 0.10484 | val R2_log: 0.9474 | val RMSE: 1,813.52 | val MAE: 908.50 | val MAPE: 15.12%\n",
      "Epoch 90/1000 | train MSE: 0.10589 | val R2_log: 0.9371 | val RMSE: 1,962.50 | val MAE: 1,041.45 | val MAPE: 17.80%\n",
      "Epoch 91/1000 | train MSE: 0.10428 | val R2_log: 0.8756 | val RMSE: 2,855.77 | val MAE: 1,749.25 | val MAPE: 29.91%\n",
      "Epoch 92/1000 | train MSE: 0.10702 | val R2_log: 0.9452 | val RMSE: 1,840.67 | val MAE: 942.40 | val MAPE: 15.81%\n",
      "Epoch 93/1000 | train MSE: 0.10413 | val R2_log: 0.9483 | val RMSE: 1,804.28 | val MAE: 926.76 | val MAPE: 14.88%\n",
      "Epoch 94/1000 | train MSE: 0.10146 | val R2_log: 0.9439 | val RMSE: 1,776.89 | val MAE: 926.90 | val MAPE: 16.17%\n",
      "Epoch 95/1000 | train MSE: 0.10688 | val R2_log: 0.9279 | val RMSE: 1,817.70 | val MAE: 995.92 | val MAPE: 19.44%\n",
      "Epoch 96/1000 | train MSE: 0.10424 | val R2_log: 0.9504 | val RMSE: 1,757.52 | val MAE: 881.97 | val MAPE: 14.15%\n",
      "Epoch 97/1000 | train MSE: 0.10110 | val R2_log: 0.9407 | val RMSE: 1,845.41 | val MAE: 986.54 | val MAPE: 17.00%\n",
      "Epoch 98/1000 | train MSE: 0.10144 | val R2_log: 0.9196 | val RMSE: 2,318.47 | val MAE: 1,267.19 | val MAPE: 21.55%\n",
      "Epoch 99/1000 | train MSE: 0.10176 | val R2_log: 0.9058 | val RMSE: 2,648.09 | val MAE: 1,582.21 | val MAPE: 24.13%\n",
      "Epoch 100/1000 | train MSE: 0.10238 | val R2_log: 0.9409 | val RMSE: 1,867.38 | val MAE: 989.27 | val MAPE: 16.87%\n",
      "Epoch 101/1000 | train MSE: 0.10260 | val R2_log: 0.9463 | val RMSE: 1,872.86 | val MAE: 960.24 | val MAPE: 14.92%\n",
      "Epoch 102/1000 | train MSE: 0.10350 | val R2_log: 0.9468 | val RMSE: 1,932.52 | val MAE: 985.53 | val MAPE: 15.16%\n",
      "Epoch 103/1000 | train MSE: 0.10139 | val R2_log: 0.9362 | val RMSE: 1,880.55 | val MAE: 1,017.79 | val MAPE: 18.06%\n",
      "Epoch 104/1000 | train MSE: 0.10097 | val R2_log: 0.9495 | val RMSE: 1,799.69 | val MAE: 883.53 | val MAPE: 14.38%\n",
      "Epoch 105/1000 | train MSE: 0.10602 | val R2_log: 0.9435 | val RMSE: 1,932.70 | val MAE: 998.53 | val MAPE: 16.14%\n",
      "Epoch 106/1000 | train MSE: 0.10004 | val R2_log: 0.9307 | val RMSE: 2,213.88 | val MAE: 1,185.96 | val MAPE: 19.24%\n",
      "Epoch 107/1000 | train MSE: 0.10029 | val R2_log: 0.9471 | val RMSE: 2,031.47 | val MAE: 984.99 | val MAPE: 15.03%\n",
      "Epoch 108/1000 | train MSE: 0.10000 | val R2_log: 0.9310 | val RMSE: 1,952.95 | val MAE: 1,100.80 | val MAPE: 19.28%\n",
      "Epoch 109/1000 | train MSE: 0.09719 | val R2_log: 0.9521 | val RMSE: 1,733.13 | val MAE: 851.89 | val MAPE: 13.69%\n",
      "Epoch 110/1000 | train MSE: 0.09737 | val R2_log: 0.9466 | val RMSE: 1,820.73 | val MAE: 952.64 | val MAPE: 15.46%\n",
      "Epoch 111/1000 | train MSE: 0.10332 | val R2_log: 0.9406 | val RMSE: 1,750.04 | val MAE: 928.82 | val MAPE: 16.89%\n",
      "Epoch 112/1000 | train MSE: 0.09725 | val R2_log: 0.9388 | val RMSE: 2,070.37 | val MAE: 1,084.09 | val MAPE: 17.40%\n",
      "Epoch 113/1000 | train MSE: 0.09834 | val R2_log: 0.9491 | val RMSE: 1,764.59 | val MAE: 904.14 | val MAPE: 14.56%\n",
      "Epoch 114/1000 | train MSE: 0.09902 | val R2_log: 0.9475 | val RMSE: 1,814.95 | val MAE: 947.29 | val MAPE: 14.85%\n",
      "Epoch 115/1000 | train MSE: 0.09703 | val R2_log: 0.9489 | val RMSE: 1,898.22 | val MAE: 947.47 | val MAPE: 14.66%\n",
      "Epoch 116/1000 | train MSE: 0.09754 | val R2_log: 0.9363 | val RMSE: 2,098.74 | val MAE: 1,092.43 | val MAPE: 17.92%\n",
      "Epoch 117/1000 | train MSE: 0.09787 | val R2_log: 0.9474 | val RMSE: 1,799.73 | val MAE: 902.95 | val MAPE: 14.23%\n",
      "Epoch 118/1000 | train MSE: 0.10014 | val R2_log: 0.9455 | val RMSE: 1,832.56 | val MAE: 902.59 | val MAPE: 13.90%\n",
      "Epoch 119/1000 | train MSE: 0.09822 | val R2_log: 0.9423 | val RMSE: 1,805.64 | val MAE: 934.41 | val MAPE: 16.52%\n",
      "Epoch 120/1000 | train MSE: 0.09678 | val R2_log: 0.9459 | val RMSE: 1,795.09 | val MAE: 929.19 | val MAPE: 15.60%\n",
      "Epoch 121/1000 | train MSE: 0.09453 | val R2_log: 0.9127 | val RMSE: 2,624.98 | val MAE: 1,534.03 | val MAPE: 22.75%\n",
      "Epoch 122/1000 | train MSE: 0.09502 | val R2_log: 0.9476 | val RMSE: 1,859.84 | val MAE: 948.19 | val MAPE: 15.10%\n",
      "Epoch 123/1000 | train MSE: 0.09556 | val R2_log: 0.9431 | val RMSE: 1,832.41 | val MAE: 945.43 | val MAPE: 16.27%\n",
      "Epoch 124/1000 | train MSE: 0.09388 | val R2_log: 0.9144 | val RMSE: 2,773.42 | val MAE: 1,513.13 | val MAPE: 22.47%\n",
      "Epoch 125/1000 | train MSE: 0.09888 | val R2_log: 0.8889 | val RMSE: 3,505.96 | val MAE: 1,940.90 | val MAPE: 27.33%\n",
      "Epoch 126/1000 | train MSE: 0.09869 | val R2_log: 0.9281 | val RMSE: 2,177.13 | val MAE: 1,149.72 | val MAPE: 19.62%\n",
      "Epoch 127/1000 | train MSE: 0.09542 | val R2_log: 0.9326 | val RMSE: 2,342.65 | val MAE: 1,230.87 | val MAPE: 18.75%\n",
      "Epoch 128/1000 | train MSE: 0.09496 | val R2_log: 0.9395 | val RMSE: 2,158.51 | val MAE: 1,093.98 | val MAPE: 17.16%\n",
      "Epoch 129/1000 | train MSE: 0.09121 | val R2_log: 0.9380 | val RMSE: 1,972.92 | val MAE: 1,004.53 | val MAPE: 17.30%\n",
      "Epoch 130/1000 | train MSE: 0.09194 | val R2_log: 0.9295 | val RMSE: 1,855.06 | val MAE: 1,022.58 | val MAPE: 19.33%\n",
      "Epoch 131/1000 | train MSE: 0.09376 | val R2_log: 0.9511 | val RMSE: 1,766.56 | val MAE: 881.20 | val MAPE: 14.11%\n",
      "Epoch 132/1000 | train MSE: 0.09512 | val R2_log: 0.9496 | val RMSE: 1,752.57 | val MAE: 895.64 | val MAPE: 14.45%\n",
      "Epoch 133/1000 | train MSE: 0.09586 | val R2_log: 0.9337 | val RMSE: 2,347.59 | val MAE: 1,251.65 | val MAPE: 18.30%\n",
      "Epoch 134/1000 | train MSE: 0.09202 | val R2_log: 0.9381 | val RMSE: 1,911.65 | val MAE: 1,056.32 | val MAPE: 17.72%\n",
      "Epoch 135/1000 | train MSE: 0.09338 | val R2_log: 0.8968 | val RMSE: 2,963.77 | val MAE: 1,701.32 | val MAPE: 26.00%\n",
      "Epoch 00135: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch 136/1000 | train MSE: 0.09233 | val R2_log: 0.9485 | val RMSE: 1,804.27 | val MAE: 914.71 | val MAPE: 15.01%\n",
      "Epoch 137/1000 | train MSE: 0.08964 | val R2_log: 0.9399 | val RMSE: 1,941.71 | val MAE: 1,047.58 | val MAPE: 17.25%\n",
      "Epoch 138/1000 | train MSE: 0.09078 | val R2_log: 0.9422 | val RMSE: 2,024.77 | val MAE: 1,050.92 | val MAPE: 16.69%\n",
      "Epoch 139/1000 | train MSE: 0.08817 | val R2_log: 0.9517 | val RMSE: 1,747.37 | val MAE: 853.77 | val MAPE: 13.93%\n",
      "Epoch 140/1000 | train MSE: 0.09103 | val R2_log: 0.9513 | val RMSE: 1,783.08 | val MAE: 862.92 | val MAPE: 13.79%\n",
      "Epoch 141/1000 | train MSE: 0.08988 | val R2_log: 0.9449 | val RMSE: 1,873.61 | val MAE: 973.89 | val MAPE: 15.99%\n",
      "Epoch 142/1000 | train MSE: 0.08927 | val R2_log: 0.9489 | val RMSE: 1,802.45 | val MAE: 913.17 | val MAPE: 14.97%\n",
      "Epoch 143/1000 | train MSE: 0.08651 | val R2_log: 0.9444 | val RMSE: 2,235.93 | val MAE: 1,107.14 | val MAPE: 15.81%\n",
      "Epoch 144/1000 | train MSE: 0.08975 | val R2_log: 0.9432 | val RMSE: 1,917.39 | val MAE: 1,006.15 | val MAPE: 16.44%\n",
      "Epoch 145/1000 | train MSE: 0.09024 | val R2_log: 0.9493 | val RMSE: 1,843.91 | val MAE: 914.19 | val MAPE: 14.71%\n",
      "Epoch 146/1000 | train MSE: 0.08941 | val R2_log: 0.9285 | val RMSE: 2,114.73 | val MAE: 1,182.54 | val MAPE: 19.90%\n",
      "Epoch 147/1000 | train MSE: 0.08981 | val R2_log: 0.9468 | val RMSE: 1,789.13 | val MAE: 929.80 | val MAPE: 15.35%\n",
      "Epoch 148/1000 | train MSE: 0.08889 | val R2_log: 0.9404 | val RMSE: 2,192.73 | val MAE: 1,124.90 | val MAPE: 17.03%\n",
      "Epoch 149/1000 | train MSE: 0.08926 | val R2_log: 0.9393 | val RMSE: 1,991.80 | val MAE: 1,070.33 | val MAPE: 17.41%\n",
      "Epoch 150/1000 | train MSE: 0.08720 | val R2_log: 0.9358 | val RMSE: 1,904.17 | val MAE: 1,056.00 | val MAPE: 18.24%\n",
      "Epoch 151/1000 | train MSE: 0.08841 | val R2_log: 0.9475 | val RMSE: 1,766.31 | val MAE: 882.76 | val MAPE: 15.16%\n",
      "Epoch 152/1000 | train MSE: 0.08804 | val R2_log: 0.9504 | val RMSE: 1,771.45 | val MAE: 881.53 | val MAPE: 14.41%\n",
      "Epoch 153/1000 | train MSE: 0.09009 | val R2_log: 0.9485 | val RMSE: 1,848.90 | val MAE: 927.69 | val MAPE: 14.99%\n",
      "Epoch 154/1000 | train MSE: 0.08880 | val R2_log: 0.9375 | val RMSE: 1,965.19 | val MAE: 1,073.99 | val MAPE: 17.82%\n",
      "Epoch 155/1000 | train MSE: 0.08884 | val R2_log: 0.9503 | val RMSE: 1,766.83 | val MAE: 880.75 | val MAPE: 14.42%\n",
      "Epoch 156/1000 | train MSE: 0.08674 | val R2_log: 0.9460 | val RMSE: 1,924.99 | val MAE: 969.93 | val MAPE: 15.61%\n",
      "Epoch 157/1000 | train MSE: 0.08756 | val R2_log: 0.9483 | val RMSE: 1,822.06 | val MAE: 944.59 | val MAPE: 15.05%\n",
      "Epoch 158/1000 | train MSE: 0.08976 | val R2_log: 0.9505 | val RMSE: 1,754.70 | val MAE: 895.78 | val MAPE: 14.15%\n",
      "Epoch 159/1000 | train MSE: 0.08743 | val R2_log: 0.9492 | val RMSE: 1,776.10 | val MAE: 896.56 | val MAPE: 14.70%\n",
      "Epoch 160/1000 | train MSE: 0.08891 | val R2_log: 0.9221 | val RMSE: 2,341.02 | val MAE: 1,351.72 | val MAPE: 21.22%\n",
      "Epoch 161/1000 | train MSE: 0.08754 | val R2_log: 0.9496 | val RMSE: 1,774.40 | val MAE: 913.84 | val MAPE: 14.63%\n",
      "Epoch 00161: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 162/1000 | train MSE: 0.08450 | val R2_log: 0.9461 | val RMSE: 1,856.08 | val MAE: 962.43 | val MAPE: 15.80%\n",
      "Epoch 163/1000 | train MSE: 0.08479 | val R2_log: 0.9468 | val RMSE: 1,912.60 | val MAE: 994.22 | val MAPE: 15.53%\n",
      "Epoch 164/1000 | train MSE: 0.08420 | val R2_log: 0.9504 | val RMSE: 1,785.48 | val MAE: 904.69 | val MAPE: 14.64%\n",
      "Epoch 165/1000 | train MSE: 0.08563 | val R2_log: 0.9408 | val RMSE: 2,009.99 | val MAE: 1,059.70 | val MAPE: 16.91%\n",
      "Epoch 166/1000 | train MSE: 0.08596 | val R2_log: 0.9428 | val RMSE: 2,074.81 | val MAE: 1,068.96 | val MAPE: 16.51%\n",
      "Epoch 167/1000 | train MSE: 0.08546 | val R2_log: 0.9528 | val RMSE: 1,751.59 | val MAE: 854.78 | val MAPE: 13.69%\n",
      "Epoch 168/1000 | train MSE: 0.08509 | val R2_log: 0.9519 | val RMSE: 1,736.99 | val MAE: 870.65 | val MAPE: 13.95%\n",
      "Epoch 169/1000 | train MSE: 0.08657 | val R2_log: 0.9497 | val RMSE: 1,818.28 | val MAE: 923.40 | val MAPE: 14.62%\n",
      "Epoch 170/1000 | train MSE: 0.08691 | val R2_log: 0.9405 | val RMSE: 2,032.33 | val MAE: 1,063.17 | val MAPE: 17.13%\n",
      "Epoch 171/1000 | train MSE: 0.08611 | val R2_log: 0.9406 | val RMSE: 2,187.41 | val MAE: 1,138.60 | val MAPE: 16.88%\n",
      "Epoch 172/1000 | train MSE: 0.08514 | val R2_log: 0.9493 | val RMSE: 1,898.50 | val MAE: 949.55 | val MAPE: 14.81%\n",
      "Epoch 173/1000 | train MSE: 0.08398 | val R2_log: 0.9482 | val RMSE: 1,798.00 | val MAE: 917.93 | val MAPE: 15.08%\n",
      "Epoch 174/1000 | train MSE: 0.08525 | val R2_log: 0.9517 | val RMSE: 1,799.35 | val MAE: 888.10 | val MAPE: 14.12%\n",
      "Epoch 175/1000 | train MSE: 0.08597 | val R2_log: 0.9496 | val RMSE: 1,863.57 | val MAE: 933.30 | val MAPE: 14.72%\n",
      "Epoch 176/1000 | train MSE: 0.08582 | val R2_log: 0.9409 | val RMSE: 1,972.86 | val MAE: 1,057.55 | val MAPE: 17.07%\n",
      "Epoch 177/1000 | train MSE: 0.08495 | val R2_log: 0.9418 | val RMSE: 1,982.56 | val MAE: 1,061.69 | val MAPE: 16.78%\n",
      "Epoch 178/1000 | train MSE: 0.08460 | val R2_log: 0.9483 | val RMSE: 1,828.79 | val MAE: 926.95 | val MAPE: 15.18%\n",
      "Epoch 179/1000 | train MSE: 0.08403 | val R2_log: 0.9496 | val RMSE: 1,782.01 | val MAE: 907.57 | val MAPE: 14.78%\n",
      "Epoch 180/1000 | train MSE: 0.08606 | val R2_log: 0.9419 | val RMSE: 2,075.24 | val MAE: 1,067.73 | val MAPE: 16.74%\n",
      "Epoch 181/1000 | train MSE: 0.08549 | val R2_log: 0.9475 | val RMSE: 1,859.24 | val MAE: 943.73 | val MAPE: 15.32%\n",
      "Epoch 182/1000 | train MSE: 0.08447 | val R2_log: 0.9441 | val RMSE: 1,987.83 | val MAE: 1,039.44 | val MAPE: 16.18%\n",
      "Epoch 183/1000 | train MSE: 0.08526 | val R2_log: 0.9524 | val RMSE: 1,753.14 | val MAE: 853.27 | val MAPE: 13.86%\n",
      "Epoch 184/1000 | train MSE: 0.08494 | val R2_log: 0.9397 | val RMSE: 2,246.01 | val MAE: 1,150.18 | val MAPE: 17.26%\n",
      "Epoch 185/1000 | train MSE: 0.08512 | val R2_log: 0.9459 | val RMSE: 1,888.57 | val MAE: 972.78 | val MAPE: 15.92%\n",
      "Epoch 186/1000 | train MSE: 0.08446 | val R2_log: 0.9518 | val RMSE: 1,762.66 | val MAE: 883.19 | val MAPE: 14.18%\n",
      "Epoch 187/1000 | train MSE: 0.08301 | val R2_log: 0.9487 | val RMSE: 1,858.56 | val MAE: 944.49 | val MAPE: 14.83%\n",
      "Epoch 188/1000 | train MSE: 0.08337 | val R2_log: 0.9512 | val RMSE: 1,867.36 | val MAE: 911.08 | val MAPE: 13.94%\n",
      "Epoch 189/1000 | train MSE: 0.08194 | val R2_log: 0.9509 | val RMSE: 1,748.80 | val MAE: 872.23 | val MAPE: 14.19%\n",
      "Epoch 190/1000 | train MSE: 0.08507 | val R2_log: 0.9437 | val RMSE: 2,098.15 | val MAE: 1,065.94 | val MAPE: 16.34%\n",
      "Epoch 191/1000 | train MSE: 0.08435 | val R2_log: 0.9500 | val RMSE: 1,784.02 | val MAE: 907.92 | val MAPE: 14.68%\n",
      "Epoch 192/1000 | train MSE: 0.08405 | val R2_log: 0.9495 | val RMSE: 1,752.68 | val MAE: 898.25 | val MAPE: 14.66%\n",
      "Epoch 193/1000 | train MSE: 0.08476 | val R2_log: 0.9512 | val RMSE: 1,732.61 | val MAE: 866.14 | val MAPE: 14.23%\n",
      "Epoch 00193: reducing learning rate of group 0 to 2.5000e-04.\n",
      "Epoch 194/1000 | train MSE: 0.08339 | val R2_log: 0.9490 | val RMSE: 1,762.58 | val MAE: 898.07 | val MAPE: 14.96%\n",
      "Epoch 195/1000 | train MSE: 0.08332 | val R2_log: 0.9519 | val RMSE: 1,744.98 | val MAE: 870.58 | val MAPE: 13.91%\n",
      "Epoch 196/1000 | train MSE: 0.08249 | val R2_log: 0.9457 | val RMSE: 1,974.12 | val MAE: 1,022.98 | val MAPE: 15.71%\n",
      "Epoch 197/1000 | train MSE: 0.08263 | val R2_log: 0.9518 | val RMSE: 1,760.85 | val MAE: 879.18 | val MAPE: 13.99%\n",
      "Epoch 198/1000 | train MSE: 0.08269 | val R2_log: 0.9508 | val RMSE: 1,776.65 | val MAE: 896.53 | val MAPE: 14.35%\n",
      "Epoch 199/1000 | train MSE: 0.08333 | val R2_log: 0.9465 | val RMSE: 1,981.87 | val MAE: 1,000.59 | val MAPE: 15.50%\n",
      "Epoch 200/1000 | train MSE: 0.08164 | val R2_log: 0.9430 | val RMSE: 1,920.72 | val MAE: 1,013.97 | val MAPE: 16.56%\n",
      "Epoch 201/1000 | train MSE: 0.08157 | val R2_log: 0.9522 | val RMSE: 1,736.36 | val MAE: 858.67 | val MAPE: 13.87%\n",
      "Epoch 202/1000 | train MSE: 0.08150 | val R2_log: 0.9495 | val RMSE: 1,795.71 | val MAE: 909.17 | val MAPE: 14.85%\n",
      "Epoch 203/1000 | train MSE: 0.08159 | val R2_log: 0.9425 | val RMSE: 1,859.66 | val MAE: 1,004.78 | val MAPE: 16.63%\n",
      "Epoch 204/1000 | train MSE: 0.08349 | val R2_log: 0.9463 | val RMSE: 1,900.46 | val MAE: 985.25 | val MAPE: 15.54%\n",
      "Epoch 205/1000 | train MSE: 0.08274 | val R2_log: 0.9494 | val RMSE: 1,844.91 | val MAE: 934.91 | val MAPE: 14.72%\n",
      "Epoch 206/1000 | train MSE: 0.08144 | val R2_log: 0.9503 | val RMSE: 1,866.21 | val MAE: 929.67 | val MAPE: 14.35%\n",
      "Epoch 207/1000 | train MSE: 0.08286 | val R2_log: 0.9484 | val RMSE: 1,872.89 | val MAE: 961.81 | val MAPE: 15.13%\n",
      "Epoch 208/1000 | train MSE: 0.07995 | val R2_log: 0.9472 | val RMSE: 1,864.99 | val MAE: 954.95 | val MAPE: 15.44%\n",
      "Epoch 209/1000 | train MSE: 0.08239 | val R2_log: 0.9506 | val RMSE: 1,773.18 | val MAE: 895.82 | val MAPE: 14.51%\n",
      "Epoch 210/1000 | train MSE: 0.08035 | val R2_log: 0.9469 | val RMSE: 1,908.19 | val MAE: 975.45 | val MAPE: 15.51%\n",
      "Epoch 211/1000 | train MSE: 0.08238 | val R2_log: 0.9471 | val RMSE: 1,905.24 | val MAE: 975.26 | val MAPE: 15.39%\n",
      "Epoch 212/1000 | train MSE: 0.08083 | val R2_log: 0.9513 | val RMSE: 1,788.57 | val MAE: 888.70 | val MAPE: 14.28%\n",
      "Epoch 213/1000 | train MSE: 0.08197 | val R2_log: 0.9491 | val RMSE: 1,784.93 | val MAE: 908.50 | val MAPE: 14.84%\n",
      "Epoch 214/1000 | train MSE: 0.08276 | val R2_log: 0.9496 | val RMSE: 1,754.10 | val MAE: 889.91 | val MAPE: 14.74%\n",
      "Epoch 215/1000 | train MSE: 0.08185 | val R2_log: 0.9481 | val RMSE: 1,922.89 | val MAE: 977.04 | val MAPE: 15.23%\n",
      "Epoch 216/1000 | train MSE: 0.08083 | val R2_log: 0.9517 | val RMSE: 1,789.81 | val MAE: 895.13 | val MAPE: 14.03%\n",
      "Epoch 217/1000 | train MSE: 0.08130 | val R2_log: 0.9525 | val RMSE: 1,722.93 | val MAE: 852.93 | val MAPE: 13.83%\n",
      "Epoch 218/1000 | train MSE: 0.08068 | val R2_log: 0.9423 | val RMSE: 1,871.52 | val MAE: 989.29 | val MAPE: 16.72%\n",
      "Epoch 219/1000 | train MSE: 0.08197 | val R2_log: 0.9488 | val RMSE: 1,867.95 | val MAE: 936.04 | val MAPE: 15.01%\n",
      "Epoch 00219: reducing learning rate of group 0 to 1.2500e-04.\n",
      "Epoch 220/1000 | train MSE: 0.08012 | val R2_log: 0.9523 | val RMSE: 1,762.33 | val MAE: 880.36 | val MAPE: 13.90%\n",
      "Epoch 221/1000 | train MSE: 0.08173 | val R2_log: 0.9468 | val RMSE: 1,908.37 | val MAE: 977.28 | val MAPE: 15.54%\n",
      "Epoch 222/1000 | train MSE: 0.08056 | val R2_log: 0.9497 | val RMSE: 1,860.06 | val MAE: 940.84 | val MAPE: 14.78%\n",
      "Epoch 223/1000 | train MSE: 0.08131 | val R2_log: 0.9506 | val RMSE: 1,767.58 | val MAE: 900.10 | val MAPE: 14.51%\n",
      "Epoch 224/1000 | train MSE: 0.07945 | val R2_log: 0.9509 | val RMSE: 1,805.17 | val MAE: 915.88 | val MAPE: 14.29%\n",
      "Epoch 225/1000 | train MSE: 0.08167 | val R2_log: 0.9518 | val RMSE: 1,715.28 | val MAE: 856.76 | val MAPE: 14.24%\n",
      "Epoch 226/1000 | train MSE: 0.08067 | val R2_log: 0.9508 | val RMSE: 1,801.91 | val MAE: 911.00 | val MAPE: 14.28%\n",
      "Epoch 227/1000 | train MSE: 0.07906 | val R2_log: 0.9509 | val RMSE: 1,810.53 | val MAE: 911.31 | val MAPE: 14.43%\n",
      "Epoch 228/1000 | train MSE: 0.07991 | val R2_log: 0.9471 | val RMSE: 1,873.93 | val MAE: 973.17 | val MAPE: 15.58%\n",
      "Epoch 229/1000 | train MSE: 0.07931 | val R2_log: 0.9515 | val RMSE: 1,863.77 | val MAE: 929.98 | val MAPE: 14.19%\n",
      "Epoch 230/1000 | train MSE: 0.08201 | val R2_log: 0.9525 | val RMSE: 1,775.06 | val MAE: 888.82 | val MAPE: 13.97%\n",
      "Epoch 231/1000 | train MSE: 0.07975 | val R2_log: 0.9487 | val RMSE: 1,945.48 | val MAE: 975.89 | val MAPE: 14.89%\n",
      "Epoch 232/1000 | train MSE: 0.07941 | val R2_log: 0.9470 | val RMSE: 1,886.61 | val MAE: 970.34 | val MAPE: 15.49%\n",
      "Epoch 233/1000 | train MSE: 0.08176 | val R2_log: 0.9509 | val RMSE: 1,765.79 | val MAE: 897.39 | val MAPE: 14.33%\n",
      "Epoch 234/1000 | train MSE: 0.07913 | val R2_log: 0.9463 | val RMSE: 1,874.06 | val MAE: 993.59 | val MAPE: 15.73%\n",
      "Epoch 235/1000 | train MSE: 0.07944 | val R2_log: 0.9496 | val RMSE: 1,824.07 | val MAE: 926.85 | val MAPE: 14.77%\n",
      "Epoch 236/1000 | train MSE: 0.07917 | val R2_log: 0.9496 | val RMSE: 1,841.51 | val MAE: 932.28 | val MAPE: 14.86%\n",
      "Epoch 237/1000 | train MSE: 0.08151 | val R2_log: 0.9433 | val RMSE: 1,991.86 | val MAE: 1,041.37 | val MAPE: 16.48%\n",
      "Epoch 238/1000 | train MSE: 0.08070 | val R2_log: 0.9466 | val RMSE: 1,912.12 | val MAE: 989.13 | val MAPE: 15.67%\n",
      "Epoch 239/1000 | train MSE: 0.07925 | val R2_log: 0.9496 | val RMSE: 1,851.60 | val MAE: 939.18 | val MAPE: 14.78%\n",
      "Epoch 240/1000 | train MSE: 0.08142 | val R2_log: 0.9505 | val RMSE: 1,825.98 | val MAE: 913.89 | val MAPE: 14.29%\n",
      "Epoch 241/1000 | train MSE: 0.08050 | val R2_log: 0.9510 | val RMSE: 1,724.26 | val MAE: 869.99 | val MAPE: 14.45%\n",
      "Epoch 242/1000 | train MSE: 0.07967 | val R2_log: 0.9470 | val RMSE: 1,865.85 | val MAE: 967.78 | val MAPE: 15.57%\n",
      "Epoch 243/1000 | train MSE: 0.08102 | val R2_log: 0.9521 | val RMSE: 1,780.91 | val MAE: 882.53 | val MAPE: 13.85%\n",
      "Epoch 244/1000 | train MSE: 0.07860 | val R2_log: 0.9513 | val RMSE: 1,828.64 | val MAE: 914.81 | val MAPE: 14.17%\n",
      "Epoch 245/1000 | train MSE: 0.08150 | val R2_log: 0.9498 | val RMSE: 1,828.56 | val MAE: 931.00 | val MAPE: 14.81%\n",
      "Epoch 00245: reducing learning rate of group 0 to 6.2500e-05.\n",
      "Epoch 246/1000 | train MSE: 0.07956 | val R2_log: 0.9504 | val RMSE: 1,758.61 | val MAE: 898.77 | val MAPE: 14.65%\n",
      "Epoch 247/1000 | train MSE: 0.08054 | val R2_log: 0.9477 | val RMSE: 1,832.11 | val MAE: 947.92 | val MAPE: 15.38%\n",
      "Epoch 248/1000 | train MSE: 0.07906 | val R2_log: 0.9491 | val RMSE: 1,850.23 | val MAE: 949.16 | val MAPE: 14.93%\n",
      "Epoch 249/1000 | train MSE: 0.08070 | val R2_log: 0.9484 | val RMSE: 1,829.17 | val MAE: 947.05 | val MAPE: 15.19%\n",
      "Epoch 250/1000 | train MSE: 0.07915 | val R2_log: 0.9516 | val RMSE: 1,744.13 | val MAE: 881.03 | val MAPE: 14.11%\n",
      "Epoch 251/1000 | train MSE: 0.08197 | val R2_log: 0.9505 | val RMSE: 1,823.95 | val MAE: 925.98 | val MAPE: 14.55%\n",
      "Epoch 252/1000 | train MSE: 0.07811 | val R2_log: 0.9509 | val RMSE: 1,791.08 | val MAE: 905.70 | val MAPE: 14.45%\n",
      "Epoch 253/1000 | train MSE: 0.08050 | val R2_log: 0.9517 | val RMSE: 1,750.13 | val MAE: 881.04 | val MAPE: 14.18%\n",
      "Epoch 254/1000 | train MSE: 0.07765 | val R2_log: 0.9505 | val RMSE: 1,767.66 | val MAE: 895.99 | val MAPE: 14.58%\n",
      "Epoch 255/1000 | train MSE: 0.07874 | val R2_log: 0.9485 | val RMSE: 1,915.23 | val MAE: 973.39 | val MAPE: 15.10%\n",
      "Epoch 256/1000 | train MSE: 0.07776 | val R2_log: 0.9498 | val RMSE: 1,840.78 | val MAE: 936.26 | val MAPE: 14.82%\n",
      "Epoch 257/1000 | train MSE: 0.07827 | val R2_log: 0.9501 | val RMSE: 1,853.09 | val MAE: 938.47 | val MAPE: 14.58%\n",
      "Epoch 258/1000 | train MSE: 0.07982 | val R2_log: 0.9499 | val RMSE: 1,830.18 | val MAE: 926.53 | val MAPE: 14.60%\n",
      "Epoch 259/1000 | train MSE: 0.07971 | val R2_log: 0.9529 | val RMSE: 1,738.47 | val MAE: 865.43 | val MAPE: 13.81%\n",
      "Epoch 260/1000 | train MSE: 0.08013 | val R2_log: 0.9522 | val RMSE: 1,776.47 | val MAE: 887.42 | val MAPE: 14.04%\n",
      "Epoch 261/1000 | train MSE: 0.07971 | val R2_log: 0.9512 | val RMSE: 1,795.94 | val MAE: 907.60 | val MAPE: 14.34%\n",
      "Epoch 262/1000 | train MSE: 0.07859 | val R2_log: 0.9495 | val RMSE: 1,875.29 | val MAE: 955.35 | val MAPE: 14.81%\n",
      "Epoch 263/1000 | train MSE: 0.08225 | val R2_log: 0.9519 | val RMSE: 1,778.33 | val MAE: 889.32 | val MAPE: 14.07%\n",
      "Epoch 264/1000 | train MSE: 0.07866 | val R2_log: 0.9523 | val RMSE: 1,787.80 | val MAE: 892.83 | val MAPE: 13.97%\n",
      "Epoch 265/1000 | train MSE: 0.07803 | val R2_log: 0.9532 | val RMSE: 1,721.86 | val MAE: 856.80 | val MAPE: 13.79%\n",
      "Epoch 266/1000 | train MSE: 0.08107 | val R2_log: 0.9521 | val RMSE: 1,766.89 | val MAE: 883.48 | val MAPE: 14.10%\n",
      "Epoch 267/1000 | train MSE: 0.07874 | val R2_log: 0.9480 | val RMSE: 1,897.92 | val MAE: 972.29 | val MAPE: 15.31%\n",
      "Epoch 268/1000 | train MSE: 0.08065 | val R2_log: 0.9500 | val RMSE: 1,840.29 | val MAE: 943.10 | val MAPE: 14.77%\n",
      "Epoch 269/1000 | train MSE: 0.07914 | val R2_log: 0.9478 | val RMSE: 1,962.09 | val MAE: 993.40 | val MAPE: 15.35%\n",
      "Epoch 270/1000 | train MSE: 0.07781 | val R2_log: 0.9504 | val RMSE: 1,771.27 | val MAE: 904.53 | val MAPE: 14.65%\n",
      "Epoch 271/1000 | train MSE: 0.07892 | val R2_log: 0.9478 | val RMSE: 1,938.92 | val MAE: 991.09 | val MAPE: 15.32%\n",
      "Epoch 272/1000 | train MSE: 0.07745 | val R2_log: 0.9504 | val RMSE: 1,790.31 | val MAE: 913.52 | val MAPE: 14.53%\n",
      "Epoch 273/1000 | train MSE: 0.07823 | val R2_log: 0.9504 | val RMSE: 1,840.97 | val MAE: 934.76 | val MAPE: 14.62%\n",
      "Epoch 274/1000 | train MSE: 0.07982 | val R2_log: 0.9489 | val RMSE: 1,856.74 | val MAE: 948.87 | val MAPE: 15.00%\n",
      "Epoch 275/1000 | train MSE: 0.08095 | val R2_log: 0.9495 | val RMSE: 1,851.84 | val MAE: 940.48 | val MAPE: 14.77%\n",
      "Epoch 276/1000 | train MSE: 0.07959 | val R2_log: 0.9474 | val RMSE: 1,924.23 | val MAE: 978.26 | val MAPE: 15.38%\n",
      "Epoch 277/1000 | train MSE: 0.07924 | val R2_log: 0.9520 | val RMSE: 1,783.63 | val MAE: 889.73 | val MAPE: 13.93%\n",
      "Epoch 278/1000 | train MSE: 0.07896 | val R2_log: 0.9518 | val RMSE: 1,795.01 | val MAE: 894.54 | val MAPE: 14.14%\n",
      "Epoch 279/1000 | train MSE: 0.07918 | val R2_log: 0.9520 | val RMSE: 1,755.05 | val MAE: 880.98 | val MAPE: 14.05%\n",
      "Epoch 280/1000 | train MSE: 0.07793 | val R2_log: 0.9489 | val RMSE: 1,947.07 | val MAE: 976.42 | val MAPE: 14.89%\n",
      "Epoch 281/1000 | train MSE: 0.07930 | val R2_log: 0.9497 | val RMSE: 1,809.48 | val MAE: 918.86 | val MAPE: 14.80%\n",
      "Epoch 282/1000 | train MSE: 0.07982 | val R2_log: 0.9496 | val RMSE: 1,798.10 | val MAE: 926.41 | val MAPE: 14.88%\n",
      "Epoch 283/1000 | train MSE: 0.07922 | val R2_log: 0.9490 | val RMSE: 1,839.36 | val MAE: 949.53 | val MAPE: 14.99%\n",
      "Epoch 284/1000 | train MSE: 0.07791 | val R2_log: 0.9498 | val RMSE: 1,869.30 | val MAE: 951.76 | val MAPE: 14.68%\n",
      "Epoch 285/1000 | train MSE: 0.07943 | val R2_log: 0.9506 | val RMSE: 1,775.69 | val MAE: 901.27 | val MAPE: 14.57%\n",
      "Epoch 286/1000 | train MSE: 0.07818 | val R2_log: 0.9502 | val RMSE: 1,851.15 | val MAE: 947.43 | val MAPE: 14.68%\n",
      "Epoch 287/1000 | train MSE: 0.07810 | val R2_log: 0.9513 | val RMSE: 1,735.87 | val MAE: 877.89 | val MAPE: 14.41%\n",
      "Epoch 288/1000 | train MSE: 0.07954 | val R2_log: 0.9510 | val RMSE: 1,840.56 | val MAE: 920.90 | val MAPE: 14.47%\n",
      "Epoch 289/1000 | train MSE: 0.07761 | val R2_log: 0.9519 | val RMSE: 1,766.65 | val MAE: 886.77 | val MAPE: 13.93%\n",
      "Epoch 290/1000 | train MSE: 0.07765 | val R2_log: 0.9513 | val RMSE: 1,792.94 | val MAE: 903.90 | val MAPE: 14.28%\n",
      "Epoch 291/1000 | train MSE: 0.07936 | val R2_log: 0.9525 | val RMSE: 1,717.34 | val MAE: 850.72 | val MAPE: 13.91%\n",
      "Epoch 00291: reducing learning rate of group 0 to 3.1250e-05.\n",
      "Epoch 292/1000 | train MSE: 0.07747 | val R2_log: 0.9520 | val RMSE: 1,749.70 | val MAE: 875.88 | val MAPE: 14.02%\n",
      "Epoch 293/1000 | train MSE: 0.07905 | val R2_log: 0.9508 | val RMSE: 1,827.25 | val MAE: 915.90 | val MAPE: 14.28%\n",
      "Epoch 294/1000 | train MSE: 0.07701 | val R2_log: 0.9499 | val RMSE: 1,836.09 | val MAE: 932.63 | val MAPE: 14.66%\n",
      "Epoch 295/1000 | train MSE: 0.07775 | val R2_log: 0.9503 | val RMSE: 1,797.21 | val MAE: 917.31 | val MAPE: 14.66%\n",
      "Epoch 296/1000 | train MSE: 0.07783 | val R2_log: 0.9522 | val RMSE: 1,737.29 | val MAE: 873.44 | val MAPE: 13.91%\n",
      "Epoch 297/1000 | train MSE: 0.07947 | val R2_log: 0.9487 | val RMSE: 1,824.94 | val MAE: 941.25 | val MAPE: 15.04%\n",
      "Epoch 298/1000 | train MSE: 0.07872 | val R2_log: 0.9521 | val RMSE: 1,743.41 | val MAE: 877.01 | val MAPE: 13.94%\n",
      "Epoch 299/1000 | train MSE: 0.07746 | val R2_log: 0.9507 | val RMSE: 1,776.35 | val MAE: 903.22 | val MAPE: 14.48%\n",
      "Epoch 300/1000 | train MSE: 0.07778 | val R2_log: 0.9528 | val RMSE: 1,741.43 | val MAE: 864.24 | val MAPE: 13.68%\n",
      "Epoch 301/1000 | train MSE: 0.07899 | val R2_log: 0.9494 | val RMSE: 1,830.10 | val MAE: 936.43 | val MAPE: 14.83%\n",
      "Epoch 302/1000 | train MSE: 0.07810 | val R2_log: 0.9522 | val RMSE: 1,732.59 | val MAE: 865.24 | val MAPE: 14.00%\n",
      "Epoch 303/1000 | train MSE: 0.07952 | val R2_log: 0.9510 | val RMSE: 1,864.95 | val MAE: 925.99 | val MAPE: 14.35%\n",
      "Epoch 304/1000 | train MSE: 0.07929 | val R2_log: 0.9481 | val RMSE: 1,853.56 | val MAE: 955.99 | val MAPE: 15.20%\n",
      "Epoch 305/1000 | train MSE: 0.07863 | val R2_log: 0.9482 | val RMSE: 1,877.58 | val MAE: 970.17 | val MAPE: 15.16%\n",
      "Epoch 306/1000 | train MSE: 0.07787 | val R2_log: 0.9511 | val RMSE: 1,807.85 | val MAE: 913.23 | val MAPE: 14.32%\n",
      "Epoch 307/1000 | train MSE: 0.07795 | val R2_log: 0.9502 | val RMSE: 1,838.54 | val MAE: 933.07 | val MAPE: 14.62%\n",
      "Epoch 308/1000 | train MSE: 0.07902 | val R2_log: 0.9500 | val RMSE: 1,814.97 | val MAE: 922.44 | val MAPE: 14.69%\n",
      "Epoch 309/1000 | train MSE: 0.07786 | val R2_log: 0.9486 | val RMSE: 1,957.73 | val MAE: 989.76 | val MAPE: 15.00%\n",
      "Epoch 310/1000 | train MSE: 0.07945 | val R2_log: 0.9490 | val RMSE: 1,813.58 | val MAE: 936.14 | val MAPE: 15.02%\n",
      "Epoch 311/1000 | train MSE: 0.07821 | val R2_log: 0.9503 | val RMSE: 1,821.34 | val MAE: 920.42 | val MAPE: 14.56%\n",
      "Epoch 312/1000 | train MSE: 0.07874 | val R2_log: 0.9504 | val RMSE: 1,847.67 | val MAE: 931.21 | val MAPE: 14.47%\n",
      "Epoch 313/1000 | train MSE: 0.07816 | val R2_log: 0.9496 | val RMSE: 1,847.13 | val MAE: 942.33 | val MAPE: 14.81%\n",
      "Epoch 314/1000 | train MSE: 0.07917 | val R2_log: 0.9495 | val RMSE: 1,862.48 | val MAE: 945.23 | val MAPE: 14.82%\n",
      "Epoch 315/1000 | train MSE: 0.07757 | val R2_log: 0.9489 | val RMSE: 1,867.46 | val MAE: 952.28 | val MAPE: 14.99%\n",
      "Epoch 316/1000 | train MSE: 0.07834 | val R2_log: 0.9502 | val RMSE: 1,838.67 | val MAE: 932.79 | val MAPE: 14.62%\n",
      "Epoch 317/1000 | train MSE: 0.07769 | val R2_log: 0.9514 | val RMSE: 1,802.84 | val MAE: 901.72 | val MAPE: 14.29%\n",
      "Epoch 00317: reducing learning rate of group 0 to 1.5625e-05.\n",
      "Early stopping at epoch 317 (best @ 167 with R2_log=0.9528).\n",
      "\n",
      "Entrenamiento terminado en 393.33 s (device=cuda).\n",
      "Restored best model from epoch 167 (R2_log=0.9528).\n",
      "\n",
      "=== FT-Transformer con coords No VCR - Test ===\n",
      "R^2 (log): 0.9555\n",
      "RMSE ($): 1,686.77\n",
      "MAE  ($): 834.62\n",
      "MAPE (%): 13.56\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Entrenamiento\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = FTTransformer(\n",
    "    n_features=n_features,\n",
    "    d_token=D_TOKEN,\n",
    "    n_layers=N_LAYERS,\n",
    "    n_head=N_HEAD,\n",
    "    ff_mult=FF_MULT,\n",
    "    dropout=DROPOUT,\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode=\"max\",               # maximizamos R^2 en validación\n",
    "    factor=LR_FACTOR,\n",
    "    patience=LR_PATIENCE,\n",
    "    min_lr=MIN_LR,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "best_r2 = -float(\"inf\")\n",
    "best_state = None\n",
    "best_epoch = 0\n",
    "no_improve = 0\n",
    "\n",
    "def evaluate(dl: DataLoader) -> tuple[float, float, float, float]:\n",
    "    model.eval()\n",
    "    y_true_log, y_pred_log = [], []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in dl:\n",
    "            xb = xb.to(device, non_blocking=True)\n",
    "            yb = yb.to(device, non_blocking=True)\n",
    "            pred = model(xb)\n",
    "            y_true_log.append(yb.detach().cpu().numpy())\n",
    "            y_pred_log.append(pred.detach().cpu().numpy())\n",
    "    y_true_log = np.concatenate(y_true_log)\n",
    "    y_pred_log = np.concatenate(y_pred_log)\n",
    "    # Métricas\n",
    "    r2_log = r2_score(y_true_log, y_pred_log)\n",
    "    y_true_price = np.exp(y_true_log)\n",
    "    y_pred_price = np.exp(y_pred_log)\n",
    "    rmse = root_mean_squared_error(y_true_price, y_pred_price)\n",
    "    mae = mean_absolute_error(y_true_price, y_pred_price)\n",
    "    mape = np.mean(np.abs((y_true_price - y_pred_price) / np.clip(y_true_price, 1e-9, None))) * 100\n",
    "    return r2_log, rmse, mae, mape\n",
    "\n",
    "start = time.perf_counter()\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for xb, yb in train_dl:\n",
    "        xb = xb.to(device, non_blocking=True)\n",
    "        yb = yb.to(device, non_blocking=True)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        pred = model(xb)\n",
    "        loss = criterion(pred, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * xb.size(0)\n",
    "    train_loss = total_loss / len(train_dl.dataset)\n",
    "\n",
    "    r2_log_val, rmse_val, mae_val, mape_val = evaluate(val_dl)\n",
    "    print(\n",
    "        f\"Epoch {epoch:02d}/{EPOCHS} | train MSE: {train_loss:.5f} | \"\n",
    "        f\"val R2_log: {r2_log_val:.4f} | val RMSE: {rmse_val:,.2f} | val MAE: {mae_val:,.2f} | val MAPE: {mape_val:.2f}%\"\n",
    "    )\n",
    "    # Early stopping + ReduceLROnPlateau (monitor: R2_log de validación)\n",
    "    improved = (r2_log_val - best_r2) > MIN_DELTA_R2\n",
    "    if improved:\n",
    "        best_r2 = r2_log_val\n",
    "        best_state = deepcopy(model.state_dict())   # restore best weights\n",
    "        best_epoch = epoch\n",
    "        no_improve = 0\n",
    "    else:\n",
    "        no_improve += 1\n",
    "\n",
    "    scheduler.step(r2_log_val)\n",
    "\n",
    "    if no_improve >= EARLY_STOP_PATIENCE:\n",
    "        print(f\"Early stopping at epoch {epoch} (best @ {best_epoch} with R2_log={best_r2:.4f}).\")\n",
    "        break\n",
    "\n",
    "elapsed = time.perf_counter() - start\n",
    "print(f\"\\nEntrenamiento terminado en {elapsed:.2f} s (device={device}).\")\n",
    "\n",
    "# Restaurar los mejores pesos\n",
    "if best_state is not None:\n",
    "    model.load_state_dict(best_state)\n",
    "    print(f\"Restored best model from epoch {best_epoch} (R2_log={best_r2:.4f}).\")\n",
    "\n",
    "# Evaluación final en test\n",
    "r2_log_test, rmse_test, mae_test, mape_test = evaluate(test_dl)\n",
    "print(\"\\n=== FT-Transformer con coords No VCR - Test ===\")\n",
    "print(f\"R^2 (log): {r2_log_test:.4f}\")\n",
    "print(f\"RMSE ($): {rmse_test:,.2f}\")\n",
    "print(f\"MAE  ($): {mae_test:,.2f}\")\n",
    "print(f\"MAPE (%): {mape_test:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1b4eed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cc1e928d",
   "metadata": {},
   "source": [
    "### Tercer modelo FT-Transformer --> Con coordenadas y VCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "6fe2251b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    25211.000000\n",
       "mean         8.395828\n",
       "std          0.830310\n",
       "min          5.950643\n",
       "25%          7.740664\n",
       "50%          8.242756\n",
       "75%          8.984694\n",
       "max         10.915088\n",
       "Name: log_monto, dtype: float64"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_vcr_e = pd.read_csv('dataset_vcr_expanded.csv')\n",
    "df_vcr_e = df_vcr_e[df_vcr_e['monto'] < 56000].copy()\n",
    "df_vcr_e['log_monto']=np.log(df_vcr_e['monto'])\n",
    "df_vcr_e['log_monto'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "930f065d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 25211 entries, 0 to 25214\n",
      "Data columns (total 181 columns):\n",
      " #    Column                        Dtype  \n",
      "---   ------                        -----  \n",
      " 0    monto                         int64  \n",
      " 1    superficie_t                  float64\n",
      " 2    dormitorios                   int64  \n",
      " 3    dormitorios_faltante          int64  \n",
      " 4    banos                         int64  \n",
      " 5    banos_faltante                int64  \n",
      " 6    antiguedad                    int64  \n",
      " 7    antiguedad_faltante           int64  \n",
      " 8    Or_N                          int64  \n",
      " 9    Or_S                          int64  \n",
      " 10   Or_E                          int64  \n",
      " 11   Or_O                          int64  \n",
      " 12   Or_Faltante                   int64  \n",
      " 13   terraza                       float64\n",
      " 14   estacionamiento               int64  \n",
      " 15   bodegas                       int64  \n",
      " 16   flag_Departamento             int64  \n",
      " 17   flag_Multinivel               int64  \n",
      " 18   flag_Semipiso                 int64  \n",
      " 19   flag_Premium                  int64  \n",
      " 20   flag_Monoambiente             int64  \n",
      " 21   flag_Loft                     int64  \n",
      " 22   latitud                       float64\n",
      " 23   longitud                      float64\n",
      " 24   sport_and_leisure_dim00       float64\n",
      " 25   sport_and_leisure_dim01       float64\n",
      " 26   sport_and_leisure_dim02       float64\n",
      " 27   sport_and_leisure_dim03       float64\n",
      " 28   sport_and_leisure_dim04       float64\n",
      " 29   sport_and_leisure_dim05       float64\n",
      " 30   sport_and_leisure_dim06       float64\n",
      " 31   sport_and_leisure_dim07       float64\n",
      " 32   sport_and_leisure_dim08       float64\n",
      " 33   sport_and_leisure_dim09       float64\n",
      " 34   sport_and_leisure_dim10       float64\n",
      " 35   sport_and_leisure_dim11       float64\n",
      " 36   medical_dim00                 float64\n",
      " 37   medical_dim01                 float64\n",
      " 38   medical_dim02                 float64\n",
      " 39   medical_dim03                 float64\n",
      " 40   medical_dim04                 float64\n",
      " 41   medical_dim05                 float64\n",
      " 42   medical_dim06                 float64\n",
      " 43   medical_dim07                 float64\n",
      " 44   medical_dim08                 float64\n",
      " 45   medical_dim09                 float64\n",
      " 46   medical_dim10                 float64\n",
      " 47   medical_dim11                 float64\n",
      " 48   education_prim_dim00          float64\n",
      " 49   education_prim_dim01          float64\n",
      " 50   education_prim_dim02          float64\n",
      " 51   education_prim_dim03          float64\n",
      " 52   education_prim_dim04          float64\n",
      " 53   education_prim_dim05          float64\n",
      " 54   education_prim_dim06          float64\n",
      " 55   education_prim_dim07          float64\n",
      " 56   education_prim_dim08          float64\n",
      " 57   education_prim_dim09          float64\n",
      " 58   education_prim_dim10          float64\n",
      " 59   education_prim_dim11          float64\n",
      " 60   veterinary_dim00              float64\n",
      " 61   veterinary_dim01              float64\n",
      " 62   veterinary_dim02              float64\n",
      " 63   veterinary_dim03              float64\n",
      " 64   veterinary_dim04              float64\n",
      " 65   veterinary_dim05              float64\n",
      " 66   veterinary_dim06              float64\n",
      " 67   veterinary_dim07              float64\n",
      " 68   veterinary_dim08              float64\n",
      " 69   veterinary_dim09              float64\n",
      " 70   veterinary_dim10              float64\n",
      " 71   veterinary_dim11              float64\n",
      " 72   food_and_drink_stores_dim00   float64\n",
      " 73   food_and_drink_stores_dim01   float64\n",
      " 74   food_and_drink_stores_dim02   float64\n",
      " 75   food_and_drink_stores_dim03   float64\n",
      " 76   food_and_drink_stores_dim04   float64\n",
      " 77   food_and_drink_stores_dim05   float64\n",
      " 78   food_and_drink_stores_dim06   float64\n",
      " 79   food_and_drink_stores_dim07   float64\n",
      " 80   food_and_drink_stores_dim08   float64\n",
      " 81   food_and_drink_stores_dim09   float64\n",
      " 82   food_and_drink_stores_dim10   float64\n",
      " 83   food_and_drink_stores_dim11   float64\n",
      " 84   arts_and_entertainment_dim00  float64\n",
      " 85   arts_and_entertainment_dim01  float64\n",
      " 86   arts_and_entertainment_dim02  float64\n",
      " 87   arts_and_entertainment_dim03  float64\n",
      " 88   arts_and_entertainment_dim04  float64\n",
      " 89   arts_and_entertainment_dim05  float64\n",
      " 90   arts_and_entertainment_dim06  float64\n",
      " 91   arts_and_entertainment_dim07  float64\n",
      " 92   arts_and_entertainment_dim08  float64\n",
      " 93   arts_and_entertainment_dim09  float64\n",
      " 94   arts_and_entertainment_dim10  float64\n",
      " 95   arts_and_entertainment_dim11  float64\n",
      " 96   food_and_drink_dim00          float64\n",
      " 97   food_and_drink_dim01          float64\n",
      " 98   food_and_drink_dim02          float64\n",
      " 99   food_and_drink_dim03          float64\n",
      " 100  food_and_drink_dim04          float64\n",
      " 101  food_and_drink_dim05          float64\n",
      " 102  food_and_drink_dim06          float64\n",
      " 103  food_and_drink_dim07          float64\n",
      " 104  food_and_drink_dim08          float64\n",
      " 105  food_and_drink_dim09          float64\n",
      " 106  food_and_drink_dim10          float64\n",
      " 107  food_and_drink_dim11          float64\n",
      " 108  park_like_dim00               float64\n",
      " 109  park_like_dim01               float64\n",
      " 110  park_like_dim02               float64\n",
      " 111  park_like_dim03               float64\n",
      " 112  park_like_dim04               float64\n",
      " 113  park_like_dim05               float64\n",
      " 114  park_like_dim06               float64\n",
      " 115  park_like_dim07               float64\n",
      " 116  park_like_dim08               float64\n",
      " 117  park_like_dim09               float64\n",
      " 118  park_like_dim10               float64\n",
      " 119  park_like_dim11               float64\n",
      " 120  security_dim00                float64\n",
      " 121  security_dim01                float64\n",
      " 122  security_dim02                float64\n",
      " 123  security_dim03                float64\n",
      " 124  security_dim04                float64\n",
      " 125  security_dim05                float64\n",
      " 126  security_dim06                float64\n",
      " 127  security_dim07                float64\n",
      " 128  security_dim08                float64\n",
      " 129  security_dim09                float64\n",
      " 130  security_dim10                float64\n",
      " 131  security_dim11                float64\n",
      " 132  religion_dim00                float64\n",
      " 133  religion_dim01                float64\n",
      " 134  religion_dim02                float64\n",
      " 135  religion_dim03                float64\n",
      " 136  religion_dim04                float64\n",
      " 137  religion_dim05                float64\n",
      " 138  religion_dim06                float64\n",
      " 139  religion_dim07                float64\n",
      " 140  religion_dim08                float64\n",
      " 141  religion_dim09                float64\n",
      " 142  religion_dim10                float64\n",
      " 143  religion_dim11                float64\n",
      " 144  education_sup_dim00           float64\n",
      " 145  education_sup_dim01           float64\n",
      " 146  education_sup_dim02           float64\n",
      " 147  education_sup_dim03           float64\n",
      " 148  education_sup_dim04           float64\n",
      " 149  education_sup_dim05           float64\n",
      " 150  education_sup_dim06           float64\n",
      " 151  education_sup_dim07           float64\n",
      " 152  education_sup_dim08           float64\n",
      " 153  education_sup_dim09           float64\n",
      " 154  education_sup_dim10           float64\n",
      " 155  education_sup_dim11           float64\n",
      " 156  metro_dim00                   float64\n",
      " 157  metro_dim01                   float64\n",
      " 158  metro_dim02                   float64\n",
      " 159  metro_dim03                   float64\n",
      " 160  metro_dim04                   float64\n",
      " 161  metro_dim05                   float64\n",
      " 162  metro_dim06                   float64\n",
      " 163  metro_dim07                   float64\n",
      " 164  metro_dim08                   float64\n",
      " 165  metro_dim09                   float64\n",
      " 166  metro_dim10                   float64\n",
      " 167  metro_dim11                   float64\n",
      " 168  bus_dim00                     float64\n",
      " 169  bus_dim01                     float64\n",
      " 170  bus_dim02                     float64\n",
      " 171  bus_dim03                     float64\n",
      " 172  bus_dim04                     float64\n",
      " 173  bus_dim05                     float64\n",
      " 174  bus_dim06                     float64\n",
      " 175  bus_dim07                     float64\n",
      " 176  bus_dim08                     float64\n",
      " 177  bus_dim09                     float64\n",
      " 178  bus_dim10                     float64\n",
      " 179  bus_dim11                     float64\n",
      " 180  log_monto                     float64\n",
      "dtypes: float64(161), int64(20)\n",
      "memory usage: 35.0 MB\n"
     ]
    }
   ],
   "source": [
    "df_vcr =df_vcr_e.copy()\n",
    "obj_cols = df_vcr.select_dtypes(include=[\"object\"]).columns\n",
    "cols_to_drop = list(obj_cols)\n",
    "cols_to_drop.append(\"id\")\n",
    "df_vcr = df_vcr.drop(columns=cols_to_drop)\n",
    "df_vcr.info(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "1ce58bb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imputación VCR completada. NaNs antes: 246,228 -> después: 0\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 25211 entries, 0 to 25214\n",
      "Columns: 194 entries, monto to has_bus\n",
      "dtypes: float64(161), int64(33)\n",
      "memory usage: 37.5 MB\n"
     ]
    }
   ],
   "source": [
    "# Imputación datos faltantes en VCR\n",
    "import re\n",
    "from typing import Dict, Tuple, Optional\n",
    "\n",
    "# Configuración (\n",
    "# Dimensiones (1..12) \n",
    "DIMS_MAP = {\n",
    "    1: \"count_pois\",\n",
    "    2: \"mean_distance\",\n",
    "    3: \"min_distance\",\n",
    "    4: \"max_distance\",\n",
    "    5: \"median_distance\",\n",
    "    6: \"std_distance\",\n",
    "    7: \"mean_inverse_distance\",\n",
    "    8: \"max_inverse_distance\",\n",
    "    9: \"sum_inverse_distance\",\n",
    "    10: \"ratio_within_near_radius\",\n",
    "    11: \"ratio_within_mid_radius\",\n",
    "    12: \"ratio_within_far_radius\",\n",
    "}\n",
    "\n",
    "# Rol por dimensión (para decidir la imputación semántica)\n",
    "DIM_ROLE = {\n",
    "    1: \"count\",                # -> 0\n",
    "    2: \"distance\",             # -> R3\n",
    "    3: \"distance\",             # -> R3\n",
    "    4: \"distance\",             # -> R3\n",
    "    5: \"distance\",             # -> R3\n",
    "    6: \"std\",                  # -> 0\n",
    "    7: \"inverse\",              # -> 0\n",
    "    8: \"inverse\",              # -> 0\n",
    "    9: \"inverse\",              # -> 0\n",
    "    10: \"ratio\",               # -> 0\n",
    "    11: \"ratio\",               # -> 0\n",
    "    12: \"ratio\",               # -> 0\n",
    "}\n",
    "\n",
    "# R3 por tipo de clase\n",
    "R3_DEFAULT = 2400.0  # clases generales\n",
    "R3_METRO = 1600.0\n",
    "R3_BUS = 800.0\n",
    "\n",
    "# Funciones\n",
    "def _class_and_dim(col: str) -> Optional[Tuple[str, int]]:\n",
    "    \"\"\"Extrae (clase, índice de dimensión) de columnas tipo '<clase>_dimXX'.\"\"\"\n",
    "    m = re.match(r\"^(?P<klass>.+)_dim(?P<idx>\\d{1,2})$\", col)\n",
    "    if not m:\n",
    "        return None\n",
    "    return m.group(\"klass\"), int(m.group(\"idx\"))\n",
    "\n",
    "\n",
    "def _r3_for_class(klass: str) -> float:\n",
    "    k = klass.lower()\n",
    "    if \"metro\" in k:\n",
    "        return R3_METRO\n",
    "    if \"bus\" in k:\n",
    "        return R3_BUS\n",
    "    return R3_DEFAULT\n",
    "\n",
    "\n",
    "def impute_vcr_semantic(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Imputa VCR por semántica de ausencia: distancias=R3, inversas/ratios=0, count=0, std=0.\n",
    "    Además agrega flags `has_<clase>` indicando presencia de POIs por clase.\n",
    "    \"\"\"\n",
    "    out = df.copy()\n",
    "\n",
    "    # Agrupar columnas por clase\n",
    "    groups: Dict[str, Dict[int, str]] = {}\n",
    "    vcr_cols = []\n",
    "    for c in out.columns:\n",
    "        parsed = _class_and_dim(c)\n",
    "        if parsed is None:\n",
    "            continue\n",
    "        klass, idx = parsed\n",
    "        groups.setdefault(klass, {})[idx] = c\n",
    "        vcr_cols.append(c)\n",
    "\n",
    "    if not groups:\n",
    "        # Nada que imputar\n",
    "        return out\n",
    "\n",
    "    # Flags de presencia por clase (antes de imputar)\n",
    "    for klass, dim_map in groups.items():\n",
    "        cols = list(dim_map.values())\n",
    "        has_series = out[cols].notna().any(axis=1).astype(\"int64\")\n",
    "        out[f\"has_{klass}\"] = has_series  # por qué: distingue ausencia real vs lejanía\n",
    "\n",
    "    # Imputación por clase/dim\n",
    "    n_total_nans = int(out[vcr_cols].isna().sum().sum())\n",
    "    for klass, dim_map in groups.items():\n",
    "        r3 = _r3_for_class(klass)\n",
    "        for idx, col in dim_map.items():\n",
    "            role = DIM_ROLE.get(idx)\n",
    "            if role == \"distance\":\n",
    "                fill_value = r3\n",
    "            elif role in {\"inverse\", \"ratio\", \"std\", \"count\"}:\n",
    "                fill_value = 0.0\n",
    "            else:\n",
    "                # Si hay una dimensión desconocida, ser conservador con 0.0\n",
    "                fill_value = 0.0\n",
    "            out[col] = out[col].fillna(fill_value)\n",
    "\n",
    "    n_after_nans = int(out[vcr_cols].isna().sum().sum())\n",
    "    print(f\"Imputación VCR completada. NaNs antes: {n_total_nans:,d} -> después: {n_after_nans:,d}\")\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "\n",
    "df_vcr_imp = impute_vcr_semantic(df_vcr)\n",
    "df_vcr_imp.info()  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "d4a75e7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_features: 192\n"
     ]
    }
   ],
   "source": [
    "# Split\n",
    "X_df = df_vcr_imp.drop(columns=[\"monto\", \"log_monto\"]).copy()\n",
    "y = df_vcr_imp[\"log_monto\"].values.astype(np.float32)\n",
    "\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "    X_df.values, y, test_size=TEST_SIZE, random_state=SEED\n",
    ")\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_full, y_train_full, test_size=VAL_SIZE, random_state=SEED\n",
    ")\n",
    "\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train = scaler.transform(X_train).astype(np.float32)\n",
    "X_val = scaler.transform(X_val).astype(np.float32)\n",
    "X_test = scaler.transform(X_test).astype(np.float32)\n",
    "\n",
    "n_features = X_train.shape[1]\n",
    "print(f\"n_features: {n_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "b2a18102",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset & Loader\n",
    "class NpDataset(Dataset):\n",
    "    def __init__(self, X: np.ndarray, y: np.ndarray):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            torch.from_numpy(self.X[idx]),\n",
    "            torch.tensor(self.y[idx], dtype=torch.float32),\n",
    "        )\n",
    "\n",
    "train_dl = DataLoader(NpDataset(X_train, y_train), batch_size=BATCH_SIZE, shuffle=True, drop_last=False, pin_memory=True)\n",
    "val_dl = DataLoader(NpDataset(X_val, y_val), batch_size=BATCH_SIZE, shuffle=False, drop_last=False, pin_memory=True)\n",
    "test_dl = DataLoader(NpDataset(X_test, y_test), batch_size=BATCH_SIZE, shuffle=False, drop_last=False, pin_memory=True)\n",
    "\n",
    "# Modelo\n",
    "class NumericTokenizer(nn.Module):\n",
    "    \"\"\"Mapea features numéricas a tokens: token_i = x_i * W_i + b_i\"\"\"\n",
    "    def __init__(self, n_features: int, d_token: int):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.empty(n_features, d_token))\n",
    "        self.bias = nn.Parameter(torch.zeros(n_features, d_token))\n",
    "        nn.init.xavier_uniform_(self.weight)\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x: (B, F) -> (B, F, D)\n",
    "        return x.unsqueeze(-1) * self.weight + self.bias\n",
    "\n",
    "class FTTransformer(nn.Module):\n",
    "    def __init__(self, n_features: int, d_token: int, n_layers: int, n_head: int, ff_mult: int, dropout: float):\n",
    "        super().__init__()\n",
    "        self.tokenizer = NumericTokenizer(n_features, d_token)\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, d_token))\n",
    "        enc_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_token,\n",
    "            nhead=n_head,\n",
    "            dim_feedforward=d_token * ff_mult,\n",
    "            dropout=dropout,\n",
    "            batch_first=True,\n",
    "            activation=\"gelu\",\n",
    "            norm_first=True,\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(enc_layer, num_layers=n_layers)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.LayerNorm(d_token),\n",
    "            nn.Linear(d_token, d_token),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_token, 1),\n",
    "        )\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        B = x.size(0)\n",
    "        tokens = self.tokenizer(x)                  # (B, F, D)\n",
    "        cls = self.cls_token.expand(B, -1, -1)     # (B, 1, D)\n",
    "        x_tok = torch.cat([cls, tokens], dim=1)    # (B, 1+F, D)\n",
    "        x_enc = self.encoder(x_tok)                # (B, 1+F, D)\n",
    "        cls_out = x_enc[:, 0, :]                   # (B, D)\n",
    "        y = self.head(cls_out).squeeze(-1)         # (B,)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "c9497e4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01/1000 | train MSE: 8.19166 | val R2_log: 0.0356 | val RMSE: 6,841.29 | val MAE: 4,176.12 | val MAPE: 77.25%\n",
      "Epoch 02/1000 | train MSE: 0.75642 | val R2_log: 0.4980 | val RMSE: 4,982.91 | val MAE: 3,042.15 | val MAPE: 62.75%\n",
      "Epoch 03/1000 | train MSE: 0.46935 | val R2_log: 0.6709 | val RMSE: 4,121.70 | val MAE: 2,426.80 | val MAPE: 47.03%\n",
      "Epoch 04/1000 | train MSE: 0.37539 | val R2_log: 0.7673 | val RMSE: 3,496.25 | val MAE: 2,055.10 | val MAPE: 38.14%\n",
      "Epoch 05/1000 | train MSE: 0.31988 | val R2_log: 0.8133 | val RMSE: 3,481.24 | val MAE: 2,023.16 | val MAPE: 33.87%\n",
      "Epoch 06/1000 | train MSE: 0.27397 | val R2_log: 0.7997 | val RMSE: 3,202.98 | val MAE: 1,986.70 | val MAPE: 38.04%\n",
      "Epoch 07/1000 | train MSE: 0.25222 | val R2_log: 0.8394 | val RMSE: 2,795.18 | val MAE: 1,685.75 | val MAPE: 32.52%\n",
      "Epoch 08/1000 | train MSE: 0.23553 | val R2_log: 0.8416 | val RMSE: 3,036.90 | val MAE: 1,803.72 | val MAPE: 32.52%\n",
      "Epoch 09/1000 | train MSE: 0.21835 | val R2_log: 0.8730 | val RMSE: 2,490.80 | val MAE: 1,516.57 | val MAPE: 28.21%\n",
      "Epoch 10/1000 | train MSE: 0.21566 | val R2_log: 0.9176 | val RMSE: 2,252.31 | val MAE: 1,217.92 | val MAPE: 19.82%\n",
      "Epoch 11/1000 | train MSE: 0.20308 | val R2_log: 0.8948 | val RMSE: 2,224.86 | val MAE: 1,304.40 | val MAPE: 24.68%\n",
      "Epoch 12/1000 | train MSE: 0.19637 | val R2_log: 0.9034 | val RMSE: 2,265.07 | val MAE: 1,294.75 | val MAPE: 23.29%\n",
      "Epoch 13/1000 | train MSE: 0.19582 | val R2_log: 0.8604 | val RMSE: 2,646.19 | val MAE: 1,631.89 | val MAPE: 30.92%\n",
      "Epoch 14/1000 | train MSE: 0.18829 | val R2_log: 0.9291 | val RMSE: 1,990.59 | val MAE: 1,066.99 | val MAPE: 18.05%\n",
      "Epoch 15/1000 | train MSE: 0.18603 | val R2_log: 0.9039 | val RMSE: 2,439.03 | val MAE: 1,391.15 | val MAPE: 23.49%\n",
      "Epoch 16/1000 | train MSE: 0.18338 | val R2_log: 0.8677 | val RMSE: 2,781.72 | val MAE: 1,666.31 | val MAPE: 29.84%\n",
      "Epoch 17/1000 | train MSE: 0.17747 | val R2_log: 0.9009 | val RMSE: 2,170.88 | val MAE: 1,255.85 | val MAPE: 23.99%\n",
      "Epoch 18/1000 | train MSE: 0.17684 | val R2_log: 0.9314 | val RMSE: 2,034.86 | val MAE: 1,087.51 | val MAPE: 17.05%\n",
      "Epoch 19/1000 | train MSE: 0.18241 | val R2_log: 0.8905 | val RMSE: 2,640.91 | val MAE: 1,512.49 | val MAPE: 26.00%\n",
      "Epoch 20/1000 | train MSE: 0.17862 | val R2_log: 0.9368 | val RMSE: 1,883.93 | val MAE: 991.25 | val MAPE: 16.74%\n",
      "Epoch 21/1000 | train MSE: 0.17225 | val R2_log: 0.9302 | val RMSE: 2,376.02 | val MAE: 1,235.80 | val MAPE: 18.27%\n",
      "Epoch 22/1000 | train MSE: 0.17694 | val R2_log: 0.9238 | val RMSE: 2,091.31 | val MAE: 1,122.35 | val MAPE: 19.70%\n",
      "Epoch 23/1000 | train MSE: 0.16773 | val R2_log: 0.8900 | val RMSE: 2,506.75 | val MAE: 1,462.05 | val MAPE: 26.33%\n",
      "Epoch 24/1000 | train MSE: 0.16961 | val R2_log: 0.9161 | val RMSE: 1,911.44 | val MAE: 1,060.02 | val MAPE: 21.10%\n",
      "Epoch 25/1000 | train MSE: 0.16630 | val R2_log: 0.9150 | val RMSE: 2,421.04 | val MAE: 1,324.95 | val MAPE: 21.84%\n",
      "Epoch 26/1000 | train MSE: 0.16517 | val R2_log: 0.9384 | val RMSE: 1,934.89 | val MAE: 1,025.27 | val MAPE: 16.78%\n",
      "Epoch 27/1000 | train MSE: 0.17016 | val R2_log: 0.9039 | val RMSE: 2,269.66 | val MAE: 1,324.55 | val MAPE: 24.05%\n",
      "Epoch 28/1000 | train MSE: 0.16079 | val R2_log: 0.9423 | val RMSE: 1,911.44 | val MAE: 992.51 | val MAPE: 15.65%\n",
      "Epoch 29/1000 | train MSE: 0.16600 | val R2_log: 0.8609 | val RMSE: 3,165.37 | val MAE: 1,885.18 | val MAPE: 31.64%\n",
      "Epoch 30/1000 | train MSE: 0.16198 | val R2_log: 0.8568 | val RMSE: 2,797.38 | val MAE: 1,737.13 | val MAPE: 32.31%\n",
      "Epoch 31/1000 | train MSE: 0.16289 | val R2_log: 0.9254 | val RMSE: 2,471.28 | val MAE: 1,292.12 | val MAPE: 19.88%\n",
      "Epoch 32/1000 | train MSE: 0.15829 | val R2_log: 0.9367 | val RMSE: 2,406.07 | val MAE: 1,194.10 | val MAPE: 16.83%\n",
      "Epoch 33/1000 | train MSE: 0.15827 | val R2_log: 0.8492 | val RMSE: 3,258.69 | val MAE: 2,012.72 | val MAPE: 33.57%\n",
      "Epoch 34/1000 | train MSE: 0.15803 | val R2_log: 0.9248 | val RMSE: 2,413.78 | val MAE: 1,277.08 | val MAPE: 20.09%\n",
      "Epoch 35/1000 | train MSE: 0.15774 | val R2_log: 0.9162 | val RMSE: 2,313.05 | val MAE: 1,329.51 | val MAPE: 22.02%\n",
      "Epoch 36/1000 | train MSE: 0.14874 | val R2_log: 0.9238 | val RMSE: 2,554.41 | val MAE: 1,336.43 | val MAPE: 20.37%\n",
      "Epoch 37/1000 | train MSE: 0.15080 | val R2_log: 0.9135 | val RMSE: 2,502.74 | val MAE: 1,359.63 | val MAPE: 22.49%\n",
      "Epoch 38/1000 | train MSE: 0.14761 | val R2_log: 0.9404 | val RMSE: 2,053.76 | val MAE: 1,075.86 | val MAPE: 16.78%\n",
      "Epoch 39/1000 | train MSE: 0.14618 | val R2_log: 0.9096 | val RMSE: 2,688.15 | val MAE: 1,503.80 | val MAPE: 23.40%\n",
      "Epoch 40/1000 | train MSE: 0.14486 | val R2_log: 0.9132 | val RMSE: 2,453.26 | val MAE: 1,332.84 | val MAPE: 22.53%\n",
      "Epoch 41/1000 | train MSE: 0.14610 | val R2_log: 0.9393 | val RMSE: 2,157.48 | val MAE: 1,094.83 | val MAPE: 17.13%\n",
      "Epoch 42/1000 | train MSE: 0.14547 | val R2_log: 0.9131 | val RMSE: 2,492.55 | val MAE: 1,415.74 | val MAPE: 22.84%\n",
      "Epoch 43/1000 | train MSE: 0.14229 | val R2_log: 0.8960 | val RMSE: 2,102.48 | val MAE: 1,300.69 | val MAPE: 25.78%\n",
      "Epoch 44/1000 | train MSE: 0.14345 | val R2_log: 0.9164 | val RMSE: 2,648.67 | val MAE: 1,463.08 | val MAPE: 22.05%\n",
      "Epoch 45/1000 | train MSE: 0.14049 | val R2_log: 0.9130 | val RMSE: 1,881.18 | val MAE: 1,120.63 | val MAPE: 22.49%\n",
      "Epoch 46/1000 | train MSE: 0.14087 | val R2_log: 0.9397 | val RMSE: 1,839.47 | val MAE: 992.43 | val MAPE: 17.26%\n",
      "Epoch 47/1000 | train MSE: 0.13533 | val R2_log: 0.9039 | val RMSE: 2,992.39 | val MAE: 1,663.74 | val MAPE: 24.52%\n",
      "Epoch 48/1000 | train MSE: 0.13814 | val R2_log: 0.9108 | val RMSE: 2,456.03 | val MAE: 1,419.29 | val MAPE: 23.38%\n",
      "Epoch 49/1000 | train MSE: 0.13592 | val R2_log: 0.9369 | val RMSE: 1,891.07 | val MAE: 1,056.84 | val MAPE: 17.92%\n",
      "Epoch 50/1000 | train MSE: 0.13421 | val R2_log: 0.8836 | val RMSE: 2,840.17 | val MAE: 1,690.91 | val MAPE: 28.20%\n",
      "Epoch 51/1000 | train MSE: 0.13424 | val R2_log: 0.9315 | val RMSE: 2,723.73 | val MAE: 1,402.48 | val MAPE: 18.87%\n",
      "Epoch 52/1000 | train MSE: 0.13624 | val R2_log: 0.9066 | val RMSE: 2,418.65 | val MAE: 1,391.07 | val MAPE: 24.06%\n",
      "Epoch 53/1000 | train MSE: 0.13604 | val R2_log: 0.8936 | val RMSE: 3,218.06 | val MAE: 1,807.69 | val MAPE: 26.56%\n",
      "Epoch 54/1000 | train MSE: 0.13156 | val R2_log: 0.9410 | val RMSE: 1,841.58 | val MAE: 958.33 | val MAPE: 16.76%\n",
      "Epoch 00054: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch 55/1000 | train MSE: 0.12942 | val R2_log: 0.9359 | val RMSE: 1,927.33 | val MAE: 1,048.92 | val MAPE: 18.19%\n",
      "Epoch 56/1000 | train MSE: 0.12912 | val R2_log: 0.9243 | val RMSE: 2,374.99 | val MAE: 1,306.46 | val MAPE: 20.81%\n",
      "Epoch 57/1000 | train MSE: 0.12583 | val R2_log: 0.9255 | val RMSE: 2,266.82 | val MAE: 1,237.56 | val MAPE: 20.37%\n",
      "Epoch 58/1000 | train MSE: 0.12625 | val R2_log: 0.9227 | val RMSE: 2,337.43 | val MAE: 1,294.99 | val MAPE: 21.07%\n",
      "Epoch 59/1000 | train MSE: 0.12787 | val R2_log: 0.9355 | val RMSE: 1,969.37 | val MAE: 1,057.54 | val MAPE: 18.26%\n",
      "Epoch 60/1000 | train MSE: 0.12716 | val R2_log: 0.9170 | val RMSE: 2,370.74 | val MAE: 1,325.88 | val MAPE: 22.19%\n",
      "Epoch 61/1000 | train MSE: 0.12175 | val R2_log: 0.9351 | val RMSE: 2,276.64 | val MAE: 1,167.01 | val MAPE: 18.34%\n",
      "Epoch 62/1000 | train MSE: 0.12595 | val R2_log: 0.9258 | val RMSE: 2,093.38 | val MAE: 1,188.90 | val MAPE: 20.38%\n",
      "Epoch 63/1000 | train MSE: 0.12343 | val R2_log: 0.9346 | val RMSE: 2,136.46 | val MAE: 1,120.20 | val MAPE: 18.38%\n",
      "Epoch 64/1000 | train MSE: 0.12311 | val R2_log: 0.9423 | val RMSE: 2,079.20 | val MAE: 1,046.63 | val MAPE: 16.67%\n",
      "Epoch 65/1000 | train MSE: 0.12232 | val R2_log: 0.9140 | val RMSE: 2,644.21 | val MAE: 1,426.32 | val MAPE: 22.71%\n",
      "Epoch 66/1000 | train MSE: 0.12174 | val R2_log: 0.9348 | val RMSE: 2,208.72 | val MAE: 1,173.09 | val MAPE: 18.63%\n",
      "Epoch 67/1000 | train MSE: 0.12055 | val R2_log: 0.8914 | val RMSE: 3,268.39 | val MAE: 1,837.14 | val MAPE: 26.95%\n",
      "Epoch 68/1000 | train MSE: 0.12331 | val R2_log: 0.9438 | val RMSE: 2,110.01 | val MAE: 1,090.50 | val MAPE: 16.35%\n",
      "Epoch 69/1000 | train MSE: 0.12105 | val R2_log: 0.9373 | val RMSE: 2,124.59 | val MAE: 1,139.63 | val MAPE: 17.97%\n",
      "Epoch 70/1000 | train MSE: 0.12193 | val R2_log: 0.9353 | val RMSE: 2,508.97 | val MAE: 1,289.63 | val MAPE: 18.33%\n",
      "Epoch 71/1000 | train MSE: 0.11886 | val R2_log: 0.9251 | val RMSE: 2,066.81 | val MAE: 1,171.68 | val MAPE: 20.63%\n",
      "Epoch 72/1000 | train MSE: 0.11663 | val R2_log: 0.8944 | val RMSE: 2,949.63 | val MAE: 1,688.84 | val MAPE: 26.58%\n",
      "Epoch 73/1000 | train MSE: 0.11779 | val R2_log: 0.9344 | val RMSE: 2,408.00 | val MAE: 1,256.49 | val MAPE: 18.59%\n",
      "Epoch 74/1000 | train MSE: 0.11800 | val R2_log: 0.9487 | val RMSE: 1,787.47 | val MAE: 924.25 | val MAPE: 15.18%\n",
      "Epoch 75/1000 | train MSE: 0.11526 | val R2_log: 0.9137 | val RMSE: 2,647.46 | val MAE: 1,461.25 | val MAPE: 22.96%\n",
      "Epoch 76/1000 | train MSE: 0.11461 | val R2_log: 0.9322 | val RMSE: 2,608.99 | val MAE: 1,320.40 | val MAPE: 18.98%\n",
      "Epoch 77/1000 | train MSE: 0.11546 | val R2_log: 0.9265 | val RMSE: 2,520.97 | val MAE: 1,369.80 | val MAPE: 20.35%\n",
      "Epoch 78/1000 | train MSE: 0.11591 | val R2_log: 0.9323 | val RMSE: 2,033.35 | val MAE: 1,102.63 | val MAPE: 18.96%\n",
      "Epoch 79/1000 | train MSE: 0.11578 | val R2_log: 0.9454 | val RMSE: 2,026.08 | val MAE: 1,031.73 | val MAPE: 15.88%\n",
      "Epoch 80/1000 | train MSE: 0.11378 | val R2_log: 0.9423 | val RMSE: 2,038.28 | val MAE: 1,050.87 | val MAPE: 16.74%\n",
      "Epoch 81/1000 | train MSE: 0.11461 | val R2_log: 0.9262 | val RMSE: 2,217.51 | val MAE: 1,235.55 | val MAPE: 20.45%\n",
      "Epoch 82/1000 | train MSE: 0.11428 | val R2_log: 0.9485 | val RMSE: 1,894.34 | val MAE: 937.96 | val MAPE: 15.14%\n",
      "Epoch 83/1000 | train MSE: 0.11571 | val R2_log: 0.9454 | val RMSE: 1,866.37 | val MAE: 964.28 | val MAPE: 15.97%\n",
      "Epoch 84/1000 | train MSE: 0.11691 | val R2_log: 0.9237 | val RMSE: 2,176.44 | val MAE: 1,224.21 | val MAPE: 20.81%\n",
      "Epoch 85/1000 | train MSE: 0.11376 | val R2_log: 0.9146 | val RMSE: 2,327.30 | val MAE: 1,316.78 | val MAPE: 22.79%\n",
      "Epoch 86/1000 | train MSE: 0.11230 | val R2_log: 0.9337 | val RMSE: 1,800.53 | val MAE: 1,002.77 | val MAPE: 18.63%\n",
      "Epoch 87/1000 | train MSE: 0.11520 | val R2_log: 0.9494 | val RMSE: 1,891.79 | val MAE: 935.31 | val MAPE: 14.77%\n",
      "Epoch 88/1000 | train MSE: 0.11408 | val R2_log: 0.9507 | val RMSE: 1,727.34 | val MAE: 878.17 | val MAPE: 14.68%\n",
      "Epoch 89/1000 | train MSE: 0.11128 | val R2_log: 0.9214 | val RMSE: 2,107.84 | val MAE: 1,227.52 | val MAPE: 21.45%\n",
      "Epoch 90/1000 | train MSE: 0.10698 | val R2_log: 0.9431 | val RMSE: 1,867.50 | val MAE: 983.69 | val MAPE: 16.65%\n",
      "Epoch 91/1000 | train MSE: 0.10741 | val R2_log: 0.9415 | val RMSE: 2,152.32 | val MAE: 1,106.13 | val MAPE: 17.01%\n",
      "Epoch 92/1000 | train MSE: 0.10880 | val R2_log: 0.9496 | val RMSE: 1,872.03 | val MAE: 941.28 | val MAPE: 14.73%\n",
      "Epoch 93/1000 | train MSE: 0.10985 | val R2_log: 0.9367 | val RMSE: 2,541.92 | val MAE: 1,249.70 | val MAPE: 17.95%\n",
      "Epoch 94/1000 | train MSE: 0.11268 | val R2_log: 0.9182 | val RMSE: 2,486.36 | val MAE: 1,393.83 | val MAPE: 22.14%\n",
      "Epoch 95/1000 | train MSE: 0.10914 | val R2_log: 0.9298 | val RMSE: 2,576.50 | val MAE: 1,358.82 | val MAPE: 19.51%\n",
      "Epoch 96/1000 | train MSE: 0.11115 | val R2_log: 0.9403 | val RMSE: 2,440.21 | val MAE: 1,236.02 | val MAPE: 17.11%\n",
      "Epoch 97/1000 | train MSE: 0.10873 | val R2_log: 0.9439 | val RMSE: 2,119.62 | val MAE: 1,082.26 | val MAPE: 16.44%\n",
      "Epoch 98/1000 | train MSE: 0.10582 | val R2_log: 0.9450 | val RMSE: 1,839.57 | val MAE: 972.13 | val MAPE: 16.22%\n",
      "Epoch 99/1000 | train MSE: 0.10435 | val R2_log: 0.9353 | val RMSE: 2,251.92 | val MAE: 1,210.25 | val MAPE: 18.54%\n",
      "Epoch 100/1000 | train MSE: 0.10862 | val R2_log: 0.9391 | val RMSE: 2,047.11 | val MAE: 1,100.91 | val MAPE: 17.69%\n",
      "Epoch 101/1000 | train MSE: 0.10825 | val R2_log: 0.9388 | val RMSE: 1,902.96 | val MAE: 1,033.56 | val MAPE: 17.73%\n",
      "Epoch 102/1000 | train MSE: 0.10587 | val R2_log: 0.9505 | val RMSE: 1,790.98 | val MAE: 908.67 | val MAPE: 14.80%\n",
      "Epoch 103/1000 | train MSE: 0.10591 | val R2_log: 0.9392 | val RMSE: 2,612.07 | val MAE: 1,253.68 | val MAPE: 17.31%\n",
      "Epoch 104/1000 | train MSE: 0.10641 | val R2_log: 0.9348 | val RMSE: 2,154.30 | val MAE: 1,158.64 | val MAPE: 18.58%\n",
      "Epoch 105/1000 | train MSE: 0.10300 | val R2_log: 0.9433 | val RMSE: 1,733.45 | val MAE: 942.55 | val MAPE: 16.64%\n",
      "Epoch 106/1000 | train MSE: 0.10342 | val R2_log: 0.9340 | val RMSE: 2,185.65 | val MAE: 1,196.73 | val MAPE: 18.74%\n",
      "Epoch 107/1000 | train MSE: 0.10366 | val R2_log: 0.9264 | val RMSE: 2,441.02 | val MAE: 1,289.60 | val MAPE: 20.31%\n",
      "Epoch 108/1000 | train MSE: 0.10258 | val R2_log: 0.9491 | val RMSE: 1,830.63 | val MAE: 909.26 | val MAPE: 14.84%\n",
      "Epoch 109/1000 | train MSE: 0.10202 | val R2_log: 0.9325 | val RMSE: 2,219.34 | val MAE: 1,196.68 | val MAPE: 19.16%\n",
      "Epoch 110/1000 | train MSE: 0.10183 | val R2_log: 0.9329 | val RMSE: 1,946.95 | val MAE: 1,080.68 | val MAPE: 18.97%\n",
      "Epoch 111/1000 | train MSE: 0.10009 | val R2_log: 0.9420 | val RMSE: 2,432.76 | val MAE: 1,213.75 | val MAPE: 16.70%\n",
      "Epoch 112/1000 | train MSE: 0.10106 | val R2_log: 0.9456 | val RMSE: 2,027.39 | val MAE: 1,052.46 | val MAPE: 16.07%\n",
      "Epoch 113/1000 | train MSE: 0.09907 | val R2_log: 0.9451 | val RMSE: 1,960.05 | val MAE: 1,007.66 | val MAPE: 16.04%\n",
      "Epoch 114/1000 | train MSE: 0.10061 | val R2_log: 0.9383 | val RMSE: 2,063.66 | val MAE: 1,098.48 | val MAPE: 17.82%\n",
      "Epoch 00114: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 115/1000 | train MSE: 0.09917 | val R2_log: 0.9370 | val RMSE: 2,406.59 | val MAE: 1,249.30 | val MAPE: 18.03%\n",
      "Epoch 116/1000 | train MSE: 0.09690 | val R2_log: 0.9423 | val RMSE: 1,784.75 | val MAE: 976.24 | val MAPE: 16.86%\n",
      "Epoch 117/1000 | train MSE: 0.09975 | val R2_log: 0.9430 | val RMSE: 2,076.42 | val MAE: 1,083.43 | val MAPE: 16.74%\n",
      "Epoch 118/1000 | train MSE: 0.09748 | val R2_log: 0.9462 | val RMSE: 2,106.54 | val MAE: 1,066.78 | val MAPE: 15.88%\n",
      "Epoch 119/1000 | train MSE: 0.09700 | val R2_log: 0.9183 | val RMSE: 2,486.98 | val MAE: 1,386.72 | val MAPE: 22.12%\n",
      "Epoch 120/1000 | train MSE: 0.09809 | val R2_log: 0.9200 | val RMSE: 2,423.11 | val MAE: 1,371.79 | val MAPE: 21.70%\n",
      "Epoch 121/1000 | train MSE: 0.09661 | val R2_log: 0.9389 | val RMSE: 2,074.51 | val MAE: 1,092.40 | val MAPE: 17.57%\n",
      "Epoch 122/1000 | train MSE: 0.09717 | val R2_log: 0.9372 | val RMSE: 2,020.57 | val MAE: 1,083.24 | val MAPE: 17.99%\n",
      "Epoch 123/1000 | train MSE: 0.09446 | val R2_log: 0.9381 | val RMSE: 2,231.44 | val MAE: 1,151.78 | val MAPE: 17.85%\n",
      "Epoch 124/1000 | train MSE: 0.09439 | val R2_log: 0.9531 | val RMSE: 1,787.25 | val MAE: 874.79 | val MAPE: 13.51%\n",
      "Epoch 125/1000 | train MSE: 0.09802 | val R2_log: 0.9464 | val RMSE: 2,058.96 | val MAE: 1,057.67 | val MAPE: 15.75%\n",
      "Epoch 126/1000 | train MSE: 0.09709 | val R2_log: 0.9292 | val RMSE: 2,106.18 | val MAE: 1,136.30 | val MAPE: 19.73%\n",
      "Epoch 127/1000 | train MSE: 0.09760 | val R2_log: 0.9435 | val RMSE: 1,834.98 | val MAE: 967.57 | val MAPE: 16.50%\n",
      "Epoch 128/1000 | train MSE: 0.09509 | val R2_log: 0.9392 | val RMSE: 2,230.55 | val MAE: 1,151.29 | val MAPE: 17.54%\n",
      "Epoch 129/1000 | train MSE: 0.09446 | val R2_log: 0.9411 | val RMSE: 2,244.32 | val MAE: 1,158.24 | val MAPE: 16.93%\n",
      "Epoch 130/1000 | train MSE: 0.09556 | val R2_log: 0.9414 | val RMSE: 1,907.76 | val MAE: 1,020.59 | val MAPE: 17.03%\n",
      "Epoch 131/1000 | train MSE: 0.09336 | val R2_log: 0.9369 | val RMSE: 2,528.34 | val MAE: 1,250.01 | val MAPE: 17.75%\n",
      "Epoch 132/1000 | train MSE: 0.09512 | val R2_log: 0.9519 | val RMSE: 1,941.84 | val MAE: 949.45 | val MAPE: 14.14%\n",
      "Epoch 133/1000 | train MSE: 0.09536 | val R2_log: 0.9228 | val RMSE: 2,258.78 | val MAE: 1,289.78 | val MAPE: 21.14%\n",
      "Epoch 134/1000 | train MSE: 0.09647 | val R2_log: 0.9400 | val RMSE: 2,054.13 | val MAE: 1,072.95 | val MAPE: 17.40%\n",
      "Epoch 135/1000 | train MSE: 0.09503 | val R2_log: 0.9388 | val RMSE: 2,064.54 | val MAE: 1,108.14 | val MAPE: 17.70%\n",
      "Epoch 136/1000 | train MSE: 0.09316 | val R2_log: 0.9440 | val RMSE: 1,805.73 | val MAE: 982.88 | val MAPE: 16.42%\n",
      "Epoch 137/1000 | train MSE: 0.09370 | val R2_log: 0.9390 | val RMSE: 2,308.29 | val MAE: 1,175.84 | val MAPE: 17.51%\n",
      "Epoch 138/1000 | train MSE: 0.09298 | val R2_log: 0.9306 | val RMSE: 2,323.91 | val MAE: 1,260.61 | val MAPE: 19.43%\n",
      "Epoch 139/1000 | train MSE: 0.09540 | val R2_log: 0.9248 | val RMSE: 2,076.11 | val MAE: 1,188.22 | val MAPE: 20.71%\n",
      "Epoch 140/1000 | train MSE: 0.09346 | val R2_log: 0.9413 | val RMSE: 2,273.28 | val MAE: 1,147.58 | val MAPE: 16.83%\n",
      "Epoch 141/1000 | train MSE: 0.09279 | val R2_log: 0.9410 | val RMSE: 2,024.16 | val MAE: 1,057.32 | val MAPE: 17.18%\n",
      "Epoch 142/1000 | train MSE: 0.09339 | val R2_log: 0.9305 | val RMSE: 2,300.68 | val MAE: 1,233.19 | val MAPE: 19.47%\n",
      "Epoch 143/1000 | train MSE: 0.09267 | val R2_log: 0.9399 | val RMSE: 2,056.05 | val MAE: 1,087.60 | val MAPE: 17.34%\n",
      "Epoch 144/1000 | train MSE: 0.09180 | val R2_log: 0.9333 | val RMSE: 2,376.77 | val MAE: 1,263.70 | val MAPE: 18.90%\n",
      "Epoch 145/1000 | train MSE: 0.09134 | val R2_log: 0.9480 | val RMSE: 1,861.73 | val MAE: 965.54 | val MAPE: 15.43%\n",
      "Epoch 146/1000 | train MSE: 0.09119 | val R2_log: 0.9389 | val RMSE: 1,795.53 | val MAE: 952.17 | val MAPE: 17.20%\n",
      "Epoch 147/1000 | train MSE: 0.08986 | val R2_log: 0.9499 | val RMSE: 1,812.47 | val MAE: 923.02 | val MAPE: 14.40%\n",
      "Epoch 148/1000 | train MSE: 0.09263 | val R2_log: 0.9518 | val RMSE: 1,874.36 | val MAE: 910.59 | val MAPE: 13.87%\n",
      "Epoch 149/1000 | train MSE: 0.09377 | val R2_log: 0.9423 | val RMSE: 1,937.65 | val MAE: 1,014.54 | val MAPE: 16.64%\n",
      "Epoch 150/1000 | train MSE: 0.09065 | val R2_log: 0.9307 | val RMSE: 2,246.03 | val MAE: 1,243.98 | val MAPE: 19.45%\n",
      "Epoch 00150: reducing learning rate of group 0 to 2.5000e-04.\n",
      "Epoch 151/1000 | train MSE: 0.09053 | val R2_log: 0.9421 | val RMSE: 2,171.49 | val MAE: 1,112.23 | val MAPE: 16.85%\n",
      "Epoch 152/1000 | train MSE: 0.08771 | val R2_log: 0.9225 | val RMSE: 2,275.99 | val MAE: 1,305.86 | val MAPE: 21.13%\n",
      "Epoch 153/1000 | train MSE: 0.08935 | val R2_log: 0.9406 | val RMSE: 1,971.89 | val MAE: 1,050.61 | val MAPE: 17.09%\n",
      "Epoch 154/1000 | train MSE: 0.08904 | val R2_log: 0.9499 | val RMSE: 1,903.49 | val MAE: 949.62 | val MAPE: 14.79%\n",
      "Epoch 155/1000 | train MSE: 0.09157 | val R2_log: 0.9457 | val RMSE: 1,939.48 | val MAE: 992.84 | val MAPE: 15.77%\n",
      "Epoch 156/1000 | train MSE: 0.08822 | val R2_log: 0.9468 | val RMSE: 1,896.54 | val MAE: 976.78 | val MAPE: 15.51%\n",
      "Epoch 157/1000 | train MSE: 0.09084 | val R2_log: 0.9512 | val RMSE: 1,871.85 | val MAE: 923.68 | val MAPE: 14.30%\n",
      "Epoch 158/1000 | train MSE: 0.08927 | val R2_log: 0.9432 | val RMSE: 1,950.78 | val MAE: 1,009.76 | val MAPE: 16.39%\n",
      "Epoch 159/1000 | train MSE: 0.08830 | val R2_log: 0.9485 | val RMSE: 1,868.72 | val MAE: 952.67 | val MAPE: 14.99%\n",
      "Epoch 160/1000 | train MSE: 0.08824 | val R2_log: 0.9393 | val RMSE: 2,024.80 | val MAE: 1,077.13 | val MAPE: 17.30%\n",
      "Epoch 161/1000 | train MSE: 0.08862 | val R2_log: 0.9329 | val RMSE: 2,428.71 | val MAE: 1,278.23 | val MAPE: 18.79%\n",
      "Epoch 162/1000 | train MSE: 0.08897 | val R2_log: 0.9458 | val RMSE: 2,148.60 | val MAE: 1,054.55 | val MAPE: 15.72%\n",
      "Epoch 163/1000 | train MSE: 0.08814 | val R2_log: 0.9499 | val RMSE: 1,752.13 | val MAE: 899.79 | val MAPE: 14.56%\n",
      "Epoch 164/1000 | train MSE: 0.08978 | val R2_log: 0.9442 | val RMSE: 1,942.86 | val MAE: 1,009.33 | val MAPE: 15.99%\n",
      "Epoch 165/1000 | train MSE: 0.08625 | val R2_log: 0.9426 | val RMSE: 2,082.68 | val MAE: 1,093.32 | val MAPE: 16.63%\n",
      "Epoch 166/1000 | train MSE: 0.08916 | val R2_log: 0.9452 | val RMSE: 2,176.23 | val MAE: 1,094.75 | val MAPE: 15.82%\n",
      "Epoch 167/1000 | train MSE: 0.08712 | val R2_log: 0.9442 | val RMSE: 1,963.93 | val MAE: 1,007.42 | val MAPE: 16.04%\n",
      "Epoch 168/1000 | train MSE: 0.08732 | val R2_log: 0.9430 | val RMSE: 2,077.12 | val MAE: 1,077.40 | val MAPE: 16.57%\n",
      "Epoch 169/1000 | train MSE: 0.08945 | val R2_log: 0.9471 | val RMSE: 2,103.19 | val MAE: 1,053.69 | val MAPE: 15.17%\n",
      "Epoch 170/1000 | train MSE: 0.08759 | val R2_log: 0.9481 | val RMSE: 1,918.98 | val MAE: 974.18 | val MAPE: 15.14%\n",
      "Epoch 171/1000 | train MSE: 0.08645 | val R2_log: 0.9442 | val RMSE: 2,049.02 | val MAE: 1,062.01 | val MAPE: 16.05%\n",
      "Epoch 172/1000 | train MSE: 0.08710 | val R2_log: 0.9441 | val RMSE: 2,127.10 | val MAE: 1,069.43 | val MAPE: 16.16%\n",
      "Epoch 173/1000 | train MSE: 0.08541 | val R2_log: 0.9422 | val RMSE: 2,209.98 | val MAE: 1,129.63 | val MAPE: 16.67%\n",
      "Epoch 174/1000 | train MSE: 0.08642 | val R2_log: 0.9390 | val RMSE: 1,984.44 | val MAE: 1,044.25 | val MAPE: 17.28%\n",
      "Epoch 175/1000 | train MSE: 0.08552 | val R2_log: 0.9465 | val RMSE: 2,027.37 | val MAE: 991.97 | val MAPE: 15.51%\n",
      "Epoch 176/1000 | train MSE: 0.08809 | val R2_log: 0.9470 | val RMSE: 1,990.17 | val MAE: 998.11 | val MAPE: 15.27%\n",
      "Epoch 00176: reducing learning rate of group 0 to 1.2500e-04.\n",
      "Epoch 177/1000 | train MSE: 0.08600 | val R2_log: 0.9480 | val RMSE: 1,780.02 | val MAE: 905.54 | val MAPE: 15.08%\n",
      "Epoch 178/1000 | train MSE: 0.08477 | val R2_log: 0.9403 | val RMSE: 2,109.30 | val MAE: 1,100.67 | val MAPE: 16.95%\n",
      "Epoch 179/1000 | train MSE: 0.08572 | val R2_log: 0.9443 | val RMSE: 2,033.69 | val MAE: 1,039.45 | val MAPE: 16.06%\n",
      "Epoch 180/1000 | train MSE: 0.08574 | val R2_log: 0.9530 | val RMSE: 1,772.12 | val MAE: 867.42 | val MAPE: 13.57%\n",
      "Epoch 181/1000 | train MSE: 0.08557 | val R2_log: 0.9429 | val RMSE: 1,928.51 | val MAE: 1,013.90 | val MAPE: 16.40%\n",
      "Epoch 182/1000 | train MSE: 0.08431 | val R2_log: 0.9480 | val RMSE: 1,864.88 | val MAE: 945.02 | val MAPE: 15.05%\n",
      "Epoch 183/1000 | train MSE: 0.08565 | val R2_log: 0.9391 | val RMSE: 2,196.21 | val MAE: 1,156.85 | val MAPE: 17.30%\n",
      "Epoch 184/1000 | train MSE: 0.08464 | val R2_log: 0.9384 | val RMSE: 2,339.34 | val MAE: 1,195.22 | val MAPE: 17.48%\n",
      "Epoch 185/1000 | train MSE: 0.08720 | val R2_log: 0.9423 | val RMSE: 2,017.05 | val MAE: 1,032.48 | val MAPE: 16.46%\n",
      "Epoch 186/1000 | train MSE: 0.08562 | val R2_log: 0.9411 | val RMSE: 1,994.59 | val MAE: 1,050.60 | val MAPE: 16.74%\n",
      "Epoch 187/1000 | train MSE: 0.08536 | val R2_log: 0.9456 | val RMSE: 1,995.20 | val MAE: 1,009.40 | val MAPE: 15.57%\n",
      "Epoch 188/1000 | train MSE: 0.08445 | val R2_log: 0.9471 | val RMSE: 1,944.14 | val MAE: 979.60 | val MAPE: 15.20%\n",
      "Epoch 189/1000 | train MSE: 0.08552 | val R2_log: 0.9473 | val RMSE: 1,923.45 | val MAE: 966.82 | val MAPE: 15.01%\n",
      "Epoch 190/1000 | train MSE: 0.08513 | val R2_log: 0.9497 | val RMSE: 1,838.12 | val MAE: 914.78 | val MAPE: 14.42%\n",
      "Epoch 191/1000 | train MSE: 0.08544 | val R2_log: 0.9422 | val RMSE: 2,068.60 | val MAE: 1,076.42 | val MAPE: 16.47%\n",
      "Epoch 192/1000 | train MSE: 0.08448 | val R2_log: 0.9422 | val RMSE: 2,249.92 | val MAE: 1,128.75 | val MAPE: 16.33%\n",
      "Epoch 193/1000 | train MSE: 0.08445 | val R2_log: 0.9330 | val RMSE: 2,359.71 | val MAE: 1,253.24 | val MAPE: 18.51%\n",
      "Epoch 194/1000 | train MSE: 0.08636 | val R2_log: 0.9372 | val RMSE: 2,020.97 | val MAE: 1,083.80 | val MAPE: 17.71%\n",
      "Epoch 195/1000 | train MSE: 0.08402 | val R2_log: 0.9457 | val RMSE: 2,008.33 | val MAE: 1,009.17 | val MAPE: 15.51%\n",
      "Epoch 196/1000 | train MSE: 0.08590 | val R2_log: 0.9495 | val RMSE: 1,797.46 | val MAE: 911.55 | val MAPE: 14.47%\n",
      "Epoch 197/1000 | train MSE: 0.08436 | val R2_log: 0.9453 | val RMSE: 2,086.41 | val MAE: 1,041.61 | val MAPE: 15.66%\n",
      "Epoch 198/1000 | train MSE: 0.08243 | val R2_log: 0.9455 | val RMSE: 2,307.19 | val MAE: 1,113.55 | val MAPE: 15.48%\n",
      "Epoch 199/1000 | train MSE: 0.08468 | val R2_log: 0.9390 | val RMSE: 2,054.75 | val MAE: 1,073.98 | val MAPE: 17.21%\n",
      "Epoch 200/1000 | train MSE: 0.08405 | val R2_log: 0.9457 | val RMSE: 2,113.77 | val MAE: 1,040.40 | val MAPE: 15.35%\n",
      "Epoch 201/1000 | train MSE: 0.08477 | val R2_log: 0.9479 | val RMSE: 2,078.42 | val MAE: 1,003.10 | val MAPE: 14.94%\n",
      "Epoch 202/1000 | train MSE: 0.08507 | val R2_log: 0.9303 | val RMSE: 2,411.27 | val MAE: 1,280.26 | val MAPE: 19.18%\n",
      "Epoch 00202: reducing learning rate of group 0 to 6.2500e-05.\n",
      "Epoch 203/1000 | train MSE: 0.08334 | val R2_log: 0.9426 | val RMSE: 2,107.48 | val MAE: 1,072.66 | val MAPE: 16.32%\n",
      "Epoch 204/1000 | train MSE: 0.08434 | val R2_log: 0.9398 | val RMSE: 2,125.17 | val MAE: 1,108.07 | val MAPE: 17.07%\n",
      "Epoch 205/1000 | train MSE: 0.08394 | val R2_log: 0.9394 | val RMSE: 2,096.53 | val MAE: 1,108.11 | val MAPE: 17.16%\n",
      "Epoch 206/1000 | train MSE: 0.08166 | val R2_log: 0.9457 | val RMSE: 2,025.22 | val MAE: 1,007.95 | val MAPE: 15.50%\n",
      "Epoch 207/1000 | train MSE: 0.08254 | val R2_log: 0.9451 | val RMSE: 1,858.74 | val MAE: 959.69 | val MAPE: 15.66%\n",
      "Epoch 208/1000 | train MSE: 0.08281 | val R2_log: 0.9391 | val RMSE: 1,959.28 | val MAE: 1,029.42 | val MAPE: 17.10%\n",
      "Epoch 209/1000 | train MSE: 0.08350 | val R2_log: 0.9479 | val RMSE: 2,079.57 | val MAE: 999.95 | val MAPE: 14.79%\n",
      "Epoch 210/1000 | train MSE: 0.08382 | val R2_log: 0.9415 | val RMSE: 2,048.15 | val MAE: 1,058.81 | val MAPE: 16.62%\n",
      "Epoch 211/1000 | train MSE: 0.08417 | val R2_log: 0.9470 | val RMSE: 2,002.00 | val MAE: 986.67 | val MAPE: 15.20%\n",
      "Epoch 212/1000 | train MSE: 0.08180 | val R2_log: 0.9414 | val RMSE: 2,168.49 | val MAE: 1,116.58 | val MAPE: 16.64%\n",
      "Epoch 213/1000 | train MSE: 0.08231 | val R2_log: 0.9420 | val RMSE: 2,159.24 | val MAE: 1,099.07 | val MAPE: 16.46%\n",
      "Epoch 214/1000 | train MSE: 0.08229 | val R2_log: 0.9413 | val RMSE: 2,123.77 | val MAE: 1,093.65 | val MAPE: 16.59%\n",
      "Epoch 215/1000 | train MSE: 0.08150 | val R2_log: 0.9412 | val RMSE: 2,232.07 | val MAE: 1,134.69 | val MAPE: 16.54%\n",
      "Epoch 216/1000 | train MSE: 0.08252 | val R2_log: 0.9418 | val RMSE: 2,105.52 | val MAE: 1,087.08 | val MAPE: 16.50%\n",
      "Epoch 217/1000 | train MSE: 0.08304 | val R2_log: 0.9419 | val RMSE: 2,152.60 | val MAE: 1,090.27 | val MAPE: 16.35%\n",
      "Epoch 218/1000 | train MSE: 0.08272 | val R2_log: 0.9447 | val RMSE: 1,969.96 | val MAE: 999.27 | val MAPE: 15.82%\n",
      "Epoch 219/1000 | train MSE: 0.08385 | val R2_log: 0.9436 | val RMSE: 2,046.40 | val MAE: 1,061.50 | val MAPE: 16.16%\n",
      "Epoch 220/1000 | train MSE: 0.08285 | val R2_log: 0.9431 | val RMSE: 2,108.76 | val MAE: 1,062.98 | val MAPE: 16.14%\n",
      "Epoch 221/1000 | train MSE: 0.08146 | val R2_log: 0.9415 | val RMSE: 2,104.67 | val MAE: 1,082.74 | val MAPE: 16.55%\n",
      "Epoch 222/1000 | train MSE: 0.08229 | val R2_log: 0.9471 | val RMSE: 1,911.92 | val MAE: 968.54 | val MAPE: 15.14%\n",
      "Epoch 223/1000 | train MSE: 0.08116 | val R2_log: 0.9472 | val RMSE: 1,884.57 | val MAE: 962.03 | val MAPE: 15.12%\n",
      "Epoch 224/1000 | train MSE: 0.08149 | val R2_log: 0.9445 | val RMSE: 1,927.31 | val MAE: 997.80 | val MAPE: 15.73%\n",
      "Epoch 225/1000 | train MSE: 0.08287 | val R2_log: 0.9455 | val RMSE: 2,035.58 | val MAE: 1,031.76 | val MAPE: 15.53%\n",
      "Epoch 226/1000 | train MSE: 0.08267 | val R2_log: 0.9446 | val RMSE: 2,118.99 | val MAE: 1,066.16 | val MAPE: 15.86%\n",
      "Epoch 227/1000 | train MSE: 0.08158 | val R2_log: 0.9423 | val RMSE: 2,009.75 | val MAE: 1,044.21 | val MAPE: 16.36%\n",
      "Epoch 228/1000 | train MSE: 0.08279 | val R2_log: 0.9494 | val RMSE: 1,814.11 | val MAE: 904.66 | val MAPE: 14.51%\n",
      "Epoch 00228: reducing learning rate of group 0 to 3.1250e-05.\n",
      "Epoch 229/1000 | train MSE: 0.08302 | val R2_log: 0.9464 | val RMSE: 2,105.00 | val MAE: 1,033.54 | val MAPE: 15.25%\n",
      "Epoch 230/1000 | train MSE: 0.08350 | val R2_log: 0.9456 | val RMSE: 1,977.64 | val MAE: 1,000.09 | val MAPE: 15.41%\n",
      "Epoch 231/1000 | train MSE: 0.08164 | val R2_log: 0.9441 | val RMSE: 1,887.37 | val MAE: 970.89 | val MAPE: 15.86%\n",
      "Epoch 232/1000 | train MSE: 0.08362 | val R2_log: 0.9381 | val RMSE: 2,154.29 | val MAE: 1,118.62 | val MAPE: 17.38%\n",
      "Epoch 233/1000 | train MSE: 0.08189 | val R2_log: 0.9435 | val RMSE: 2,028.88 | val MAE: 1,032.65 | val MAPE: 16.00%\n",
      "Epoch 234/1000 | train MSE: 0.08204 | val R2_log: 0.9434 | val RMSE: 2,129.75 | val MAE: 1,073.20 | val MAPE: 16.00%\n",
      "Epoch 235/1000 | train MSE: 0.08197 | val R2_log: 0.9461 | val RMSE: 1,976.52 | val MAE: 998.00 | val MAPE: 15.30%\n",
      "Epoch 236/1000 | train MSE: 0.08187 | val R2_log: 0.9432 | val RMSE: 2,103.44 | val MAE: 1,072.85 | val MAPE: 16.08%\n",
      "Epoch 237/1000 | train MSE: 0.08226 | val R2_log: 0.9417 | val RMSE: 1,986.23 | val MAE: 1,037.39 | val MAPE: 16.46%\n",
      "Epoch 238/1000 | train MSE: 0.08205 | val R2_log: 0.9452 | val RMSE: 1,951.92 | val MAE: 998.34 | val MAPE: 15.55%\n",
      "Epoch 239/1000 | train MSE: 0.08160 | val R2_log: 0.9458 | val RMSE: 1,970.59 | val MAE: 989.80 | val MAPE: 15.33%\n",
      "Epoch 240/1000 | train MSE: 0.08140 | val R2_log: 0.9440 | val RMSE: 2,098.24 | val MAE: 1,060.04 | val MAPE: 15.92%\n",
      "Epoch 241/1000 | train MSE: 0.08253 | val R2_log: 0.9447 | val RMSE: 1,928.43 | val MAE: 999.97 | val MAPE: 15.71%\n",
      "Epoch 242/1000 | train MSE: 0.08090 | val R2_log: 0.9419 | val RMSE: 2,086.24 | val MAE: 1,076.58 | val MAPE: 16.46%\n",
      "Epoch 243/1000 | train MSE: 0.08234 | val R2_log: 0.9457 | val RMSE: 1,937.82 | val MAE: 980.49 | val MAPE: 15.52%\n",
      "Epoch 244/1000 | train MSE: 0.08149 | val R2_log: 0.9451 | val RMSE: 1,994.31 | val MAE: 1,012.12 | val MAPE: 15.54%\n",
      "Epoch 245/1000 | train MSE: 0.08244 | val R2_log: 0.9476 | val RMSE: 1,955.15 | val MAE: 973.10 | val MAPE: 14.81%\n",
      "Epoch 246/1000 | train MSE: 0.08199 | val R2_log: 0.9479 | val RMSE: 1,952.20 | val MAE: 977.86 | val MAPE: 14.75%\n",
      "Epoch 247/1000 | train MSE: 0.08044 | val R2_log: 0.9468 | val RMSE: 1,963.32 | val MAE: 986.48 | val MAPE: 15.12%\n",
      "Epoch 248/1000 | train MSE: 0.07980 | val R2_log: 0.9438 | val RMSE: 1,996.37 | val MAE: 1,027.62 | val MAPE: 15.97%\n",
      "Epoch 249/1000 | train MSE: 0.08270 | val R2_log: 0.9425 | val RMSE: 2,018.70 | val MAE: 1,056.44 | val MAPE: 16.31%\n",
      "Epoch 250/1000 | train MSE: 0.08085 | val R2_log: 0.9475 | val RMSE: 1,890.34 | val MAE: 958.65 | val MAPE: 14.93%\n",
      "Epoch 251/1000 | train MSE: 0.08029 | val R2_log: 0.9455 | val RMSE: 1,999.81 | val MAE: 1,016.68 | val MAPE: 15.50%\n",
      "Epoch 252/1000 | train MSE: 0.08148 | val R2_log: 0.9440 | val RMSE: 1,953.74 | val MAE: 1,015.06 | val MAPE: 15.93%\n",
      "Epoch 253/1000 | train MSE: 0.08179 | val R2_log: 0.9449 | val RMSE: 1,935.10 | val MAE: 988.74 | val MAPE: 15.63%\n",
      "Epoch 254/1000 | train MSE: 0.08249 | val R2_log: 0.9462 | val RMSE: 1,938.78 | val MAE: 971.36 | val MAPE: 15.30%\n",
      "Epoch 00254: reducing learning rate of group 0 to 1.5625e-05.\n",
      "Epoch 255/1000 | train MSE: 0.07885 | val R2_log: 0.9448 | val RMSE: 1,976.38 | val MAE: 1,012.77 | val MAPE: 15.69%\n",
      "Epoch 256/1000 | train MSE: 0.08155 | val R2_log: 0.9459 | val RMSE: 1,956.90 | val MAE: 992.33 | val MAPE: 15.42%\n",
      "Epoch 257/1000 | train MSE: 0.08081 | val R2_log: 0.9474 | val RMSE: 1,918.27 | val MAE: 968.16 | val MAPE: 14.95%\n",
      "Epoch 258/1000 | train MSE: 0.07910 | val R2_log: 0.9458 | val RMSE: 2,076.28 | val MAE: 1,038.08 | val MAPE: 15.36%\n",
      "Epoch 259/1000 | train MSE: 0.08155 | val R2_log: 0.9430 | val RMSE: 1,986.00 | val MAE: 1,013.46 | val MAPE: 16.13%\n",
      "Epoch 260/1000 | train MSE: 0.08059 | val R2_log: 0.9464 | val RMSE: 1,939.49 | val MAE: 976.02 | val MAPE: 15.20%\n",
      "Epoch 261/1000 | train MSE: 0.07997 | val R2_log: 0.9457 | val RMSE: 1,969.01 | val MAE: 999.40 | val MAPE: 15.36%\n",
      "Epoch 262/1000 | train MSE: 0.08186 | val R2_log: 0.9464 | val RMSE: 1,867.17 | val MAE: 956.42 | val MAPE: 15.21%\n",
      "Epoch 263/1000 | train MSE: 0.08157 | val R2_log: 0.9449 | val RMSE: 2,019.64 | val MAE: 1,019.33 | val MAPE: 15.61%\n",
      "Epoch 264/1000 | train MSE: 0.08151 | val R2_log: 0.9445 | val RMSE: 2,081.04 | val MAE: 1,042.39 | val MAPE: 15.76%\n",
      "Epoch 265/1000 | train MSE: 0.07999 | val R2_log: 0.9430 | val RMSE: 2,051.97 | val MAE: 1,054.50 | val MAPE: 16.13%\n",
      "Epoch 266/1000 | train MSE: 0.08241 | val R2_log: 0.9447 | val RMSE: 2,040.42 | val MAE: 1,033.18 | val MAPE: 15.64%\n",
      "Epoch 267/1000 | train MSE: 0.08257 | val R2_log: 0.9432 | val RMSE: 2,101.11 | val MAE: 1,060.23 | val MAPE: 16.08%\n",
      "Epoch 268/1000 | train MSE: 0.07965 | val R2_log: 0.9460 | val RMSE: 1,989.58 | val MAE: 996.53 | val MAPE: 15.34%\n",
      "Epoch 269/1000 | train MSE: 0.08195 | val R2_log: 0.9439 | val RMSE: 2,028.69 | val MAE: 1,032.74 | val MAPE: 15.93%\n",
      "Epoch 270/1000 | train MSE: 0.07967 | val R2_log: 0.9444 | val RMSE: 2,007.49 | val MAE: 1,017.82 | val MAPE: 15.80%\n",
      "Epoch 271/1000 | train MSE: 0.08062 | val R2_log: 0.9446 | val RMSE: 1,982.70 | val MAE: 1,010.99 | val MAPE: 15.74%\n",
      "Epoch 272/1000 | train MSE: 0.08042 | val R2_log: 0.9448 | val RMSE: 1,991.99 | val MAE: 1,009.23 | val MAPE: 15.71%\n",
      "Epoch 273/1000 | train MSE: 0.08057 | val R2_log: 0.9449 | val RMSE: 1,971.72 | val MAE: 1,000.19 | val MAPE: 15.60%\n",
      "Epoch 274/1000 | train MSE: 0.08113 | val R2_log: 0.9449 | val RMSE: 1,996.20 | val MAE: 1,011.60 | val MAPE: 15.60%\n",
      "Early stopping at epoch 274 (best @ 124 with R2_log=0.9531).\n",
      "\n",
      "Entrenamiento terminado en 2933.66 s (device=cuda).\n",
      "Restored best model from epoch 124 (R2_log=0.9531).\n",
      "\n",
      "=== FT-Transformer completo - Test ===\n",
      "R^2 (log): 0.9573\n",
      "RMSE ($): 1,676.25\n",
      "MAE  ($): 837.69\n",
      "MAPE (%): 13.16\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Entrenamiento\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = FTTransformer(\n",
    "    n_features=n_features,\n",
    "    d_token=D_TOKEN,\n",
    "    n_layers=N_LAYERS,\n",
    "    n_head=N_HEAD,\n",
    "    ff_mult=FF_MULT,\n",
    "    dropout=DROPOUT,\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode=\"max\",               # maximizamos R^2 en validación\n",
    "    factor=LR_FACTOR,\n",
    "    patience=LR_PATIENCE,\n",
    "    min_lr=MIN_LR,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "best_r2 = -float(\"inf\")\n",
    "best_state = None\n",
    "best_epoch = 0\n",
    "no_improve = 0\n",
    "\n",
    "def evaluate(dl: DataLoader) -> tuple[float, float, float, float]:\n",
    "    model.eval()\n",
    "    y_true_log, y_pred_log = [], []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in dl:\n",
    "            xb = xb.to(device, non_blocking=True)\n",
    "            yb = yb.to(device, non_blocking=True)\n",
    "            pred = model(xb)\n",
    "            y_true_log.append(yb.detach().cpu().numpy())\n",
    "            y_pred_log.append(pred.detach().cpu().numpy())\n",
    "    y_true_log = np.concatenate(y_true_log)\n",
    "    y_pred_log = np.concatenate(y_pred_log)\n",
    "    # Métricas\n",
    "    r2_log = r2_score(y_true_log, y_pred_log)\n",
    "    y_true_price = np.exp(y_true_log)\n",
    "    y_pred_price = np.exp(y_pred_log)\n",
    "    rmse = root_mean_squared_error(y_true_price, y_pred_price)\n",
    "    mae = mean_absolute_error(y_true_price, y_pred_price)\n",
    "    mape = np.mean(np.abs((y_true_price - y_pred_price) / np.clip(y_true_price, 1e-9, None))) * 100\n",
    "    return r2_log, rmse, mae, mape\n",
    "\n",
    "start = time.perf_counter()\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for xb, yb in train_dl:\n",
    "        xb = xb.to(device, non_blocking=True)\n",
    "        yb = yb.to(device, non_blocking=True)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        pred = model(xb)\n",
    "        loss = criterion(pred, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * xb.size(0)\n",
    "    train_loss = total_loss / len(train_dl.dataset)\n",
    "\n",
    "    r2_log_val, rmse_val, mae_val, mape_val = evaluate(val_dl)\n",
    "    print(\n",
    "        f\"Epoch {epoch:02d}/{EPOCHS} | train MSE: {train_loss:.5f} | \"\n",
    "        f\"val R2_log: {r2_log_val:.4f} | val RMSE: {rmse_val:,.2f} | val MAE: {mae_val:,.2f} | val MAPE: {mape_val:.2f}%\"\n",
    "    )\n",
    "    # Early stopping + ReduceLROnPlateau (monitor: R2_log de validación)\n",
    "    improved = (r2_log_val - best_r2) > MIN_DELTA_R2\n",
    "    if improved:\n",
    "        best_r2 = r2_log_val\n",
    "        best_state = deepcopy(model.state_dict())   # restore best weights\n",
    "        best_epoch = epoch\n",
    "        no_improve = 0\n",
    "    else:\n",
    "        no_improve += 1\n",
    "\n",
    "    scheduler.step(r2_log_val)\n",
    "\n",
    "    if no_improve >= EARLY_STOP_PATIENCE:\n",
    "        print(f\"Early stopping at epoch {epoch} (best @ {best_epoch} with R2_log={best_r2:.4f}).\")\n",
    "        break\n",
    "\n",
    "elapsed = time.perf_counter() - start\n",
    "print(f\"\\nEntrenamiento terminado en {elapsed:.2f} s (device={device}).\")\n",
    "\n",
    "# Restaurar los mejores pesos\n",
    "if best_state is not None:\n",
    "    model.load_state_dict(best_state)\n",
    "    print(f\"Restored best model from epoch {best_epoch} (R2_log={best_r2:.4f}).\")\n",
    "\n",
    "# Evaluación final en test\n",
    "r2_log_test, rmse_test, mae_test, mape_test = evaluate(test_dl)\n",
    "print(\"\\n=== FT-Transformer completo - Test ===\")\n",
    "print(f\"R^2 (log): {r2_log_test:.4f}\")\n",
    "print(f\"RMSE ($): {rmse_test:,.2f}\")\n",
    "print(f\"MAE  ($): {mae_test:,.2f}\")\n",
    "print(f\"MAPE (%): {mape_test:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4f09d1",
   "metadata": {},
   "source": [
    "##### 1000 epochs -> 155 min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e562c03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== FT-Transformer (simple) - Test ===\n",
      "R^2 (log): 0.9541\n",
      "RMSE ($): 1,753.83\n",
      "MAE  ($): 834.79\n",
      "MAPE (%): 13.37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Pc-ADS\\.conda\\envs\\tf\\lib\\site-packages\\sklearn\\metrics\\_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "r2_log_test, rmse_test, mae_test, mape_test = evaluate(test_dl)\n",
    "print(\"\\n=== FT-Transformer (simple) - Test ===\")\n",
    "print(f\"R^2 (log): {r2_log_test:.4f}\")\n",
    "print(f\"RMSE ($): {rmse_test:,.2f}\")\n",
    "print(f\"MAE  ($): {mae_test:,.2f}\")\n",
    "print(f\"MAPE (%): {mape_test:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da19e63",
   "metadata": {},
   "source": [
    "#### Recuento modelos (100 epochs)\n",
    "| Modelo                                 | R² (log) | RMSE ($) | MAE ($) | MAPE (%) |\n",
    "|----------------------------------------|:--------:|---------:|--------:|---------:|\n",
    "| 1) FT-Transformer (solo estructural)   |  0.8829  |  2,660.34| 1,463.98|    25.63 |\n",
    "| 2) FT-Transformer + latitud/longitud   |  0.9213  |  1,901.99| 1,074.23|    20.88 |\n",
    "| 3) FT-Transformer con VCR completos    |  0.9381  |  1,934.98| 1,083.63|    18.22 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ddad65",
   "metadata": {},
   "source": [
    "#### Recuento modelos (1000 epochs)\n",
    "| Modelo                                 | R² (log) | RMSE ($) | MAE ($) | MAPE (%) |\n",
    "|----------------------------------------|:--------:|---------:|--------:|---------:|\n",
    "| 1) FT-Transformer (solo estructural)   |    0.8997|  2,058.75| 1,131.58|     20.72|\n",
    "| 2) FT-Transformer + latitud/longitud   |    0.9505|  1,698.56|   857.12|     13.74|\n",
    "| 3) FT-Transformer con VCR completos    |  0.9541  |  1,753.83|   834.79|    13.37 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4bdd2a",
   "metadata": {},
   "source": [
    "#### Recuento modelos (1000 epochs) con EarlyStopping (muy agresivo al parecer)\n",
    "| Modelo                                    | R² (log) | RMSE ($) | MAE ($) | MAPE (%) |\n",
    "|-------------------------------------------|:--------:|---------:|--------:|---------:|\n",
    "| 1) FT-Transformer (solo estructural) epoch 52 |    0.9048|  2,057.48| 1,141.61|     20.74|\n",
    "| 2) FT-Transformer + latitud/longitud epoch 65 |    0.9464|  1,900.77|   978.96|     15.75|\n",
    "| 3) FT-Transformer con VCR completos epoch 41  |    0.9481|  1,794.88|   927.01|   14.40|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc144789",
   "metadata": {},
   "source": [
    "#### Recuento modelos (1000 epochs) con EarlyStopping (mas suave)\n",
    "| Modelo                                         | R² (log) | RMSE ($) | MAE ($) | MAPE (%) |\n",
    "|------------------------------------------------|:--------:|---------:|--------:|---------:|\n",
    "| 1) FT-Transformer (solo estructural) epoch 175 |    0.9086|  2,110.31| 1,129.95|     19.75|\n",
    "| 2) FT-Transformer + latitud/longitud epoch 167 |    0.9555|  1,686.77|   834.62|     13.56|\n",
    "| 3) FT-Transformer con VCR completos epoch 124  |    0.9573|  1,676.25|   837.69|     13.16|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6006b7",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
